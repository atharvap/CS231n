{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cuda\n",
      "==> Preparing data..\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import sampler\n",
    "from Models import *\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F \n",
    "import torchvision.datasets as dset\n",
    "from __future__ import print_function\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from Models.gan_model import Generator, Discriminator\n",
    "from training import Trainer\n",
    "import os\n",
    "import argparse\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import numpy as np\n",
    "USE_GPU = True\n",
    "\n",
    "dtype = torch.float32 # we will be using float throughout this tutorial\n",
    "\n",
    "if USE_GPU and torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "# Constant to control how frequently we print train loss\n",
    "print_every = 50\n",
    "\n",
    "print('using device:', device)\n",
    "\n",
    "NUM_TRAIN = 49000\n",
    "\n",
    "# The torchvision.transforms package provides tools for preprocessing data\n",
    "# and for performing data augmentation; here we set up a transform to\n",
    "# preprocess the data by subtracting the mean RGB value and dividing by the\n",
    "# standard deviation of each RGB value; we've hardcoded the mean and std.\n",
    "# On training data we perform flips, crops etc\n",
    "print('==> Preparing data..')\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "# We set up a Dataset object for each split (train / val / test); Datasets load\n",
    "# training examples one at a time, so we wrap each Dataset in a DataLoader which\n",
    "# iterates through the Dataset and forms minibatches. We divide the CIFAR-10\n",
    "# training set into train and val sets by passing a Sampler object to the\n",
    "# DataLoader telling how it should sample from the underlying Dataset.\n",
    "cifar10_train = dset.CIFAR10('./Datasets', train=True, download=True,\n",
    "                             transform=transform_train)\n",
    "\n",
    "loader_train = DataLoader(cifar10_train, batch_size=128, \n",
    "                          sampler=sampler.SubsetRandomSampler(range(NUM_TRAIN)))\n",
    "\n",
    "cifar10_val = dset.CIFAR10('./Datasets', train=True, download=True,\n",
    "                           transform=transform_test)\n",
    "loader_val = DataLoader(cifar10_val, batch_size=128, \n",
    "                        sampler=sampler.SubsetRandomSampler(range(NUM_TRAIN, 50000)))\n",
    "\n",
    "cifar10_test = dset.CIFAR10('./Datasets', train=False, download=True, \n",
    "                            transform=transform_test)\n",
    "\n",
    "loader_test = DataLoader(cifar10_test, batch_size=64)\n",
    "\n",
    "\n",
    "\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.002)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "md_spec=[64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M']\n",
    "learning_rate = 2e-3\n",
    "model = cnn_net(md_spec)\n",
    "model.apply(weights_init)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate,\n",
    "                       betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, optimizer, epochs=1):\n",
    "    \"\"\"\n",
    "    Train a model on CIFAR-10 using the PyTorch Module API.\n",
    "    \n",
    "    Inputs:\n",
    "    - model: A PyTorch Module giving the model to train.\n",
    "    - optimizer: An Optimizer object we will use to train the model\n",
    "    - epochs: (Optional) A Python integer giving the number of epochs to train for\n",
    "    \n",
    "    Returns: Nothing, but prints model accuracies during training.\n",
    "    \"\"\"\n",
    "    model = model.to(device=device)  # move the model parameters to CPU/GPU\n",
    "    best_accur=0\n",
    "    num_correct=0\n",
    "    num_samples=0\n",
    "    train_loss=[]\n",
    "    val_loss=[]\n",
    "    train_accuracy=[]\n",
    "    val_accuracy=[]\n",
    "    accur_val,loss_val=check_accuracy(loader_val, model)\n",
    "    val_loss.append(loss_val)\n",
    "    val_accuracy.append(accur_val)\n",
    "    for e in range(epochs):\n",
    "        for t, (x, y) in enumerate(loader_train):\n",
    "            model.train()  # put model to training mode\n",
    "            x = x.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n",
    "            y = y.to(device=device, dtype=torch.long)\n",
    "\n",
    "            scores = model(x)\n",
    "            loss = F.cross_entropy(scores, y)\n",
    "            _, preds = scores.max(1)\n",
    "            num_correct += (preds == y).sum()\n",
    "            num_samples += preds.size(0)\n",
    "            \n",
    "            # Zero out all of the gradients for the variables which the optimizer\n",
    "            # will update.\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # This is the backwards pass: compute the gradient of the loss with\n",
    "            # respect to each  parameter of the model.\n",
    "            loss.backward()\n",
    "\n",
    "            # Actually update the parameters of the model using the gradients\n",
    "            # computed by the backwards pass.\n",
    "            optimizer.step()\n",
    "\n",
    "            if t % print_every == 0:\n",
    "                print('Epoch %d, Iteration %d, loss = %.4f' % (e, t, loss.item()))\n",
    "                accur_val,loss_val=check_accuracy(loader_val, model)\n",
    "                if accur_val>best_accur:\n",
    "                    best_accur=accur_val\n",
    "                    best_model=model\n",
    "                    save_checkpoint({'epoch': e + 1,\n",
    "                                    'state_dict': model.state_dict(),\n",
    "                                    'best_accuracy': best_accur}, True)\n",
    "                print()\n",
    "                acc = float(num_correct) / num_samples\n",
    "                acc=acc*100\n",
    "                num_correct=0\n",
    "                num_samples=0\n",
    "                train_loss.append(loss)\n",
    "                val_loss.append(loss_val)\n",
    "                train_accuracy.append(acc)\n",
    "                val_accuracy.append(accur_val)\n",
    "    return best_model,train_accuracy,val_accuracy,train_loss,val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_accuracy(loader, model):\n",
    "    if loader.dataset.train:\n",
    "        print('Checking accuracy on validation set')\n",
    "    else:\n",
    "        print('Checking accuracy on test set')   \n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    loss = 0\n",
    "    model.eval()  # set model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n",
    "            y = y.to(device=device, dtype=torch.long)\n",
    "            scores = model(x)\n",
    "            loss = loss + F.cross_entropy(scores, y)\n",
    "            _, preds = scores.max(1)\n",
    "            num_correct += (preds == y).sum()\n",
    "            num_samples += preds.size(0)\n",
    "        acc = float(num_correct) / num_samples\n",
    "        print('Got %d / %d correct (%.2f)' % (num_correct, num_samples, 100 * acc))\n",
    "    return acc*100,loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(state, is_best, filename='checkpoint.pth.tar'):\n",
    "    \"\"\"Save checkpoint if a new best is achieved\"\"\"\n",
    "    if is_best:\n",
    "        print (\"=> Saving a new best\")\n",
    "        torch.save(state, filename)  # save checkpoint\n",
    "    else:\n",
    "        print (\"=> Validation Accuracy did not improve\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#best_model,train_accuracy,val_accuracy,train_loss,val_loss=train_model(model, optimizer, epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_loss' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-8185413c0870>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstyle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"darkgrid\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loss Plots'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'o'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Training loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'o'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Validation loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Iteration'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_loss' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(style=\"darkgrid\")\n",
    "plt.title('Loss Plots')\n",
    "plt.plot(train_loss,'o',label='Training loss')\n",
    "plt.plot(val_loss,'o',label='Validation loss')\n",
    "plt.xlabel('Iteration')\n",
    "plt.legend(ncol=2, loc='lower right')\n",
    "plt.gcf().set_size_inches(15, 12)\n",
    "plt.savefig(\"cnn_loss.png\")\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.show(*args, **kw)>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x864 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.set(style=\"darkgrid\")\n",
    "plt.title('Accuracy Plots')\n",
    "plt.plot(train_accuracy,'o',label='Training accuracy')\n",
    "plt.plot(val_accuracy,'o',label='Validation accuracy')\n",
    "plt.xlabel('Iteration')\n",
    "plt.legend(ncol=2, loc='lower right')\n",
    "plt.gcf().set_size_inches(15, 12)\n",
    "plt.savefig(\"cnn_accuracy.png\")\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'best_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-ae116529203e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msave_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'epoch'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'state_dict'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbest_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Saving the objects:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'best_model' is not defined"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "save_checkpoint({'epoch': 15,'state_dict': best_model.state_dict()}, True)\n",
    "\n",
    "\n",
    "# Saving the objects:\n",
    "with open('cnn_hist.pkl', 'wb') as f:  # Python 3: open(..., 'wb')\n",
    "    pickle.dump([train_accuracy,val_accuracy,train_loss,val_loss], f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "checkpoint = torch.load('checkpoint.pth.tar')\n",
    "start_epoch = checkpoint['epoch']\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "model.eval()\n",
    "# Getting back the objects:\n",
    "with open('cnn_hist.pkl','rb') as f:  # Python 3: open(..., 'rb')\n",
    "    train_accuracy,val_accuracy,train_loss,val_loss = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1\n",
      "Iteration 1\n",
      "D loss: 93.25762176513672\n",
      "GP: 93.26564025878906\n",
      "Gradient norm: 0.03425893560051918\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shared/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:98: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "/home/shared/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:44: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "/home/shared/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:54: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "/home/shared/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:71: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 51\n",
      "D loss: 22.992740631103516\n",
      "GP: 30.000804901123047\n",
      "Gradient norm: 0.45337846875190735\n",
      "G loss: -0.3751060962677002\n",
      "Iteration 101\n",
      "D loss: -52.12873840332031\n",
      "GP: 9.242987632751465\n",
      "Gradient norm: 1.2684121131896973\n",
      "G loss: -3.482785940170288\n",
      "Iteration 151\n",
      "D loss: -70.83946990966797\n",
      "GP: 11.469614028930664\n",
      "Gradient norm: 1.30046546459198\n",
      "G loss: -3.9776148796081543\n",
      "Iteration 201\n",
      "D loss: -77.4244613647461\n",
      "GP: 11.754507064819336\n",
      "Gradient norm: 1.3014732599258423\n",
      "G loss: -4.277538299560547\n",
      "Iteration 251\n",
      "D loss: -78.86371612548828\n",
      "GP: 11.715482711791992\n",
      "Gradient norm: 1.3040128946304321\n",
      "G loss: -4.3093695640563965\n",
      "Iteration 301\n",
      "D loss: -82.03868103027344\n",
      "GP: 13.941326141357422\n",
      "Gradient norm: 1.349661946296692\n",
      "G loss: -4.093405723571777\n",
      "Iteration 351\n",
      "D loss: -83.04389190673828\n",
      "GP: 13.600264549255371\n",
      "Gradient norm: 1.3399714231491089\n",
      "G loss: -3.742159605026245\n",
      "\n",
      "Epoch 2\n",
      "Iteration 1\n",
      "D loss: -81.20692443847656\n",
      "GP: 15.087495803833008\n",
      "Gradient norm: 1.3685487508773804\n",
      "G loss: -3.7554731369018555\n",
      "Iteration 51\n",
      "D loss: -83.3359603881836\n",
      "GP: 12.800813674926758\n",
      "Gradient norm: 1.3350626230239868\n",
      "G loss: -3.478426694869995\n",
      "Iteration 101\n",
      "D loss: -81.90824127197266\n",
      "GP: 12.439180374145508\n",
      "Gradient norm: 1.3328298330307007\n",
      "G loss: -3.1094272136688232\n",
      "Iteration 151\n",
      "D loss: -85.4578628540039\n",
      "GP: 12.966933250427246\n",
      "Gradient norm: 1.3432583808898926\n",
      "G loss: -2.906862497329712\n",
      "Iteration 201\n",
      "D loss: -85.02235412597656\n",
      "GP: 12.680005073547363\n",
      "Gradient norm: 1.3356719017028809\n",
      "G loss: -2.6851401329040527\n",
      "Iteration 251\n",
      "D loss: -86.07047271728516\n",
      "GP: 15.633606910705566\n",
      "Gradient norm: 1.3685376644134521\n",
      "G loss: -2.5298566818237305\n",
      "Iteration 301\n",
      "D loss: -83.8338851928711\n",
      "GP: 12.647419929504395\n",
      "Gradient norm: 1.3368057012557983\n",
      "G loss: -2.4210901260375977\n",
      "Iteration 351\n",
      "D loss: -81.37641906738281\n",
      "GP: 12.236141204833984\n",
      "Gradient norm: 1.329256534576416\n",
      "G loss: -2.5998728275299072\n",
      "\n",
      "Epoch 3\n",
      "Iteration 1\n",
      "D loss: -83.78296661376953\n",
      "GP: 14.494095802307129\n",
      "Gradient norm: 1.3634028434753418\n",
      "G loss: -2.6153528690338135\n",
      "Iteration 51\n",
      "D loss: -82.62675476074219\n",
      "GP: 11.77713394165039\n",
      "Gradient norm: 1.3199717998504639\n",
      "G loss: -2.7925400733947754\n",
      "Iteration 101\n",
      "D loss: -82.93500518798828\n",
      "GP: 14.701467514038086\n",
      "Gradient norm: 1.3634988069534302\n",
      "G loss: -2.4691519737243652\n",
      "Iteration 151\n",
      "D loss: -84.99801635742188\n",
      "GP: 12.322769165039062\n",
      "Gradient norm: 1.3396601676940918\n",
      "G loss: -2.3325674533843994\n",
      "Iteration 201\n",
      "D loss: -83.8224868774414\n",
      "GP: 15.006324768066406\n",
      "Gradient norm: 1.3753235340118408\n",
      "G loss: -2.269299030303955\n",
      "Iteration 251\n",
      "D loss: -84.81641387939453\n",
      "GP: 9.934601783752441\n",
      "Gradient norm: 1.2944200038909912\n",
      "G loss: -2.0225911140441895\n",
      "Iteration 301\n",
      "D loss: -81.61536407470703\n",
      "GP: 14.557106018066406\n",
      "Gradient norm: 1.3661057949066162\n",
      "G loss: -2.566192150115967\n",
      "Iteration 351\n",
      "D loss: -82.33963012695312\n",
      "GP: 14.792825698852539\n",
      "Gradient norm: 1.3700144290924072\n",
      "G loss: -2.5680935382843018\n",
      "\n",
      "Epoch 4\n",
      "Iteration 1\n",
      "D loss: -82.56375122070312\n",
      "GP: 11.197577476501465\n",
      "Gradient norm: 1.318306565284729\n",
      "G loss: -2.7479724884033203\n",
      "Iteration 51\n",
      "D loss: -81.26776123046875\n",
      "GP: 12.890881538391113\n",
      "Gradient norm: 1.3408546447753906\n",
      "G loss: -2.6449520587921143\n",
      "Iteration 101\n",
      "D loss: -82.33553314208984\n",
      "GP: 14.191606521606445\n",
      "Gradient norm: 1.3654102087020874\n",
      "G loss: -2.693211555480957\n",
      "Iteration 151\n",
      "D loss: -83.35466003417969\n",
      "GP: 13.908193588256836\n",
      "Gradient norm: 1.351623773574829\n",
      "G loss: -2.593233346939087\n",
      "Iteration 201\n",
      "D loss: -81.38969421386719\n",
      "GP: 15.395706176757812\n",
      "Gradient norm: 1.377663493156433\n",
      "G loss: -2.8258121013641357\n",
      "Iteration 251\n",
      "D loss: -81.00884246826172\n",
      "GP: 14.958996772766113\n",
      "Gradient norm: 1.372806429862976\n",
      "G loss: -2.702885866165161\n",
      "Iteration 301\n",
      "D loss: -81.6861343383789\n",
      "GP: 13.332581520080566\n",
      "Gradient norm: 1.3492578268051147\n",
      "G loss: -3.112596035003662\n",
      "Iteration 351\n",
      "D loss: -83.78831481933594\n",
      "GP: 11.622499465942383\n",
      "Gradient norm: 1.318528175354004\n",
      "G loss: -2.7713820934295654\n",
      "\n",
      "Epoch 5\n",
      "Iteration 1\n",
      "D loss: -79.28539276123047\n",
      "GP: 11.084360122680664\n",
      "Gradient norm: 1.316972017288208\n",
      "G loss: -3.081660747528076\n",
      "Iteration 51\n",
      "D loss: -84.47505187988281\n",
      "GP: 14.35521125793457\n",
      "Gradient norm: 1.3630073070526123\n",
      "G loss: -3.306380033493042\n",
      "Iteration 101\n",
      "D loss: -81.7042236328125\n",
      "GP: 14.254074096679688\n",
      "Gradient norm: 1.3640873432159424\n",
      "G loss: -3.291283369064331\n",
      "Iteration 151\n",
      "D loss: -82.45396423339844\n",
      "GP: 12.707035064697266\n",
      "Gradient norm: 1.3381881713867188\n",
      "G loss: -3.8838346004486084\n",
      "Iteration 201\n",
      "D loss: -84.81427001953125\n",
      "GP: 14.286942481994629\n",
      "Gradient norm: 1.3599051237106323\n",
      "G loss: -4.22551965713501\n",
      "Iteration 251\n",
      "D loss: -82.82731628417969\n",
      "GP: 15.186073303222656\n",
      "Gradient norm: 1.377251386642456\n",
      "G loss: -4.177265167236328\n",
      "Iteration 301\n",
      "D loss: -80.18318939208984\n",
      "GP: 12.902505874633789\n",
      "Gradient norm: 1.345371961593628\n",
      "G loss: -4.581193923950195\n",
      "Iteration 351\n",
      "D loss: -80.27114868164062\n",
      "GP: 13.404132843017578\n",
      "Gradient norm: 1.3514022827148438\n",
      "G loss: -4.676007270812988\n",
      "\n",
      "Epoch 6\n",
      "Iteration 1\n",
      "D loss: -78.89082336425781\n",
      "GP: 10.920219421386719\n",
      "Gradient norm: 1.3156940937042236\n",
      "G loss: -5.267555236816406\n",
      "Iteration 51\n",
      "D loss: -79.75125885009766\n",
      "GP: 10.838995933532715\n",
      "Gradient norm: 1.319268822669983\n",
      "G loss: -5.296205997467041\n",
      "Iteration 101\n",
      "D loss: -78.88292694091797\n",
      "GP: 10.595545768737793\n",
      "Gradient norm: 1.3082484006881714\n",
      "G loss: -5.841742038726807\n",
      "Iteration 151\n",
      "D loss: -75.81188201904297\n",
      "GP: 12.218048095703125\n",
      "Gradient norm: 1.3356894254684448\n",
      "G loss: -5.978842258453369\n",
      "Iteration 201\n",
      "D loss: -78.33235168457031\n",
      "GP: 10.490058898925781\n",
      "Gradient norm: 1.3116099834442139\n",
      "G loss: -6.7060370445251465\n",
      "Iteration 251\n",
      "D loss: -75.61418914794922\n",
      "GP: 9.874802589416504\n",
      "Gradient norm: 1.3028837442398071\n",
      "G loss: -7.394242286682129\n",
      "Iteration 301\n",
      "D loss: -76.46118927001953\n",
      "GP: 10.312398910522461\n",
      "Gradient norm: 1.3118340969085693\n",
      "G loss: -7.6368303298950195\n",
      "Iteration 351\n",
      "D loss: -72.74663543701172\n",
      "GP: 9.93107795715332\n",
      "Gradient norm: 1.3060380220413208\n",
      "G loss: -8.26025676727295\n",
      "\n",
      "Epoch 7\n",
      "Iteration 1\n",
      "D loss: -72.79900360107422\n",
      "GP: 9.638208389282227\n",
      "Gradient norm: 1.3008356094360352\n",
      "G loss: -8.827373504638672\n",
      "Iteration 51\n",
      "D loss: -76.53350067138672\n",
      "GP: 11.627508163452148\n",
      "Gradient norm: 1.3334457874298096\n",
      "G loss: -9.208395004272461\n",
      "Iteration 101\n",
      "D loss: -72.4513931274414\n",
      "GP: 10.103822708129883\n",
      "Gradient norm: 1.3093287944793701\n",
      "G loss: -9.610897064208984\n",
      "Iteration 151\n",
      "D loss: -76.27705383300781\n",
      "GP: 12.13509750366211\n",
      "Gradient norm: 1.3427155017852783\n",
      "G loss: -9.612200736999512\n",
      "Iteration 201\n",
      "D loss: -72.52825927734375\n",
      "GP: 10.763731956481934\n",
      "Gradient norm: 1.322165608406067\n",
      "G loss: -10.51989459991455\n",
      "Iteration 251\n",
      "D loss: -71.8669204711914\n",
      "GP: 9.745233535766602\n",
      "Gradient norm: 1.305694341659546\n",
      "G loss: -10.523004531860352\n",
      "Iteration 301\n",
      "D loss: -69.32229614257812\n",
      "GP: 10.516752243041992\n",
      "Gradient norm: 1.3179165124893188\n",
      "G loss: -11.219996452331543\n",
      "Iteration 351\n",
      "D loss: -67.76791381835938\n",
      "GP: 11.652426719665527\n",
      "Gradient norm: 1.334411382675171\n",
      "G loss: -11.7529935836792\n",
      "\n",
      "Epoch 8\n",
      "Iteration 1\n",
      "D loss: -69.68704223632812\n",
      "GP: 9.565845489501953\n",
      "Gradient norm: 1.300634741783142\n",
      "G loss: -12.23547649383545\n",
      "Iteration 51\n",
      "D loss: -69.11079406738281\n",
      "GP: 8.656283378601074\n",
      "Gradient norm: 1.2861082553863525\n",
      "G loss: -12.446639060974121\n",
      "Iteration 101\n",
      "D loss: -69.44558715820312\n",
      "GP: 9.926405906677246\n",
      "Gradient norm: 1.3075709342956543\n",
      "G loss: -12.678672790527344\n",
      "Iteration 151\n",
      "D loss: -67.4576416015625\n",
      "GP: 10.095291137695312\n",
      "Gradient norm: 1.3105820417404175\n",
      "G loss: -13.110580444335938\n",
      "Iteration 201\n",
      "D loss: -66.87163543701172\n",
      "GP: 9.60682487487793\n",
      "Gradient norm: 1.3045575618743896\n",
      "G loss: -13.746596336364746\n",
      "Iteration 251\n",
      "D loss: -73.47694396972656\n",
      "GP: 10.438933372497559\n",
      "Gradient norm: 1.3164730072021484\n",
      "G loss: -14.056221961975098\n",
      "Iteration 301\n",
      "D loss: -68.14219665527344\n",
      "GP: 9.815034866333008\n",
      "Gradient norm: 1.3060160875320435\n",
      "G loss: -13.729153633117676\n",
      "Iteration 351\n",
      "D loss: -67.58357238769531\n",
      "GP: 8.390220642089844\n",
      "Gradient norm: 1.2816345691680908\n",
      "G loss: -14.389134407043457\n",
      "\n",
      "Epoch 9\n",
      "Iteration 1\n",
      "D loss: -67.1302719116211\n",
      "GP: 7.381181716918945\n",
      "Gradient norm: 1.2651948928833008\n",
      "G loss: -14.51225757598877\n",
      "Iteration 51\n",
      "D loss: -64.82231140136719\n",
      "GP: 10.742220878601074\n",
      "Gradient norm: 1.3221349716186523\n",
      "G loss: -15.491698265075684\n",
      "Iteration 101\n",
      "D loss: -67.11628723144531\n",
      "GP: 8.625542640686035\n",
      "Gradient norm: 1.288272500038147\n",
      "G loss: -14.672334671020508\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 151\n",
      "D loss: -65.1593246459961\n",
      "GP: 9.863951683044434\n",
      "Gradient norm: 1.3091100454330444\n",
      "G loss: -15.0313081741333\n",
      "Iteration 201\n",
      "D loss: -65.1894760131836\n",
      "GP: 8.582757949829102\n",
      "Gradient norm: 1.283848524093628\n",
      "G loss: -15.342292785644531\n",
      "Iteration 251\n",
      "D loss: -66.68062591552734\n",
      "GP: 8.54896068572998\n",
      "Gradient norm: 1.2871595621109009\n",
      "G loss: -15.861376762390137\n",
      "Iteration 301\n",
      "D loss: -64.20196533203125\n",
      "GP: 9.024335861206055\n",
      "Gradient norm: 1.2928731441497803\n",
      "G loss: -15.880017280578613\n",
      "Iteration 351\n",
      "D loss: -65.07984161376953\n",
      "GP: 8.75902271270752\n",
      "Gradient norm: 1.2897602319717407\n",
      "G loss: -16.492124557495117\n",
      "\n",
      "Epoch 10\n",
      "Iteration 1\n",
      "D loss: -59.97126770019531\n",
      "GP: 7.970344543457031\n",
      "Gradient norm: 1.2747958898544312\n",
      "G loss: -16.55952262878418\n",
      "Iteration 51\n",
      "D loss: -67.02168273925781\n",
      "GP: 7.251346111297607\n",
      "Gradient norm: 1.2621158361434937\n",
      "G loss: -16.75457000732422\n",
      "Iteration 101\n",
      "D loss: -64.18672943115234\n",
      "GP: 8.084830284118652\n",
      "Gradient norm: 1.2771070003509521\n",
      "G loss: -16.802202224731445\n",
      "Iteration 151\n",
      "D loss: -63.398841857910156\n",
      "GP: 8.196211814880371\n",
      "Gradient norm: 1.2785825729370117\n",
      "G loss: -17.57362174987793\n",
      "Iteration 201\n",
      "D loss: -61.408416748046875\n",
      "GP: 7.3476104736328125\n",
      "Gradient norm: 1.2636798620224\n",
      "G loss: -17.83028793334961\n",
      "Iteration 251\n",
      "D loss: -60.831539154052734\n",
      "GP: 8.380573272705078\n",
      "Gradient norm: 1.2817963361740112\n",
      "G loss: -17.380369186401367\n",
      "Iteration 301\n",
      "D loss: -63.04011917114258\n",
      "GP: 6.696314334869385\n",
      "Gradient norm: 1.2487560510635376\n",
      "G loss: -18.24816131591797\n",
      "Iteration 351\n",
      "D loss: -61.2016716003418\n",
      "GP: 7.387829303741455\n",
      "Gradient norm: 1.2634575366973877\n",
      "G loss: -17.25420379638672\n",
      "\n",
      "Epoch 11\n",
      "Iteration 1\n",
      "D loss: -63.25257873535156\n",
      "GP: 7.761735439300537\n",
      "Gradient norm: 1.2697808742523193\n",
      "G loss: -18.417509078979492\n",
      "Iteration 51\n",
      "D loss: -62.69618606567383\n",
      "GP: 8.162296295166016\n",
      "Gradient norm: 1.2776848077774048\n",
      "G loss: -17.994152069091797\n",
      "Iteration 101\n",
      "D loss: -61.616764068603516\n",
      "GP: 7.5111894607543945\n",
      "Gradient norm: 1.262734293937683\n",
      "G loss: -18.015277862548828\n",
      "Iteration 151\n",
      "D loss: -61.35603332519531\n",
      "GP: 8.157470703125\n",
      "Gradient norm: 1.2765361070632935\n",
      "G loss: -17.68337631225586\n",
      "Iteration 201\n",
      "D loss: -58.39173889160156\n",
      "GP: 7.501191139221191\n",
      "Gradient norm: 1.265696406364441\n",
      "G loss: -18.3367977142334\n",
      "Iteration 251\n",
      "D loss: -62.9393310546875\n",
      "GP: 8.253829956054688\n",
      "Gradient norm: 1.2779483795166016\n",
      "G loss: -18.813390731811523\n",
      "Iteration 301\n",
      "D loss: -59.94681167602539\n",
      "GP: 7.7452192306518555\n",
      "Gradient norm: 1.2697503566741943\n",
      "G loss: -18.338973999023438\n",
      "Iteration 351\n",
      "D loss: -60.639610290527344\n",
      "GP: 8.160263061523438\n",
      "Gradient norm: 1.2769525051116943\n",
      "G loss: -18.146406173706055\n",
      "\n",
      "Epoch 12\n",
      "Iteration 1\n",
      "D loss: -60.096343994140625\n",
      "GP: 7.812852382659912\n",
      "Gradient norm: 1.2708635330200195\n",
      "G loss: -18.947065353393555\n",
      "Iteration 51\n",
      "D loss: -60.85066223144531\n",
      "GP: 7.233720779418945\n",
      "Gradient norm: 1.2588821649551392\n",
      "G loss: -19.513324737548828\n",
      "Iteration 101\n",
      "D loss: -60.121524810791016\n",
      "GP: 8.199488639831543\n",
      "Gradient norm: 1.276786208152771\n",
      "G loss: -19.677602767944336\n",
      "Iteration 151\n",
      "D loss: -59.40294647216797\n",
      "GP: 9.10594367980957\n",
      "Gradient norm: 1.2924433946609497\n",
      "G loss: -18.82331657409668\n",
      "Iteration 201\n",
      "D loss: -58.82615661621094\n",
      "GP: 7.246942520141602\n",
      "Gradient norm: 1.2608681917190552\n",
      "G loss: -19.36631202697754\n",
      "Iteration 251\n",
      "D loss: -57.28732681274414\n",
      "GP: 7.854214668273926\n",
      "Gradient norm: 1.2721583843231201\n",
      "G loss: -19.518930435180664\n",
      "Iteration 301\n",
      "D loss: -58.207210540771484\n",
      "GP: 7.364109039306641\n",
      "Gradient norm: 1.2626529932022095\n",
      "G loss: -19.17563819885254\n",
      "Iteration 351\n",
      "D loss: -57.763145446777344\n",
      "GP: 9.004518508911133\n",
      "Gradient norm: 1.293518304824829\n",
      "G loss: -19.380084991455078\n",
      "\n",
      "Epoch 13\n",
      "Iteration 1\n",
      "D loss: -58.453678131103516\n",
      "GP: 8.302777290344238\n",
      "Gradient norm: 1.2794300317764282\n",
      "G loss: -19.183963775634766\n",
      "Iteration 51\n",
      "D loss: -57.094242095947266\n",
      "GP: 7.121851921081543\n",
      "Gradient norm: 1.2586301565170288\n",
      "G loss: -19.240859985351562\n",
      "Iteration 101\n",
      "D loss: -57.225955963134766\n",
      "GP: 6.470005989074707\n",
      "Gradient norm: 1.2439874410629272\n",
      "G loss: -20.379436492919922\n",
      "Iteration 151\n",
      "D loss: -58.95164489746094\n",
      "GP: 7.506479263305664\n",
      "Gradient norm: 1.2647064924240112\n",
      "G loss: -19.335262298583984\n",
      "Iteration 201\n",
      "D loss: -59.935142517089844\n",
      "GP: 6.1207427978515625\n",
      "Gradient norm: 1.2373408079147339\n",
      "G loss: -19.583755493164062\n",
      "Iteration 251\n",
      "D loss: -58.58513259887695\n",
      "GP: 7.063534736633301\n",
      "Gradient norm: 1.2568551301956177\n",
      "G loss: -19.702627182006836\n",
      "Iteration 301\n",
      "D loss: -56.37554931640625\n",
      "GP: 6.9977707862854\n",
      "Gradient norm: 1.2555114030838013\n",
      "G loss: -19.4169921875\n",
      "Iteration 351\n",
      "D loss: -56.51702880859375\n",
      "GP: 6.6513848304748535\n",
      "Gradient norm: 1.2476074695587158\n",
      "G loss: -18.917150497436523\n",
      "\n",
      "Epoch 14\n",
      "Iteration 1\n",
      "D loss: -56.12446594238281\n",
      "GP: 6.382340908050537\n",
      "Gradient norm: 1.242821216583252\n",
      "G loss: -19.150358200073242\n",
      "Iteration 51\n",
      "D loss: -56.51495361328125\n",
      "GP: 6.957215309143066\n",
      "Gradient norm: 1.2543489933013916\n",
      "G loss: -19.032325744628906\n",
      "Iteration 101\n",
      "D loss: -57.54552459716797\n",
      "GP: 6.249564170837402\n",
      "Gradient norm: 1.238891839981079\n",
      "G loss: -18.844623565673828\n",
      "Iteration 151\n",
      "D loss: -55.88529586791992\n",
      "GP: 7.8964996337890625\n",
      "Gradient norm: 1.2711188793182373\n",
      "G loss: -19.92374610900879\n",
      "Iteration 201\n",
      "D loss: -56.37504959106445\n",
      "GP: 6.2934370040893555\n",
      "Gradient norm: 1.2387583255767822\n",
      "G loss: -19.26247215270996\n",
      "Iteration 251\n",
      "D loss: -54.362525939941406\n",
      "GP: 7.918138027191162\n",
      "Gradient norm: 1.2727285623550415\n",
      "G loss: -19.892473220825195\n",
      "Iteration 301\n",
      "D loss: -56.40012741088867\n",
      "GP: 7.162983417510986\n",
      "Gradient norm: 1.2578034400939941\n",
      "G loss: -20.026554107666016\n",
      "Iteration 351\n",
      "D loss: -60.38042449951172\n",
      "GP: 6.82850456237793\n",
      "Gradient norm: 1.2509115934371948\n",
      "G loss: -19.027156829833984\n",
      "\n",
      "Epoch 15\n",
      "Iteration 1\n",
      "D loss: -56.44072723388672\n",
      "GP: 6.624961853027344\n",
      "Gradient norm: 1.246997356414795\n",
      "G loss: -18.910419464111328\n",
      "Iteration 51\n",
      "D loss: -55.67581558227539\n",
      "GP: 8.856488227844238\n",
      "Gradient norm: 1.2900103330612183\n",
      "G loss: -19.1180477142334\n",
      "Iteration 101\n",
      "D loss: -56.95214080810547\n",
      "GP: 6.207481384277344\n",
      "Gradient norm: 1.2367212772369385\n",
      "G loss: -19.277408599853516\n",
      "Iteration 151\n",
      "D loss: -54.74584197998047\n",
      "GP: 5.159008979797363\n",
      "Gradient norm: 1.2122466564178467\n",
      "G loss: -18.906776428222656\n",
      "Iteration 201\n",
      "D loss: -56.742286682128906\n",
      "GP: 6.816628456115723\n",
      "Gradient norm: 1.2512407302856445\n",
      "G loss: -18.61606788635254\n",
      "Iteration 251\n",
      "D loss: -54.440574645996094\n",
      "GP: 6.51113748550415\n",
      "Gradient norm: 1.2439762353897095\n",
      "G loss: -18.755605697631836\n",
      "Iteration 301\n",
      "D loss: -55.89140701293945\n",
      "GP: 5.758068561553955\n",
      "Gradient norm: 1.2300790548324585\n",
      "G loss: -19.5944881439209\n",
      "Iteration 351\n",
      "D loss: -51.7779426574707\n",
      "GP: 6.237229824066162\n",
      "Gradient norm: 1.2394487857818604\n",
      "G loss: -19.129493713378906\n",
      "\n",
      "Epoch 16\n",
      "Iteration 1\n",
      "D loss: -54.589359283447266\n",
      "GP: 6.821019172668457\n",
      "Gradient norm: 1.2502901554107666\n",
      "G loss: -18.912837982177734\n",
      "Iteration 51\n",
      "D loss: -54.41315460205078\n",
      "GP: 6.267982006072998\n",
      "Gradient norm: 1.2392157316207886\n",
      "G loss: -18.960283279418945\n",
      "Iteration 101\n",
      "D loss: -53.20350646972656\n",
      "GP: 6.29880428314209\n",
      "Gradient norm: 1.2397027015686035\n",
      "G loss: -18.448589324951172\n",
      "Iteration 151\n",
      "D loss: -54.01384735107422\n",
      "GP: 5.523102760314941\n",
      "Gradient norm: 1.2217178344726562\n",
      "G loss: -17.3538875579834\n",
      "Iteration 201\n",
      "D loss: -54.27086639404297\n",
      "GP: 6.187561988830566\n",
      "Gradient norm: 1.2346028089523315\n",
      "G loss: -18.26243019104004\n",
      "Iteration 251\n",
      "D loss: -53.8057861328125\n",
      "GP: 6.152102470397949\n",
      "Gradient norm: 1.2341060638427734\n",
      "G loss: -18.049577713012695\n",
      "Iteration 301\n",
      "D loss: -55.25214385986328\n",
      "GP: 7.167229652404785\n",
      "Gradient norm: 1.2548420429229736\n",
      "G loss: -19.34235954284668\n",
      "Iteration 351\n",
      "D loss: -53.988685607910156\n",
      "GP: 6.329828262329102\n",
      "Gradient norm: 1.2395780086517334\n",
      "G loss: -18.267200469970703\n",
      "\n",
      "Epoch 17\n",
      "Iteration 1\n",
      "D loss: -54.80180740356445\n",
      "GP: 6.002492427825928\n",
      "Gradient norm: 1.2335091829299927\n",
      "G loss: -18.856250762939453\n",
      "Iteration 51\n",
      "D loss: -56.910133361816406\n",
      "GP: 6.634583950042725\n",
      "Gradient norm: 1.2444629669189453\n",
      "G loss: -19.143884658813477\n",
      "Iteration 101\n",
      "D loss: -54.18212890625\n",
      "GP: 6.163312911987305\n",
      "Gradient norm: 1.2342511415481567\n",
      "G loss: -18.527109146118164\n",
      "Iteration 151\n",
      "D loss: -55.49127197265625\n",
      "GP: 6.659218788146973\n",
      "Gradient norm: 1.2447682619094849\n",
      "G loss: -18.15388298034668\n",
      "Iteration 201\n",
      "D loss: -54.61547088623047\n",
      "GP: 6.273458003997803\n",
      "Gradient norm: 1.2366381883621216\n",
      "G loss: -18.585994720458984\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 251\n",
      "D loss: -53.653831481933594\n",
      "GP: 6.495847702026367\n",
      "Gradient norm: 1.238140344619751\n",
      "G loss: -18.223907470703125\n",
      "Iteration 301\n",
      "D loss: -54.138214111328125\n",
      "GP: 6.134767055511475\n",
      "Gradient norm: 1.2375003099441528\n",
      "G loss: -17.31282615661621\n",
      "Iteration 351\n",
      "D loss: -52.14564514160156\n",
      "GP: 6.398934841156006\n",
      "Gradient norm: 1.2389410734176636\n",
      "G loss: -17.678499221801758\n",
      "\n",
      "Epoch 18\n",
      "Iteration 1\n",
      "D loss: -52.708465576171875\n",
      "GP: 5.821783542633057\n",
      "Gradient norm: 1.2241278886795044\n",
      "G loss: -16.807924270629883\n",
      "Iteration 51\n",
      "D loss: -52.71531295776367\n",
      "GP: 6.056720733642578\n",
      "Gradient norm: 1.2328661680221558\n",
      "G loss: -18.0438289642334\n",
      "Iteration 101\n",
      "D loss: -51.19392776489258\n",
      "GP: 6.7932820320129395\n",
      "Gradient norm: 1.2476139068603516\n",
      "G loss: -18.088966369628906\n",
      "Iteration 151\n",
      "D loss: -51.52122116088867\n",
      "GP: 7.115900993347168\n",
      "Gradient norm: 1.252652883529663\n",
      "G loss: -17.49942970275879\n",
      "Iteration 201\n",
      "D loss: -51.130943298339844\n",
      "GP: 5.453573226928711\n",
      "Gradient norm: 1.2187005281448364\n",
      "G loss: -16.813199996948242\n",
      "Iteration 251\n",
      "D loss: -53.12256622314453\n",
      "GP: 6.155648231506348\n",
      "Gradient norm: 1.2325607538223267\n",
      "G loss: -16.776073455810547\n",
      "Iteration 301\n",
      "D loss: -54.05804443359375\n",
      "GP: 5.962186813354492\n",
      "Gradient norm: 1.2267757654190063\n",
      "G loss: -16.468259811401367\n",
      "Iteration 351\n",
      "D loss: -51.671409606933594\n",
      "GP: 6.78764533996582\n",
      "Gradient norm: 1.2446807622909546\n",
      "G loss: -16.337995529174805\n",
      "\n",
      "Epoch 19\n",
      "Iteration 1\n",
      "D loss: -54.07011795043945\n",
      "GP: 6.461764335632324\n",
      "Gradient norm: 1.240772008895874\n",
      "G loss: -16.45128631591797\n",
      "Iteration 51\n",
      "D loss: -51.84429168701172\n",
      "GP: 6.022782325744629\n",
      "Gradient norm: 1.2288354635238647\n",
      "G loss: -15.476703643798828\n",
      "Iteration 101\n",
      "D loss: -53.936622619628906\n",
      "GP: 6.351005554199219\n",
      "Gradient norm: 1.2346733808517456\n",
      "G loss: -16.11681365966797\n",
      "Iteration 151\n",
      "D loss: -50.9221076965332\n",
      "GP: 6.048342704772949\n",
      "Gradient norm: 1.2310106754302979\n",
      "G loss: -15.278125762939453\n",
      "Iteration 201\n",
      "D loss: -54.38344192504883\n",
      "GP: 5.86699914932251\n",
      "Gradient norm: 1.2239265441894531\n",
      "G loss: -15.914207458496094\n",
      "Iteration 251\n",
      "D loss: -52.66375732421875\n",
      "GP: 6.928435325622559\n",
      "Gradient norm: 1.2487778663635254\n",
      "G loss: -14.863478660583496\n",
      "Iteration 301\n",
      "D loss: -49.881256103515625\n",
      "GP: 5.020264625549316\n",
      "Gradient norm: 1.204920768737793\n",
      "G loss: -14.855031967163086\n",
      "Iteration 351\n",
      "D loss: -51.87181854248047\n",
      "GP: 5.769720554351807\n",
      "Gradient norm: 1.224605679512024\n",
      "G loss: -15.283160209655762\n",
      "\n",
      "Epoch 20\n",
      "Iteration 1\n",
      "D loss: -52.10773468017578\n",
      "GP: 4.480783462524414\n",
      "Gradient norm: 1.1943004131317139\n",
      "G loss: -14.823808670043945\n",
      "Iteration 51\n",
      "D loss: -50.37963104248047\n",
      "GP: 6.288384437561035\n",
      "Gradient norm: 1.2315917015075684\n",
      "G loss: -15.435422897338867\n",
      "Iteration 101\n",
      "D loss: -52.93781280517578\n",
      "GP: 6.129133701324463\n",
      "Gradient norm: 1.2296319007873535\n",
      "G loss: -16.370519638061523\n",
      "Iteration 151\n",
      "D loss: -50.578826904296875\n",
      "GP: 4.886440277099609\n",
      "Gradient norm: 1.204012393951416\n",
      "G loss: -14.397706031799316\n",
      "Iteration 201\n",
      "D loss: -51.26096725463867\n",
      "GP: 6.365302085876465\n",
      "Gradient norm: 1.2367656230926514\n",
      "G loss: -14.029346466064453\n",
      "Iteration 251\n",
      "D loss: -51.58649444580078\n",
      "GP: 5.725722789764404\n",
      "Gradient norm: 1.2163608074188232\n",
      "G loss: -13.70009708404541\n",
      "Iteration 301\n",
      "D loss: -51.39632797241211\n",
      "GP: 5.467835426330566\n",
      "Gradient norm: 1.2179110050201416\n",
      "G loss: -13.785398483276367\n",
      "Iteration 351\n",
      "D loss: -50.580848693847656\n",
      "GP: 6.1124348640441895\n",
      "Gradient norm: 1.2324907779693604\n",
      "G loss: -13.901037216186523\n",
      "\n",
      "Epoch 21\n",
      "Iteration 1\n",
      "D loss: -52.285160064697266\n",
      "GP: 5.798518657684326\n",
      "Gradient norm: 1.2237457036972046\n",
      "G loss: -13.489554405212402\n",
      "Iteration 51\n",
      "D loss: -52.2082405090332\n",
      "GP: 5.418659210205078\n",
      "Gradient norm: 1.2130964994430542\n",
      "G loss: -13.059810638427734\n",
      "Iteration 101\n",
      "D loss: -53.1478385925293\n",
      "GP: 5.255855560302734\n",
      "Gradient norm: 1.2105687856674194\n",
      "G loss: -13.377516746520996\n",
      "Iteration 151\n",
      "D loss: -52.00859451293945\n",
      "GP: 5.439342975616455\n",
      "Gradient norm: 1.213276743888855\n",
      "G loss: -14.290273666381836\n",
      "Iteration 201\n",
      "D loss: -49.85502624511719\n",
      "GP: 5.143347263336182\n",
      "Gradient norm: 1.2103872299194336\n",
      "G loss: -13.579286575317383\n",
      "Iteration 251\n",
      "D loss: -50.53535461425781\n",
      "GP: 7.076165676116943\n",
      "Gradient norm: 1.245833158493042\n",
      "G loss: -13.838614463806152\n",
      "Iteration 301\n",
      "D loss: -50.139835357666016\n",
      "GP: 5.766548156738281\n",
      "Gradient norm: 1.224618673324585\n",
      "G loss: -13.225757598876953\n",
      "Iteration 351\n",
      "D loss: -53.16230010986328\n",
      "GP: 6.227173805236816\n",
      "Gradient norm: 1.2311395406723022\n",
      "G loss: -13.513778686523438\n",
      "\n",
      "Epoch 22\n",
      "Iteration 1\n",
      "D loss: -49.92727279663086\n",
      "GP: 5.893288612365723\n",
      "Gradient norm: 1.222260594367981\n",
      "G loss: -13.239521980285645\n",
      "Iteration 51\n",
      "D loss: -48.92658233642578\n",
      "GP: 5.4657883644104\n",
      "Gradient norm: 1.2106608152389526\n",
      "G loss: -12.689859390258789\n",
      "Iteration 101\n",
      "D loss: -49.733192443847656\n",
      "GP: 5.289831161499023\n",
      "Gradient norm: 1.204270839691162\n",
      "G loss: -12.70655345916748\n",
      "Iteration 151\n",
      "D loss: -51.97385787963867\n",
      "GP: 5.8528337478637695\n",
      "Gradient norm: 1.2152174711227417\n",
      "G loss: -12.657644271850586\n",
      "Iteration 201\n",
      "D loss: -51.34796905517578\n",
      "GP: 6.416666030883789\n",
      "Gradient norm: 1.2307758331298828\n",
      "G loss: -13.240859031677246\n",
      "Iteration 251\n",
      "D loss: -50.9675178527832\n",
      "GP: 6.55794620513916\n",
      "Gradient norm: 1.2313153743743896\n",
      "G loss: -13.001354217529297\n",
      "Iteration 301\n",
      "D loss: -51.201576232910156\n",
      "GP: 5.97041130065918\n",
      "Gradient norm: 1.2216166257858276\n",
      "G loss: -12.492538452148438\n",
      "Iteration 351\n",
      "D loss: -50.26263427734375\n",
      "GP: 5.21329402923584\n",
      "Gradient norm: 1.205384373664856\n",
      "G loss: -13.011656761169434\n",
      "\n",
      "Epoch 23\n",
      "Iteration 1\n",
      "D loss: -51.97647476196289\n",
      "GP: 5.384776592254639\n",
      "Gradient norm: 1.201907753944397\n",
      "G loss: -13.067668914794922\n",
      "Iteration 51\n",
      "D loss: -50.220855712890625\n",
      "GP: 5.643998146057129\n",
      "Gradient norm: 1.21294105052948\n",
      "G loss: -12.752891540527344\n",
      "Iteration 101\n",
      "D loss: -51.46636199951172\n",
      "GP: 4.968303203582764\n",
      "Gradient norm: 1.1949210166931152\n",
      "G loss: -13.003934860229492\n",
      "Iteration 151\n",
      "D loss: -51.915706634521484\n",
      "GP: 6.861250877380371\n",
      "Gradient norm: 1.239118218421936\n",
      "G loss: -12.407018661499023\n",
      "Iteration 201\n",
      "D loss: -49.981361389160156\n",
      "GP: 7.583974838256836\n",
      "Gradient norm: 1.2426588535308838\n",
      "G loss: -12.953259468078613\n",
      "Iteration 251\n",
      "D loss: -48.924949645996094\n",
      "GP: 6.6841721534729\n",
      "Gradient norm: 1.2335546016693115\n",
      "G loss: -12.356904983520508\n",
      "Iteration 301\n",
      "D loss: -49.93075942993164\n",
      "GP: 6.29011869430542\n",
      "Gradient norm: 1.226362705230713\n",
      "G loss: -11.414384841918945\n",
      "Iteration 351\n",
      "D loss: -51.14217758178711\n",
      "GP: 6.545397758483887\n",
      "Gradient norm: 1.2270221710205078\n",
      "G loss: -11.434117317199707\n",
      "\n",
      "Epoch 24\n",
      "Iteration 1\n",
      "D loss: -49.5285758972168\n",
      "GP: 6.034947395324707\n",
      "Gradient norm: 1.2142225503921509\n",
      "G loss: -11.511066436767578\n",
      "Iteration 51\n",
      "D loss: -51.19552993774414\n",
      "GP: 6.5051679611206055\n",
      "Gradient norm: 1.22544264793396\n",
      "G loss: -11.335765838623047\n",
      "Iteration 101\n",
      "D loss: -50.48316955566406\n",
      "GP: 5.025978088378906\n",
      "Gradient norm: 1.1998233795166016\n",
      "G loss: -10.5761137008667\n",
      "Iteration 151\n",
      "D loss: -49.89109802246094\n",
      "GP: 7.085493087768555\n",
      "Gradient norm: 1.2422306537628174\n",
      "G loss: -10.398965835571289\n",
      "Iteration 201\n",
      "D loss: -50.2896728515625\n",
      "GP: 6.583526134490967\n",
      "Gradient norm: 1.2295304536819458\n",
      "G loss: -10.397869110107422\n",
      "Iteration 251\n",
      "D loss: -48.56891632080078\n",
      "GP: 6.71314811706543\n",
      "Gradient norm: 1.2367479801177979\n",
      "G loss: -10.257159233093262\n",
      "Iteration 301\n",
      "D loss: -50.416866302490234\n",
      "GP: 7.551405429840088\n",
      "Gradient norm: 1.248350977897644\n",
      "G loss: -9.542747497558594\n",
      "Iteration 351\n",
      "D loss: -48.817020416259766\n",
      "GP: 6.312755584716797\n",
      "Gradient norm: 1.2252756357192993\n",
      "G loss: -10.413518905639648\n",
      "\n",
      "Epoch 25\n",
      "Iteration 1\n",
      "D loss: -49.55704116821289\n",
      "GP: 4.5547003746032715\n",
      "Gradient norm: 1.1852420568466187\n",
      "G loss: -9.068574905395508\n",
      "Iteration 51\n",
      "D loss: -47.65301513671875\n",
      "GP: 6.905823230743408\n",
      "Gradient norm: 1.234764814376831\n",
      "G loss: -9.239272117614746\n",
      "Iteration 101\n",
      "D loss: -49.99235916137695\n",
      "GP: 5.85503625869751\n",
      "Gradient norm: 1.2146711349487305\n",
      "G loss: -10.116775512695312\n",
      "Iteration 151\n",
      "D loss: -52.02952194213867\n",
      "GP: 6.248158931732178\n",
      "Gradient norm: 1.2292016744613647\n",
      "G loss: -10.06827449798584\n",
      "Iteration 201\n",
      "D loss: -50.46167755126953\n",
      "GP: 5.973442077636719\n",
      "Gradient norm: 1.2105817794799805\n",
      "G loss: -9.38227367401123\n",
      "Iteration 251\n",
      "D loss: -50.67779541015625\n",
      "GP: 7.822233200073242\n",
      "Gradient norm: 1.2526870965957642\n",
      "G loss: -8.657382011413574\n",
      "Iteration 301\n",
      "D loss: -49.01924514770508\n",
      "GP: 6.821148872375488\n",
      "Gradient norm: 1.2370147705078125\n",
      "G loss: -9.201292037963867\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 351\n",
      "D loss: -49.265445709228516\n",
      "GP: 5.853763580322266\n",
      "Gradient norm: 1.2113537788391113\n",
      "G loss: -9.230545997619629\n",
      "\n",
      "Epoch 26\n",
      "Iteration 1\n",
      "D loss: -50.99070739746094\n",
      "GP: 6.747167587280273\n",
      "Gradient norm: 1.2326064109802246\n",
      "G loss: -8.44346809387207\n",
      "Iteration 51\n",
      "D loss: -50.12142562866211\n",
      "GP: 5.638730525970459\n",
      "Gradient norm: 1.207950472831726\n",
      "G loss: -8.393192291259766\n",
      "Iteration 101\n",
      "D loss: -53.22245407104492\n",
      "GP: 5.7524847984313965\n",
      "Gradient norm: 1.2085716724395752\n",
      "G loss: -8.538941383361816\n",
      "Iteration 151\n",
      "D loss: -52.30476760864258\n",
      "GP: 6.216907024383545\n",
      "Gradient norm: 1.2197751998901367\n",
      "G loss: -7.737833499908447\n",
      "Iteration 201\n",
      "D loss: -50.82429122924805\n",
      "GP: 6.28706693649292\n",
      "Gradient norm: 1.2261178493499756\n",
      "G loss: -8.06147289276123\n",
      "Iteration 251\n",
      "D loss: -51.1131706237793\n",
      "GP: 7.510578155517578\n",
      "Gradient norm: 1.2506517171859741\n",
      "G loss: -9.247631072998047\n",
      "Iteration 301\n",
      "D loss: -50.78983688354492\n",
      "GP: 6.52589750289917\n",
      "Gradient norm: 1.2260773181915283\n",
      "G loss: -8.562350273132324\n",
      "Iteration 351\n",
      "D loss: -51.81684112548828\n",
      "GP: 5.672054290771484\n",
      "Gradient norm: 1.2078955173492432\n",
      "G loss: -8.366410255432129\n",
      "\n",
      "Epoch 27\n",
      "Iteration 1\n",
      "D loss: -50.02336883544922\n",
      "GP: 6.4499406814575195\n",
      "Gradient norm: 1.2247592210769653\n",
      "G loss: -7.867384433746338\n",
      "Iteration 51\n",
      "D loss: -49.50672149658203\n",
      "GP: 6.754726409912109\n",
      "Gradient norm: 1.2335940599441528\n",
      "G loss: -7.455718994140625\n",
      "Iteration 101\n",
      "D loss: -49.37358093261719\n",
      "GP: 6.1130194664001465\n",
      "Gradient norm: 1.2034740447998047\n",
      "G loss: -7.055636405944824\n",
      "Iteration 151\n",
      "D loss: -51.0113410949707\n",
      "GP: 6.947786808013916\n",
      "Gradient norm: 1.2343592643737793\n",
      "G loss: -6.721584320068359\n",
      "Iteration 201\n",
      "D loss: -51.7497673034668\n",
      "GP: 6.7415642738342285\n",
      "Gradient norm: 1.2282354831695557\n",
      "G loss: -7.455096244812012\n",
      "Iteration 251\n",
      "D loss: -50.103240966796875\n",
      "GP: 8.128079414367676\n",
      "Gradient norm: 1.2586089372634888\n",
      "G loss: -6.096939563751221\n",
      "Iteration 301\n",
      "D loss: -52.13356399536133\n",
      "GP: 6.263481140136719\n",
      "Gradient norm: 1.217816948890686\n",
      "G loss: -6.547340393066406\n",
      "Iteration 351\n",
      "D loss: -50.5031623840332\n",
      "GP: 6.376529216766357\n",
      "Gradient norm: 1.217792272567749\n",
      "G loss: -7.285845756530762\n",
      "\n",
      "Epoch 28\n",
      "Iteration 1\n",
      "D loss: -51.63369369506836\n",
      "GP: 7.115477561950684\n",
      "Gradient norm: 1.2348397970199585\n",
      "G loss: -5.814676284790039\n",
      "Iteration 51\n",
      "D loss: -50.21866226196289\n",
      "GP: 5.48244047164917\n",
      "Gradient norm: 1.2031148672103882\n",
      "G loss: -5.58220100402832\n",
      "Iteration 101\n",
      "D loss: -49.313053131103516\n",
      "GP: 5.936165809631348\n",
      "Gradient norm: 1.2155134677886963\n",
      "G loss: -5.526333332061768\n",
      "Iteration 151\n",
      "D loss: -49.33594512939453\n",
      "GP: 5.899138450622559\n",
      "Gradient norm: 1.2093969583511353\n",
      "G loss: -5.7523512840271\n",
      "Iteration 201\n",
      "D loss: -50.455909729003906\n",
      "GP: 4.94045352935791\n",
      "Gradient norm: 1.1864529848098755\n",
      "G loss: -5.201357364654541\n",
      "Iteration 251\n",
      "D loss: -49.04018020629883\n",
      "GP: 6.950264930725098\n",
      "Gradient norm: 1.228495478630066\n",
      "G loss: -5.799295902252197\n",
      "Iteration 301\n",
      "D loss: -48.982730865478516\n",
      "GP: 7.435751914978027\n",
      "Gradient norm: 1.2425378561019897\n",
      "G loss: -4.697627544403076\n",
      "Iteration 351\n",
      "D loss: -48.94715118408203\n",
      "GP: 6.9844889640808105\n",
      "Gradient norm: 1.2302119731903076\n",
      "G loss: -4.985895156860352\n",
      "\n",
      "Epoch 29\n",
      "Iteration 1\n",
      "D loss: -49.22140121459961\n",
      "GP: 5.8224334716796875\n",
      "Gradient norm: 1.210128664970398\n",
      "G loss: -4.825971603393555\n",
      "Iteration 51\n",
      "D loss: -51.1900749206543\n",
      "GP: 6.419075012207031\n",
      "Gradient norm: 1.221055269241333\n",
      "G loss: -3.8365161418914795\n",
      "Iteration 101\n",
      "D loss: -49.719688415527344\n",
      "GP: 7.332050323486328\n",
      "Gradient norm: 1.2389812469482422\n",
      "G loss: -4.481815814971924\n",
      "Iteration 151\n",
      "D loss: -49.95240020751953\n",
      "GP: 5.726556301116943\n",
      "Gradient norm: 1.1985828876495361\n",
      "G loss: -3.6214170455932617\n",
      "Iteration 201\n",
      "D loss: -49.40215301513672\n",
      "GP: 8.126279830932617\n",
      "Gradient norm: 1.2496864795684814\n",
      "G loss: -3.779144763946533\n",
      "Iteration 251\n",
      "D loss: -49.227516174316406\n",
      "GP: 6.203947067260742\n",
      "Gradient norm: 1.2097078561782837\n",
      "G loss: -4.265254020690918\n",
      "Iteration 301\n",
      "D loss: -53.4997673034668\n",
      "GP: 6.346339225769043\n",
      "Gradient norm: 1.2129241228103638\n",
      "G loss: -3.9184863567352295\n",
      "Iteration 351\n",
      "D loss: -50.82398223876953\n",
      "GP: 5.350468158721924\n",
      "Gradient norm: 1.191552758216858\n",
      "G loss: -3.457237482070923\n",
      "\n",
      "Epoch 30\n",
      "Iteration 1\n",
      "D loss: -49.730125427246094\n",
      "GP: 6.023870468139648\n",
      "Gradient norm: 1.2029757499694824\n",
      "G loss: -3.9093472957611084\n",
      "Iteration 51\n",
      "D loss: -50.548309326171875\n",
      "GP: 6.168107986450195\n",
      "Gradient norm: 1.21534264087677\n",
      "G loss: -3.13236141204834\n",
      "Iteration 101\n",
      "D loss: -48.9183235168457\n",
      "GP: 6.083132743835449\n",
      "Gradient norm: 1.210588812828064\n",
      "G loss: -3.603214740753174\n",
      "Iteration 151\n",
      "D loss: -49.902366638183594\n",
      "GP: 7.217166423797607\n",
      "Gradient norm: 1.235361933708191\n",
      "G loss: -4.072249412536621\n",
      "Iteration 201\n",
      "D loss: -50.155311584472656\n",
      "GP: 6.972380638122559\n",
      "Gradient norm: 1.2339345216751099\n",
      "G loss: -4.5176849365234375\n",
      "Iteration 251\n",
      "D loss: -50.77241516113281\n",
      "GP: 6.233166694641113\n",
      "Gradient norm: 1.205996036529541\n",
      "G loss: -3.3048484325408936\n",
      "Iteration 301\n",
      "D loss: -49.23822021484375\n",
      "GP: 5.977075576782227\n",
      "Gradient norm: 1.2012276649475098\n",
      "G loss: -4.354661464691162\n",
      "Iteration 351\n",
      "D loss: -48.288997650146484\n",
      "GP: 6.6942925453186035\n",
      "Gradient norm: 1.2187650203704834\n",
      "G loss: -4.286186695098877\n",
      "\n",
      "Epoch 31\n",
      "Iteration 1\n",
      "D loss: -49.79448699951172\n",
      "GP: 8.044595718383789\n",
      "Gradient norm: 1.2444891929626465\n",
      "G loss: -5.011377334594727\n",
      "Iteration 51\n",
      "D loss: -47.81491470336914\n",
      "GP: 5.534598350524902\n",
      "Gradient norm: 1.1989110708236694\n",
      "G loss: -4.039997577667236\n",
      "Iteration 101\n",
      "D loss: -50.58633804321289\n",
      "GP: 5.4728264808654785\n",
      "Gradient norm: 1.198814868927002\n",
      "G loss: -3.9551734924316406\n",
      "Iteration 151\n",
      "D loss: -48.13329315185547\n",
      "GP: 8.824712753295898\n",
      "Gradient norm: 1.2668266296386719\n",
      "G loss: -5.078789234161377\n",
      "Iteration 201\n",
      "D loss: -48.551204681396484\n",
      "GP: 6.657405853271484\n",
      "Gradient norm: 1.2200357913970947\n",
      "G loss: -3.5996012687683105\n",
      "Iteration 251\n",
      "D loss: -49.931793212890625\n",
      "GP: 6.773345947265625\n",
      "Gradient norm: 1.222171425819397\n",
      "G loss: -3.754336357116699\n",
      "Iteration 301\n",
      "D loss: -49.883338928222656\n",
      "GP: 7.00825309753418\n",
      "Gradient norm: 1.2283477783203125\n",
      "G loss: -4.088850021362305\n",
      "Iteration 351\n",
      "D loss: -49.22503662109375\n",
      "GP: 7.6016035079956055\n",
      "Gradient norm: 1.240173578262329\n",
      "G loss: -2.895444631576538\n",
      "\n",
      "Epoch 32\n",
      "Iteration 1\n",
      "D loss: -47.712928771972656\n",
      "GP: 6.566926956176758\n",
      "Gradient norm: 1.2193602323532104\n",
      "G loss: -3.4153339862823486\n",
      "Iteration 51\n",
      "D loss: -48.04326248168945\n",
      "GP: 7.906467437744141\n",
      "Gradient norm: 1.2417490482330322\n",
      "G loss: -3.7690584659576416\n",
      "Iteration 101\n",
      "D loss: -47.85251998901367\n",
      "GP: 6.035452365875244\n",
      "Gradient norm: 1.2121706008911133\n",
      "G loss: -3.123303174972534\n",
      "Iteration 151\n",
      "D loss: -48.598941802978516\n",
      "GP: 5.989699840545654\n",
      "Gradient norm: 1.2104790210723877\n",
      "G loss: -3.2820870876312256\n",
      "Iteration 201\n",
      "D loss: -47.9134635925293\n",
      "GP: 6.940216064453125\n",
      "Gradient norm: 1.226114273071289\n",
      "G loss: -3.344294548034668\n",
      "Iteration 251\n",
      "D loss: -49.3058967590332\n",
      "GP: 5.450842380523682\n",
      "Gradient norm: 1.193864107131958\n",
      "G loss: -2.79925799369812\n",
      "Iteration 301\n",
      "D loss: -50.16303634643555\n",
      "GP: 6.916206359863281\n",
      "Gradient norm: 1.2327609062194824\n",
      "G loss: -2.547214984893799\n",
      "Iteration 351\n",
      "D loss: -50.30012512207031\n",
      "GP: 7.335717678070068\n",
      "Gradient norm: 1.2280235290527344\n",
      "G loss: -3.7941365242004395\n",
      "\n",
      "Epoch 33\n",
      "Iteration 1\n",
      "D loss: -48.6611442565918\n",
      "GP: 5.5886030197143555\n",
      "Gradient norm: 1.1954431533813477\n",
      "G loss: -4.00255012512207\n",
      "Iteration 51\n",
      "D loss: -48.25037384033203\n",
      "GP: 5.602778434753418\n",
      "Gradient norm: 1.2073310613632202\n",
      "G loss: -4.236726760864258\n",
      "Iteration 101\n",
      "D loss: -51.29559326171875\n",
      "GP: 6.288306713104248\n",
      "Gradient norm: 1.2118619680404663\n",
      "G loss: -3.521084785461426\n",
      "Iteration 151\n",
      "D loss: -49.3606071472168\n",
      "GP: 5.4535627365112305\n",
      "Gradient norm: 1.1921863555908203\n",
      "G loss: -3.7679443359375\n",
      "Iteration 201\n",
      "D loss: -49.22401809692383\n",
      "GP: 6.738620281219482\n",
      "Gradient norm: 1.2172499895095825\n",
      "G loss: -3.887315511703491\n",
      "Iteration 251\n",
      "D loss: -47.12184143066406\n",
      "GP: 7.4019551277160645\n",
      "Gradient norm: 1.232690453529358\n",
      "G loss: -4.377485275268555\n",
      "Iteration 301\n",
      "D loss: -48.032867431640625\n",
      "GP: 6.930131435394287\n",
      "Gradient norm: 1.2296772003173828\n",
      "G loss: -4.5317301750183105\n",
      "Iteration 351\n",
      "D loss: -49.03647994995117\n",
      "GP: 6.4574055671691895\n",
      "Gradient norm: 1.2136422395706177\n",
      "G loss: -3.68532133102417\n",
      "\n",
      "Epoch 34\n",
      "Iteration 1\n",
      "D loss: -46.135345458984375\n",
      "GP: 6.464915752410889\n",
      "Gradient norm: 1.2183462381362915\n",
      "G loss: -4.07499885559082\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 51\n",
      "D loss: -50.161834716796875\n",
      "GP: 7.443522930145264\n",
      "Gradient norm: 1.2327446937561035\n",
      "G loss: -3.6313669681549072\n",
      "Iteration 101\n",
      "D loss: -48.43117141723633\n",
      "GP: 5.621676921844482\n",
      "Gradient norm: 1.1896286010742188\n",
      "G loss: -3.1239123344421387\n",
      "Iteration 151\n",
      "D loss: -49.111419677734375\n",
      "GP: 6.495522975921631\n",
      "Gradient norm: 1.2172901630401611\n",
      "G loss: -2.4143874645233154\n",
      "Iteration 201\n",
      "D loss: -48.13676834106445\n",
      "GP: 7.493610382080078\n",
      "Gradient norm: 1.2419750690460205\n",
      "G loss: -3.581394672393799\n",
      "Iteration 251\n",
      "D loss: -45.385990142822266\n",
      "GP: 6.3755998611450195\n",
      "Gradient norm: 1.204803705215454\n",
      "G loss: -3.8510210514068604\n",
      "Iteration 301\n",
      "D loss: -50.46575927734375\n",
      "GP: 6.633937835693359\n",
      "Gradient norm: 1.2136561870574951\n",
      "G loss: -3.6901049613952637\n",
      "Iteration 351\n",
      "D loss: -50.49848175048828\n",
      "GP: 7.5594329833984375\n",
      "Gradient norm: 1.2423789501190186\n",
      "G loss: -4.550803184509277\n",
      "\n",
      "Epoch 35\n",
      "Iteration 1\n",
      "D loss: -49.12940216064453\n",
      "GP: 6.281339645385742\n",
      "Gradient norm: 1.2061408758163452\n",
      "G loss: -3.2555506229400635\n",
      "Iteration 51\n",
      "D loss: -47.756710052490234\n",
      "GP: 6.700488090515137\n",
      "Gradient norm: 1.2140264511108398\n",
      "G loss: -3.148622751235962\n",
      "Iteration 101\n",
      "D loss: -48.56758117675781\n",
      "GP: 6.81833553314209\n",
      "Gradient norm: 1.2246341705322266\n",
      "G loss: -3.953772783279419\n",
      "Iteration 151\n",
      "D loss: -50.10427474975586\n",
      "GP: 5.733768463134766\n",
      "Gradient norm: 1.202951192855835\n",
      "G loss: -3.6170449256896973\n",
      "Iteration 201\n",
      "D loss: -49.01470184326172\n",
      "GP: 5.97506046295166\n",
      "Gradient norm: 1.1993963718414307\n",
      "G loss: -4.210182189941406\n",
      "Iteration 251\n",
      "D loss: -49.5063362121582\n",
      "GP: 7.395530700683594\n",
      "Gradient norm: 1.2294299602508545\n",
      "G loss: -4.324905872344971\n",
      "Iteration 301\n",
      "D loss: -48.95768356323242\n",
      "GP: 7.305278778076172\n",
      "Gradient norm: 1.2263576984405518\n",
      "G loss: -3.3736093044281006\n",
      "Iteration 351\n",
      "D loss: -48.1178092956543\n",
      "GP: 6.785974025726318\n",
      "Gradient norm: 1.218445062637329\n",
      "G loss: -3.3196027278900146\n",
      "\n",
      "Epoch 36\n",
      "Iteration 1\n",
      "D loss: -48.59943771362305\n",
      "GP: 7.409344673156738\n",
      "Gradient norm: 1.230424165725708\n",
      "G loss: -3.4136388301849365\n",
      "Iteration 51\n",
      "D loss: -48.28886795043945\n",
      "GP: 5.957701206207275\n",
      "Gradient norm: 1.198580265045166\n",
      "G loss: -3.398571252822876\n",
      "Iteration 101\n",
      "D loss: -49.61867904663086\n",
      "GP: 5.849968910217285\n",
      "Gradient norm: 1.202218770980835\n",
      "G loss: -3.478625535964966\n",
      "Iteration 151\n",
      "D loss: -48.23795700073242\n",
      "GP: 5.784397125244141\n",
      "Gradient norm: 1.188164472579956\n",
      "G loss: -4.5039896965026855\n",
      "Iteration 201\n",
      "D loss: -47.620452880859375\n",
      "GP: 6.387609958648682\n",
      "Gradient norm: 1.2119373083114624\n",
      "G loss: -4.525368690490723\n",
      "Iteration 251\n",
      "D loss: -46.25349426269531\n",
      "GP: 6.7965803146362305\n",
      "Gradient norm: 1.2228374481201172\n",
      "G loss: -4.41388463973999\n",
      "Iteration 301\n",
      "D loss: -47.50069808959961\n",
      "GP: 7.549283027648926\n",
      "Gradient norm: 1.2295351028442383\n",
      "G loss: -4.913644313812256\n",
      "Iteration 351\n",
      "D loss: -48.26235580444336\n",
      "GP: 8.148330688476562\n",
      "Gradient norm: 1.2414339780807495\n",
      "G loss: -4.707675933837891\n",
      "\n",
      "Epoch 37\n",
      "Iteration 1\n",
      "D loss: -47.9582633972168\n",
      "GP: 6.858067989349365\n",
      "Gradient norm: 1.2211391925811768\n",
      "G loss: -3.9207563400268555\n",
      "Iteration 51\n",
      "D loss: -49.38697052001953\n",
      "GP: 6.29948616027832\n",
      "Gradient norm: 1.2097686529159546\n",
      "G loss: -2.9478092193603516\n",
      "Iteration 101\n",
      "D loss: -48.728538513183594\n",
      "GP: 4.807400226593018\n",
      "Gradient norm: 1.1767131090164185\n",
      "G loss: -4.2831711769104\n",
      "Iteration 151\n",
      "D loss: -47.60395431518555\n",
      "GP: 5.807452201843262\n",
      "Gradient norm: 1.1998332738876343\n",
      "G loss: -3.643394947052002\n",
      "Iteration 201\n",
      "D loss: -48.62080764770508\n",
      "GP: 6.814208030700684\n",
      "Gradient norm: 1.2148923873901367\n",
      "G loss: -4.093183517456055\n",
      "Iteration 251\n",
      "D loss: -46.468788146972656\n",
      "GP: 6.299965858459473\n",
      "Gradient norm: 1.1950472593307495\n",
      "G loss: -4.223904132843018\n",
      "Iteration 301\n",
      "D loss: -48.92934036254883\n",
      "GP: 6.559689044952393\n",
      "Gradient norm: 1.2125248908996582\n",
      "G loss: -3.0649025440216064\n",
      "Iteration 351\n",
      "D loss: -49.06940460205078\n",
      "GP: 6.53375244140625\n",
      "Gradient norm: 1.2096517086029053\n",
      "G loss: -2.989854574203491\n",
      "\n",
      "Epoch 38\n",
      "Iteration 1\n",
      "D loss: -51.08460998535156\n",
      "GP: 6.167963027954102\n",
      "Gradient norm: 1.201128602027893\n",
      "G loss: -3.8136250972747803\n",
      "Iteration 51\n",
      "D loss: -46.267276763916016\n",
      "GP: 6.041964530944824\n",
      "Gradient norm: 1.194553017616272\n",
      "G loss: -3.3346688747406006\n",
      "Iteration 101\n",
      "D loss: -47.59202194213867\n",
      "GP: 7.237911224365234\n",
      "Gradient norm: 1.231588363647461\n",
      "G loss: -2.6210434436798096\n",
      "Iteration 151\n",
      "D loss: -50.151126861572266\n",
      "GP: 5.106194496154785\n",
      "Gradient norm: 1.185183048248291\n",
      "G loss: -1.944748044013977\n",
      "Iteration 201\n",
      "D loss: -48.34868621826172\n",
      "GP: 7.420005798339844\n",
      "Gradient norm: 1.2334911823272705\n",
      "G loss: -2.903191328048706\n",
      "Iteration 251\n",
      "D loss: -47.87698745727539\n",
      "GP: 6.432185173034668\n",
      "Gradient norm: 1.2094087600708008\n",
      "G loss: -2.8715667724609375\n",
      "Iteration 301\n",
      "D loss: -47.048377990722656\n",
      "GP: 7.2657928466796875\n",
      "Gradient norm: 1.2239694595336914\n",
      "G loss: -2.935887098312378\n",
      "Iteration 351\n",
      "D loss: -51.12425994873047\n",
      "GP: 7.931273460388184\n",
      "Gradient norm: 1.2392487525939941\n",
      "G loss: -3.2819344997406006\n",
      "\n",
      "Epoch 39\n",
      "Iteration 1\n",
      "D loss: -46.09624099731445\n",
      "GP: 7.099252223968506\n",
      "Gradient norm: 1.224047064781189\n",
      "G loss: -2.466604471206665\n",
      "Iteration 51\n",
      "D loss: -46.110965728759766\n",
      "GP: 6.40549373626709\n",
      "Gradient norm: 1.2136579751968384\n",
      "G loss: -3.424854278564453\n",
      "Iteration 101\n",
      "D loss: -49.36808395385742\n",
      "GP: 6.866982460021973\n",
      "Gradient norm: 1.2111583948135376\n",
      "G loss: -3.115293025970459\n",
      "Iteration 151\n",
      "D loss: -46.688961029052734\n",
      "GP: 5.677889823913574\n",
      "Gradient norm: 1.1981338262557983\n",
      "G loss: -3.395115852355957\n",
      "Iteration 201\n",
      "D loss: -52.64794921875\n",
      "GP: 5.6932692527771\n",
      "Gradient norm: 1.1957494020462036\n",
      "G loss: -3.2566394805908203\n",
      "Iteration 251\n",
      "D loss: -48.27486801147461\n",
      "GP: 6.071850776672363\n",
      "Gradient norm: 1.1912213563919067\n",
      "G loss: -3.0208520889282227\n",
      "Iteration 301\n",
      "D loss: -47.463722229003906\n",
      "GP: 7.656597137451172\n",
      "Gradient norm: 1.2251226902008057\n",
      "G loss: -3.436270236968994\n",
      "Iteration 351\n",
      "D loss: -49.09377670288086\n",
      "GP: 5.657890319824219\n",
      "Gradient norm: 1.1881707906723022\n",
      "G loss: -3.8866567611694336\n",
      "\n",
      "Epoch 40\n",
      "Iteration 1\n",
      "D loss: -47.1821174621582\n",
      "GP: 6.579521179199219\n",
      "Gradient norm: 1.2114487886428833\n",
      "G loss: -3.296219825744629\n",
      "Iteration 51\n",
      "D loss: -48.54526138305664\n",
      "GP: 7.135354042053223\n",
      "Gradient norm: 1.2240025997161865\n",
      "G loss: -2.7454733848571777\n",
      "Iteration 101\n",
      "D loss: -49.17901611328125\n",
      "GP: 7.200788497924805\n",
      "Gradient norm: 1.2197282314300537\n",
      "G loss: -2.9458961486816406\n",
      "Iteration 151\n",
      "D loss: -48.494590759277344\n",
      "GP: 6.33702278137207\n",
      "Gradient norm: 1.20645272731781\n",
      "G loss: -3.144775629043579\n",
      "Iteration 201\n",
      "D loss: -49.80738067626953\n",
      "GP: 8.266195297241211\n",
      "Gradient norm: 1.2400883436203003\n",
      "G loss: -2.7389018535614014\n",
      "Iteration 251\n",
      "D loss: -48.50169372558594\n",
      "GP: 6.165252685546875\n",
      "Gradient norm: 1.197853684425354\n",
      "G loss: -2.72174072265625\n",
      "Iteration 301\n",
      "D loss: -50.73436737060547\n",
      "GP: 7.040367126464844\n",
      "Gradient norm: 1.2154433727264404\n",
      "G loss: -3.498128652572632\n",
      "Iteration 351\n",
      "D loss: -49.377357482910156\n",
      "GP: 7.298116683959961\n",
      "Gradient norm: 1.2355811595916748\n",
      "G loss: -3.3347954750061035\n",
      "\n",
      "Epoch 41\n",
      "Iteration 1\n",
      "D loss: -49.77587890625\n",
      "GP: 5.606030464172363\n",
      "Gradient norm: 1.1902494430541992\n",
      "G loss: -2.5438010692596436\n",
      "Iteration 51\n",
      "D loss: -50.06562042236328\n",
      "GP: 7.228728294372559\n",
      "Gradient norm: 1.2156916856765747\n",
      "G loss: -2.9073433876037598\n",
      "Iteration 101\n",
      "D loss: -46.41341781616211\n",
      "GP: 5.884904384613037\n",
      "Gradient norm: 1.2007392644882202\n",
      "G loss: -3.1052815914154053\n",
      "Iteration 151\n",
      "D loss: -47.006019592285156\n",
      "GP: 6.859583854675293\n",
      "Gradient norm: 1.1969377994537354\n",
      "G loss: -2.677814483642578\n",
      "Iteration 201\n",
      "D loss: -49.51002502441406\n",
      "GP: 4.646425247192383\n",
      "Gradient norm: 1.166709303855896\n",
      "G loss: -1.8075737953186035\n",
      "Iteration 251\n",
      "D loss: -48.982460021972656\n",
      "GP: 8.209531784057617\n",
      "Gradient norm: 1.2398761510849\n",
      "G loss: -3.2539479732513428\n",
      "Iteration 301\n",
      "D loss: -47.169708251953125\n",
      "GP: 8.176340103149414\n",
      "Gradient norm: 1.237093448638916\n",
      "G loss: -2.736111640930176\n",
      "Iteration 351\n",
      "D loss: -50.49534606933594\n",
      "GP: 6.050410270690918\n",
      "Gradient norm: 1.1964119672775269\n",
      "G loss: -2.732940912246704\n",
      "\n",
      "Epoch 42\n",
      "Iteration 1\n",
      "D loss: -48.8228645324707\n",
      "GP: 6.25277042388916\n",
      "Gradient norm: 1.2032426595687866\n",
      "G loss: -3.1077334880828857\n",
      "Iteration 51\n",
      "D loss: -49.99150085449219\n",
      "GP: 5.6710968017578125\n",
      "Gradient norm: 1.1898138523101807\n",
      "G loss: -1.9525690078735352\n",
      "Iteration 101\n",
      "D loss: -47.047462463378906\n",
      "GP: 6.464973449707031\n",
      "Gradient norm: 1.205214500427246\n",
      "G loss: -2.76550030708313\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 151\n",
      "D loss: -49.03812026977539\n",
      "GP: 6.151785373687744\n",
      "Gradient norm: 1.1913468837738037\n",
      "G loss: -3.0007858276367188\n",
      "Iteration 201\n",
      "D loss: -46.15192413330078\n",
      "GP: 5.178701877593994\n",
      "Gradient norm: 1.1734763383865356\n",
      "G loss: -2.9661455154418945\n",
      "Iteration 251\n",
      "D loss: -48.24150085449219\n",
      "GP: 5.695127964019775\n",
      "Gradient norm: 1.1849247217178345\n",
      "G loss: -2.869607448577881\n",
      "Iteration 301\n",
      "D loss: -47.4129524230957\n",
      "GP: 6.381707191467285\n",
      "Gradient norm: 1.207217812538147\n",
      "G loss: -2.7946293354034424\n",
      "Iteration 351\n",
      "D loss: -46.74357604980469\n",
      "GP: 7.513549327850342\n",
      "Gradient norm: 1.2311594486236572\n",
      "G loss: -2.947530746459961\n",
      "\n",
      "Epoch 43\n",
      "Iteration 1\n",
      "D loss: -46.82579803466797\n",
      "GP: 6.595066070556641\n",
      "Gradient norm: 1.1986504793167114\n",
      "G loss: -3.349752187728882\n",
      "Iteration 51\n",
      "D loss: -46.752750396728516\n",
      "GP: 8.429604530334473\n",
      "Gradient norm: 1.2392823696136475\n",
      "G loss: -3.5686960220336914\n",
      "Iteration 101\n",
      "D loss: -46.73316955566406\n",
      "GP: 6.115142822265625\n",
      "Gradient norm: 1.1879758834838867\n",
      "G loss: -2.591763973236084\n",
      "Iteration 151\n",
      "D loss: -47.4688835144043\n",
      "GP: 6.303158760070801\n",
      "Gradient norm: 1.1964901685714722\n",
      "G loss: -2.247258186340332\n",
      "Iteration 201\n",
      "D loss: -48.78368377685547\n",
      "GP: 6.96605110168457\n",
      "Gradient norm: 1.2203243970870972\n",
      "G loss: -2.724397897720337\n",
      "Iteration 251\n",
      "D loss: -47.4549560546875\n",
      "GP: 6.767941474914551\n",
      "Gradient norm: 1.2126457691192627\n",
      "G loss: -3.7663660049438477\n",
      "Iteration 301\n",
      "D loss: -46.500770568847656\n",
      "GP: 7.34873104095459\n",
      "Gradient norm: 1.2187811136245728\n",
      "G loss: -2.8949031829833984\n",
      "Iteration 351\n",
      "D loss: -47.134437561035156\n",
      "GP: 7.839248180389404\n",
      "Gradient norm: 1.21721613407135\n",
      "G loss: -2.866894483566284\n",
      "\n",
      "Epoch 44\n",
      "Iteration 1\n",
      "D loss: -44.848724365234375\n",
      "GP: 5.397058486938477\n",
      "Gradient norm: 1.186275601387024\n",
      "G loss: -2.2829511165618896\n",
      "Iteration 51\n",
      "D loss: -48.725608825683594\n",
      "GP: 6.339120864868164\n",
      "Gradient norm: 1.2060682773590088\n",
      "G loss: -3.3084113597869873\n",
      "Iteration 101\n",
      "D loss: -47.78794860839844\n",
      "GP: 8.481467247009277\n",
      "Gradient norm: 1.2414519786834717\n",
      "G loss: -2.7172346115112305\n",
      "Iteration 151\n",
      "D loss: -48.174644470214844\n",
      "GP: 5.836549758911133\n",
      "Gradient norm: 1.1980904340744019\n",
      "G loss: -2.7843990325927734\n",
      "Iteration 201\n",
      "D loss: -48.74066925048828\n",
      "GP: 5.069391250610352\n",
      "Gradient norm: 1.1775779724121094\n",
      "G loss: -2.6633999347686768\n",
      "Iteration 251\n",
      "D loss: -46.535152435302734\n",
      "GP: 7.482475757598877\n",
      "Gradient norm: 1.2250579595565796\n",
      "G loss: -3.4258406162261963\n",
      "Iteration 301\n",
      "D loss: -47.682472229003906\n",
      "GP: 6.752905368804932\n",
      "Gradient norm: 1.201604962348938\n",
      "G loss: -4.133532524108887\n",
      "Iteration 351\n",
      "D loss: -47.13107681274414\n",
      "GP: 8.846564292907715\n",
      "Gradient norm: 1.2531626224517822\n",
      "G loss: -1.9122189283370972\n",
      "\n",
      "Epoch 45\n",
      "Iteration 1\n",
      "D loss: -46.62948989868164\n",
      "GP: 7.091994285583496\n",
      "Gradient norm: 1.2143306732177734\n",
      "G loss: -3.0029709339141846\n",
      "Iteration 51\n",
      "D loss: -46.50043869018555\n",
      "GP: 6.258440971374512\n",
      "Gradient norm: 1.2065531015396118\n",
      "G loss: -3.0825467109680176\n",
      "Iteration 101\n",
      "D loss: -48.58955383300781\n",
      "GP: 6.562738418579102\n",
      "Gradient norm: 1.2059062719345093\n",
      "G loss: -2.4145853519439697\n",
      "Iteration 151\n",
      "D loss: -48.03248977661133\n",
      "GP: 6.508979797363281\n",
      "Gradient norm: 1.2030771970748901\n",
      "G loss: -3.5298495292663574\n",
      "Iteration 201\n",
      "D loss: -47.156517028808594\n",
      "GP: 6.177156448364258\n",
      "Gradient norm: 1.1980242729187012\n",
      "G loss: -3.246492862701416\n",
      "Iteration 251\n",
      "D loss: -47.181495666503906\n",
      "GP: 6.180227279663086\n",
      "Gradient norm: 1.1996980905532837\n",
      "G loss: -2.3637988567352295\n",
      "Iteration 301\n",
      "D loss: -48.13314437866211\n",
      "GP: 7.32163667678833\n",
      "Gradient norm: 1.2095381021499634\n",
      "G loss: -2.3403115272521973\n",
      "Iteration 351\n",
      "D loss: -46.98511505126953\n",
      "GP: 5.6186370849609375\n",
      "Gradient norm: 1.1733521223068237\n",
      "G loss: -2.286224126815796\n",
      "\n",
      "Epoch 46\n",
      "Iteration 1\n",
      "D loss: -49.0494499206543\n",
      "GP: 5.77441930770874\n",
      "Gradient norm: 1.1833158731460571\n",
      "G loss: -2.494478702545166\n",
      "Iteration 51\n",
      "D loss: -44.575233459472656\n",
      "GP: 5.543064117431641\n",
      "Gradient norm: 1.1746573448181152\n",
      "G loss: -2.7427244186401367\n",
      "Iteration 101\n",
      "D loss: -47.806766510009766\n",
      "GP: 6.9222235679626465\n",
      "Gradient norm: 1.203493356704712\n",
      "G loss: -2.5377914905548096\n",
      "Iteration 151\n",
      "D loss: -46.0856819152832\n",
      "GP: 7.5685834884643555\n",
      "Gradient norm: 1.2321611642837524\n",
      "G loss: -2.377638101577759\n",
      "Iteration 201\n",
      "D loss: -46.544456481933594\n",
      "GP: 5.658006191253662\n",
      "Gradient norm: 1.1936028003692627\n",
      "G loss: -2.4210524559020996\n",
      "Iteration 251\n",
      "D loss: -47.57535934448242\n",
      "GP: 5.5112433433532715\n",
      "Gradient norm: 1.1839591264724731\n",
      "G loss: -1.7216055393218994\n",
      "Iteration 301\n",
      "D loss: -48.89387893676758\n",
      "GP: 6.521044731140137\n",
      "Gradient norm: 1.2041783332824707\n",
      "G loss: -2.5371344089508057\n",
      "Iteration 351\n",
      "D loss: -46.51874923706055\n",
      "GP: 6.875973701477051\n",
      "Gradient norm: 1.2056198120117188\n",
      "G loss: -0.7335113883018494\n",
      "\n",
      "Epoch 47\n",
      "Iteration 1\n",
      "D loss: -48.073814392089844\n",
      "GP: 5.190006256103516\n",
      "Gradient norm: 1.1809511184692383\n",
      "G loss: -1.3635914325714111\n",
      "Iteration 51\n",
      "D loss: -46.34238815307617\n",
      "GP: 5.231758117675781\n",
      "Gradient norm: 1.1789419651031494\n",
      "G loss: -1.5437984466552734\n",
      "Iteration 101\n",
      "D loss: -49.82923889160156\n",
      "GP: 5.716974258422852\n",
      "Gradient norm: 1.1938621997833252\n",
      "G loss: -1.212439775466919\n",
      "Iteration 151\n",
      "D loss: -45.09796142578125\n",
      "GP: 6.773381233215332\n",
      "Gradient norm: 1.2083992958068848\n",
      "G loss: -2.0587780475616455\n",
      "Iteration 201\n",
      "D loss: -45.98603820800781\n",
      "GP: 7.4996137619018555\n",
      "Gradient norm: 1.2206943035125732\n",
      "G loss: -2.2690749168395996\n",
      "Iteration 251\n",
      "D loss: -47.5140266418457\n",
      "GP: 5.199553489685059\n",
      "Gradient norm: 1.172634243965149\n",
      "G loss: -1.339551568031311\n",
      "Iteration 301\n",
      "D loss: -47.72760772705078\n",
      "GP: 5.812524795532227\n",
      "Gradient norm: 1.1924270391464233\n",
      "G loss: -2.057323694229126\n",
      "Iteration 351\n",
      "D loss: -45.718650817871094\n",
      "GP: 6.751546382904053\n",
      "Gradient norm: 1.209810733795166\n",
      "G loss: -1.869958519935608\n",
      "\n",
      "Epoch 48\n",
      "Iteration 1\n",
      "D loss: -46.37678909301758\n",
      "GP: 5.504858493804932\n",
      "Gradient norm: 1.1751288175582886\n",
      "G loss: -1.1489715576171875\n",
      "Iteration 51\n",
      "D loss: -46.297950744628906\n",
      "GP: 5.817995071411133\n",
      "Gradient norm: 1.1852344274520874\n",
      "G loss: -1.7100176811218262\n",
      "Iteration 101\n",
      "D loss: -47.098052978515625\n",
      "GP: 6.635961532592773\n",
      "Gradient norm: 1.205344796180725\n",
      "G loss: -1.1504547595977783\n",
      "Iteration 151\n",
      "D loss: -45.10107421875\n",
      "GP: 9.227636337280273\n",
      "Gradient norm: 1.250752329826355\n",
      "G loss: -0.949299693107605\n",
      "Iteration 201\n",
      "D loss: -47.77619171142578\n",
      "GP: 7.34642219543457\n",
      "Gradient norm: 1.2206095457077026\n",
      "G loss: 0.23307397961616516\n",
      "Iteration 251\n",
      "D loss: -47.271644592285156\n",
      "GP: 6.816308975219727\n",
      "Gradient norm: 1.20889151096344\n",
      "G loss: -0.777313232421875\n",
      "Iteration 301\n",
      "D loss: -45.6118278503418\n",
      "GP: 6.867152214050293\n",
      "Gradient norm: 1.2120275497436523\n",
      "G loss: -0.5678600668907166\n",
      "Iteration 351\n",
      "D loss: -47.49121856689453\n",
      "GP: 7.696939468383789\n",
      "Gradient norm: 1.2318700551986694\n",
      "G loss: -0.316364586353302\n",
      "\n",
      "Epoch 49\n",
      "Iteration 1\n",
      "D loss: -45.54497146606445\n",
      "GP: 5.9070353507995605\n",
      "Gradient norm: 1.1771491765975952\n",
      "G loss: -1.4901355504989624\n",
      "Iteration 51\n",
      "D loss: -47.8978385925293\n",
      "GP: 7.550858497619629\n",
      "Gradient norm: 1.214495062828064\n",
      "G loss: -0.5736545324325562\n",
      "Iteration 101\n",
      "D loss: -48.85721206665039\n",
      "GP: 7.389278411865234\n",
      "Gradient norm: 1.2119214534759521\n",
      "G loss: -0.3800210952758789\n",
      "Iteration 151\n",
      "D loss: -46.39490509033203\n",
      "GP: 6.741463661193848\n",
      "Gradient norm: 1.1999398469924927\n",
      "G loss: 0.33132636547088623\n",
      "Iteration 201\n",
      "D loss: -46.71958923339844\n",
      "GP: 6.403268814086914\n",
      "Gradient norm: 1.1978758573532104\n",
      "G loss: 0.3226238191127777\n",
      "Iteration 251\n",
      "D loss: -47.13005065917969\n",
      "GP: 8.17963981628418\n",
      "Gradient norm: 1.234678030014038\n",
      "G loss: -0.6056064367294312\n",
      "Iteration 301\n",
      "D loss: -46.79265594482422\n",
      "GP: 6.966371059417725\n",
      "Gradient norm: 1.206027626991272\n",
      "G loss: 0.6728110909461975\n",
      "Iteration 351\n",
      "D loss: -45.733028411865234\n",
      "GP: 5.785859107971191\n",
      "Gradient norm: 1.1772321462631226\n",
      "G loss: -0.13398252427577972\n",
      "\n",
      "Epoch 50\n",
      "Iteration 1\n",
      "D loss: -47.154022216796875\n",
      "GP: 5.804750919342041\n",
      "Gradient norm: 1.1846733093261719\n",
      "G loss: 0.6093912720680237\n",
      "Iteration 51\n",
      "D loss: -47.1948127746582\n",
      "GP: 4.1841721534729\n",
      "Gradient norm: 1.147487759590149\n",
      "G loss: -1.4608741998672485\n",
      "Iteration 101\n",
      "D loss: -45.27171325683594\n",
      "GP: 6.018763542175293\n",
      "Gradient norm: 1.189389705657959\n",
      "G loss: 0.6006056666374207\n",
      "Iteration 151\n",
      "D loss: -48.57080078125\n",
      "GP: 6.506946563720703\n",
      "Gradient norm: 1.2063806056976318\n",
      "G loss: 1.1646729707717896\n",
      "Iteration 201\n",
      "D loss: -45.96675491333008\n",
      "GP: 6.60506010055542\n",
      "Gradient norm: 1.1947516202926636\n",
      "G loss: 0.4450906217098236\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 251\n",
      "D loss: -46.671958923339844\n",
      "GP: 6.125843048095703\n",
      "Gradient norm: 1.1843740940093994\n",
      "G loss: 1.3375244140625\n",
      "Iteration 301\n",
      "D loss: -45.63658905029297\n",
      "GP: 6.355721950531006\n",
      "Gradient norm: 1.1932584047317505\n",
      "G loss: 0.13314296305179596\n",
      "Iteration 351\n",
      "D loss: -45.37000274658203\n",
      "GP: 6.973854064941406\n",
      "Gradient norm: 1.2144701480865479\n",
      "G loss: 1.034029483795166\n",
      "\n",
      "Epoch 51\n",
      "Iteration 1\n",
      "D loss: -46.03118133544922\n",
      "GP: 6.6280317306518555\n",
      "Gradient norm: 1.1970701217651367\n",
      "G loss: 0.27829480171203613\n",
      "Iteration 51\n",
      "D loss: -45.06937789916992\n",
      "GP: 6.296200275421143\n",
      "Gradient norm: 1.1985423564910889\n",
      "G loss: 0.2568296194076538\n",
      "Iteration 101\n",
      "D loss: -44.9799919128418\n",
      "GP: 5.968205451965332\n",
      "Gradient norm: 1.1775383949279785\n",
      "G loss: 0.5281473398208618\n",
      "Iteration 151\n",
      "D loss: -47.9413948059082\n",
      "GP: 8.726578712463379\n",
      "Gradient norm: 1.2421797513961792\n",
      "G loss: 0.9652965664863586\n",
      "Iteration 201\n",
      "D loss: -45.64636993408203\n",
      "GP: 6.7723236083984375\n",
      "Gradient norm: 1.2108290195465088\n",
      "G loss: -0.10795307159423828\n",
      "Iteration 251\n",
      "D loss: -46.344337463378906\n",
      "GP: 7.067161560058594\n",
      "Gradient norm: 1.2188156843185425\n",
      "G loss: 1.0731079578399658\n",
      "Iteration 301\n",
      "D loss: -47.890018463134766\n",
      "GP: 7.401280879974365\n",
      "Gradient norm: 1.2238056659698486\n",
      "G loss: 1.0594202280044556\n",
      "Iteration 351\n",
      "D loss: -47.26869201660156\n",
      "GP: 4.61780309677124\n",
      "Gradient norm: 1.1638866662979126\n",
      "G loss: 1.018998146057129\n",
      "\n",
      "Epoch 52\n",
      "Iteration 1\n",
      "D loss: -47.485694885253906\n",
      "GP: 6.442350387573242\n",
      "Gradient norm: 1.1989790201187134\n",
      "G loss: 0.5929385423660278\n",
      "Iteration 51\n",
      "D loss: -45.51028060913086\n",
      "GP: 7.071855068206787\n",
      "Gradient norm: 1.2050882577896118\n",
      "G loss: 1.6126347780227661\n",
      "Iteration 101\n",
      "D loss: -47.252445220947266\n",
      "GP: 7.848092079162598\n",
      "Gradient norm: 1.205165147781372\n",
      "G loss: 1.3098865747451782\n",
      "Iteration 151\n",
      "D loss: -46.58843231201172\n",
      "GP: 7.744276523590088\n",
      "Gradient norm: 1.2182914018630981\n",
      "G loss: 2.0539321899414062\n",
      "Iteration 201\n",
      "D loss: -48.14007568359375\n",
      "GP: 6.876487731933594\n",
      "Gradient norm: 1.2063593864440918\n",
      "G loss: 0.42749452590942383\n",
      "Iteration 251\n",
      "D loss: -44.81477737426758\n",
      "GP: 8.26497745513916\n",
      "Gradient norm: 1.235991358757019\n",
      "G loss: 1.4521234035491943\n",
      "Iteration 301\n",
      "D loss: -46.08319091796875\n",
      "GP: 6.274572372436523\n",
      "Gradient norm: 1.1923640966415405\n",
      "G loss: 1.0179086923599243\n",
      "Iteration 351\n",
      "D loss: -44.700496673583984\n",
      "GP: 6.187741756439209\n",
      "Gradient norm: 1.1950995922088623\n",
      "G loss: 0.7646386623382568\n",
      "\n",
      "Epoch 53\n",
      "Iteration 1\n",
      "D loss: -45.581077575683594\n",
      "GP: 7.5878095626831055\n",
      "Gradient norm: 1.2154475450515747\n",
      "G loss: 1.5004348754882812\n",
      "Iteration 51\n",
      "D loss: -45.15541076660156\n",
      "GP: 7.092587947845459\n",
      "Gradient norm: 1.2087681293487549\n",
      "G loss: 0.8733336925506592\n",
      "Iteration 101\n",
      "D loss: -48.55632781982422\n",
      "GP: 7.582083225250244\n",
      "Gradient norm: 1.2090994119644165\n",
      "G loss: 1.9862703084945679\n",
      "Iteration 151\n",
      "D loss: -46.69959259033203\n",
      "GP: 5.741650581359863\n",
      "Gradient norm: 1.1748822927474976\n",
      "G loss: 1.2043068408966064\n",
      "Iteration 201\n",
      "D loss: -44.72138977050781\n",
      "GP: 8.281479835510254\n",
      "Gradient norm: 1.2382376194000244\n",
      "G loss: 0.7263362407684326\n",
      "Iteration 251\n",
      "D loss: -46.65810012817383\n",
      "GP: 8.011265754699707\n",
      "Gradient norm: 1.2164982557296753\n",
      "G loss: 1.8143547773361206\n",
      "Iteration 301\n",
      "D loss: -48.39591979980469\n",
      "GP: 7.741544246673584\n",
      "Gradient norm: 1.229225993156433\n",
      "G loss: 1.6469069719314575\n",
      "Iteration 351\n",
      "D loss: -47.75335693359375\n",
      "GP: 5.824379920959473\n",
      "Gradient norm: 1.1626334190368652\n",
      "G loss: 1.649001121520996\n",
      "\n",
      "Epoch 54\n",
      "Iteration 1\n",
      "D loss: -48.75230407714844\n",
      "GP: 8.212486267089844\n",
      "Gradient norm: 1.2297804355621338\n",
      "G loss: 1.8848849534988403\n",
      "Iteration 51\n",
      "D loss: -45.910728454589844\n",
      "GP: 7.080204486846924\n",
      "Gradient norm: 1.2031683921813965\n",
      "G loss: 2.1008756160736084\n",
      "Iteration 101\n",
      "D loss: -48.09035110473633\n",
      "GP: 5.653677940368652\n",
      "Gradient norm: 1.1811236143112183\n",
      "G loss: 2.4177615642547607\n",
      "Iteration 151\n",
      "D loss: -47.96833801269531\n",
      "GP: 7.005199432373047\n",
      "Gradient norm: 1.1997641324996948\n",
      "G loss: 2.806774854660034\n",
      "Iteration 201\n",
      "D loss: -45.46146011352539\n",
      "GP: 6.997786521911621\n",
      "Gradient norm: 1.209254264831543\n",
      "G loss: 0.9330514669418335\n",
      "Iteration 251\n",
      "D loss: -45.82444763183594\n",
      "GP: 6.85087776184082\n",
      "Gradient norm: 1.2123631238937378\n",
      "G loss: 2.0688676834106445\n",
      "Iteration 301\n",
      "D loss: -46.076812744140625\n",
      "GP: 6.676811218261719\n",
      "Gradient norm: 1.1918960809707642\n",
      "G loss: 1.2347567081451416\n",
      "Iteration 351\n",
      "D loss: -46.865787506103516\n",
      "GP: 4.900578498840332\n",
      "Gradient norm: 1.1592155694961548\n",
      "G loss: 0.9772645831108093\n",
      "\n",
      "Epoch 55\n",
      "Iteration 1\n",
      "D loss: -48.82902526855469\n",
      "GP: 5.934090614318848\n",
      "Gradient norm: 1.1933221817016602\n",
      "G loss: 0.3930678367614746\n",
      "Iteration 51\n",
      "D loss: -47.98445129394531\n",
      "GP: 4.632649898529053\n",
      "Gradient norm: 1.1476346254348755\n",
      "G loss: 2.433389186859131\n",
      "Iteration 101\n",
      "D loss: -46.58129119873047\n",
      "GP: 7.850856781005859\n",
      "Gradient norm: 1.2276198863983154\n",
      "G loss: 1.5517268180847168\n",
      "Iteration 151\n",
      "D loss: -47.65229415893555\n",
      "GP: 6.578001976013184\n",
      "Gradient norm: 1.2066525220870972\n",
      "G loss: 1.843294620513916\n",
      "Iteration 201\n",
      "D loss: -45.19105529785156\n",
      "GP: 7.60530948638916\n",
      "Gradient norm: 1.2178738117218018\n",
      "G loss: 2.4395673274993896\n",
      "Iteration 251\n",
      "D loss: -46.6845817565918\n",
      "GP: 7.244568347930908\n",
      "Gradient norm: 1.2156569957733154\n",
      "G loss: 3.082900285720825\n",
      "Iteration 301\n",
      "D loss: -47.94816207885742\n",
      "GP: 6.751759052276611\n",
      "Gradient norm: 1.1997829675674438\n",
      "G loss: 2.071441411972046\n",
      "Iteration 351\n",
      "D loss: -48.80628967285156\n",
      "GP: 8.74899959564209\n",
      "Gradient norm: 1.2264729738235474\n",
      "G loss: 1.540013313293457\n",
      "\n",
      "Epoch 56\n",
      "Iteration 1\n",
      "D loss: -45.84504318237305\n",
      "GP: 6.392350196838379\n",
      "Gradient norm: 1.1787972450256348\n",
      "G loss: 1.647126317024231\n",
      "Iteration 51\n",
      "D loss: -47.53322982788086\n",
      "GP: 6.3700056076049805\n",
      "Gradient norm: 1.1913684606552124\n",
      "G loss: 2.2039954662323\n",
      "Iteration 101\n",
      "D loss: -44.11955261230469\n",
      "GP: 6.5710835456848145\n",
      "Gradient norm: 1.1927367448806763\n",
      "G loss: 3.028984785079956\n",
      "Iteration 151\n",
      "D loss: -48.068092346191406\n",
      "GP: 6.937038898468018\n",
      "Gradient norm: 1.1993699073791504\n",
      "G loss: 2.000819683074951\n",
      "Iteration 201\n",
      "D loss: -46.54071044921875\n",
      "GP: 7.667483329772949\n",
      "Gradient norm: 1.2262099981307983\n",
      "G loss: 2.532276153564453\n",
      "Iteration 251\n",
      "D loss: -46.1683349609375\n",
      "GP: 6.5322113037109375\n",
      "Gradient norm: 1.1906816959381104\n",
      "G loss: 3.0050199031829834\n",
      "Iteration 301\n",
      "D loss: -45.17725372314453\n",
      "GP: 8.547780990600586\n",
      "Gradient norm: 1.2391847372055054\n",
      "G loss: 2.71384596824646\n",
      "Iteration 351\n",
      "D loss: -47.9415397644043\n",
      "GP: 8.088092803955078\n",
      "Gradient norm: 1.2190215587615967\n",
      "G loss: 2.037571668624878\n",
      "\n",
      "Epoch 57\n",
      "Iteration 1\n",
      "D loss: -49.774757385253906\n",
      "GP: 4.339607238769531\n",
      "Gradient norm: 1.1422158479690552\n",
      "G loss: 4.180115222930908\n",
      "Iteration 51\n",
      "D loss: -47.3651123046875\n",
      "GP: 7.693744659423828\n",
      "Gradient norm: 1.2096706628799438\n",
      "G loss: 2.0553832054138184\n",
      "Iteration 101\n",
      "D loss: -46.927894592285156\n",
      "GP: 7.964697360992432\n",
      "Gradient norm: 1.2140430212020874\n",
      "G loss: 3.2792181968688965\n",
      "Iteration 151\n",
      "D loss: -44.60006332397461\n",
      "GP: 5.679249286651611\n",
      "Gradient norm: 1.188475489616394\n",
      "G loss: 2.9274792671203613\n",
      "Iteration 201\n",
      "D loss: -45.77751922607422\n",
      "GP: 6.621795177459717\n",
      "Gradient norm: 1.2023701667785645\n",
      "G loss: 3.368603229522705\n",
      "Iteration 251\n",
      "D loss: -45.590450286865234\n",
      "GP: 7.473213195800781\n",
      "Gradient norm: 1.2036751508712769\n",
      "G loss: 2.0194623470306396\n",
      "Iteration 301\n",
      "D loss: -44.69314193725586\n",
      "GP: 7.088024139404297\n",
      "Gradient norm: 1.2126177549362183\n",
      "G loss: 4.2099289894104\n",
      "Iteration 351\n",
      "D loss: -45.975711822509766\n",
      "GP: 7.152190685272217\n",
      "Gradient norm: 1.2164404392242432\n",
      "G loss: 2.559799909591675\n",
      "\n",
      "Epoch 58\n",
      "Iteration 1\n",
      "D loss: -47.27016830444336\n",
      "GP: 6.3202948570251465\n",
      "Gradient norm: 1.2064706087112427\n",
      "G loss: 2.3033578395843506\n",
      "Iteration 51\n",
      "D loss: -49.33156204223633\n",
      "GP: 6.967520713806152\n",
      "Gradient norm: 1.1990851163864136\n",
      "G loss: 1.7607544660568237\n",
      "Iteration 101\n",
      "D loss: -46.94249725341797\n",
      "GP: 6.685578346252441\n",
      "Gradient norm: 1.1906044483184814\n",
      "G loss: 3.2600739002227783\n",
      "Iteration 151\n",
      "D loss: -46.63008499145508\n",
      "GP: 6.933199882507324\n",
      "Gradient norm: 1.2093459367752075\n",
      "G loss: 3.8376011848449707\n",
      "Iteration 201\n",
      "D loss: -45.912174224853516\n",
      "GP: 7.084410667419434\n",
      "Gradient norm: 1.2059780359268188\n",
      "G loss: 2.880114793777466\n",
      "Iteration 251\n",
      "D loss: -47.481163024902344\n",
      "GP: 8.879095077514648\n",
      "Gradient norm: 1.2329072952270508\n",
      "G loss: 2.6314010620117188\n",
      "Iteration 301\n",
      "D loss: -46.32565689086914\n",
      "GP: 6.510390281677246\n",
      "Gradient norm: 1.1996865272521973\n",
      "G loss: 4.396327495574951\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 351\n",
      "D loss: -48.144927978515625\n",
      "GP: 6.011087417602539\n",
      "Gradient norm: 1.1812325716018677\n",
      "G loss: 3.246757745742798\n",
      "\n",
      "Epoch 59\n",
      "Iteration 1\n",
      "D loss: -47.55533218383789\n",
      "GP: 6.887477874755859\n",
      "Gradient norm: 1.1930863857269287\n",
      "G loss: 2.6882214546203613\n",
      "Iteration 51\n",
      "D loss: -45.74657440185547\n",
      "GP: 6.549531936645508\n",
      "Gradient norm: 1.1887043714523315\n",
      "G loss: 1.83314847946167\n",
      "Iteration 101\n",
      "D loss: -47.52967834472656\n",
      "GP: 7.30165958404541\n",
      "Gradient norm: 1.2172439098358154\n",
      "G loss: 2.4832093715667725\n",
      "Iteration 151\n",
      "D loss: -46.62558364868164\n",
      "GP: 6.464645862579346\n",
      "Gradient norm: 1.1950033903121948\n",
      "G loss: 3.167288303375244\n",
      "Iteration 201\n",
      "D loss: -44.281795501708984\n",
      "GP: 4.895506381988525\n",
      "Gradient norm: 1.1581604480743408\n",
      "G loss: 2.7430942058563232\n",
      "Iteration 251\n",
      "D loss: -47.74824142456055\n",
      "GP: 7.697758674621582\n",
      "Gradient norm: 1.22403085231781\n",
      "G loss: 2.5881924629211426\n",
      "Iteration 301\n",
      "D loss: -48.249114990234375\n",
      "GP: 6.407494068145752\n",
      "Gradient norm: 1.2005691528320312\n",
      "G loss: 3.0171921253204346\n",
      "Iteration 351\n",
      "D loss: -44.69359588623047\n",
      "GP: 7.740509033203125\n",
      "Gradient norm: 1.2327980995178223\n",
      "G loss: 3.6743664741516113\n",
      "\n",
      "Epoch 60\n",
      "Iteration 1\n",
      "D loss: -46.89641571044922\n",
      "GP: 6.606227874755859\n",
      "Gradient norm: 1.2076295614242554\n",
      "G loss: 4.178621292114258\n",
      "Iteration 51\n",
      "D loss: -47.28414535522461\n",
      "GP: 8.026145935058594\n",
      "Gradient norm: 1.212355375289917\n",
      "G loss: 3.7984039783477783\n",
      "Iteration 101\n",
      "D loss: -46.71639633178711\n",
      "GP: 6.303718090057373\n",
      "Gradient norm: 1.2064324617385864\n",
      "G loss: 2.8736655712127686\n",
      "Iteration 151\n",
      "D loss: -46.931495666503906\n",
      "GP: 7.169976711273193\n",
      "Gradient norm: 1.199116587638855\n",
      "G loss: 4.107686996459961\n",
      "Iteration 201\n",
      "D loss: -48.900760650634766\n",
      "GP: 5.756616115570068\n",
      "Gradient norm: 1.1741563081741333\n",
      "G loss: 2.1549594402313232\n",
      "Iteration 251\n",
      "D loss: -46.05221939086914\n",
      "GP: 5.8428850173950195\n",
      "Gradient norm: 1.168517827987671\n",
      "G loss: 3.9217746257781982\n",
      "Iteration 301\n",
      "D loss: -44.77884292602539\n",
      "GP: 7.414931774139404\n",
      "Gradient norm: 1.2229098081588745\n",
      "G loss: 4.269115447998047\n",
      "Iteration 351\n",
      "D loss: -46.958438873291016\n",
      "GP: 6.820272922515869\n",
      "Gradient norm: 1.193634033203125\n",
      "G loss: 4.0987324714660645\n",
      "\n",
      "Epoch 61\n",
      "Iteration 1\n",
      "D loss: -47.798255920410156\n",
      "GP: 5.727601528167725\n",
      "Gradient norm: 1.1801443099975586\n",
      "G loss: 3.482976198196411\n",
      "Iteration 51\n",
      "D loss: -46.53682327270508\n",
      "GP: 8.777636528015137\n",
      "Gradient norm: 1.2339237928390503\n",
      "G loss: 4.757786273956299\n",
      "Iteration 101\n",
      "D loss: -44.317108154296875\n",
      "GP: 7.110147476196289\n",
      "Gradient norm: 1.2028895616531372\n",
      "G loss: 3.5102834701538086\n",
      "Iteration 151\n",
      "D loss: -45.560848236083984\n",
      "GP: 7.1262030601501465\n",
      "Gradient norm: 1.1943187713623047\n",
      "G loss: 3.108567953109741\n",
      "Iteration 201\n",
      "D loss: -45.27569580078125\n",
      "GP: 6.4009928703308105\n",
      "Gradient norm: 1.1911816596984863\n",
      "G loss: 3.0727925300598145\n",
      "Iteration 251\n",
      "D loss: -47.61371994018555\n",
      "GP: 7.386727333068848\n",
      "Gradient norm: 1.2115967273712158\n",
      "G loss: 3.210679292678833\n",
      "Iteration 301\n",
      "D loss: -47.36060333251953\n",
      "GP: 7.577389717102051\n",
      "Gradient norm: 1.2138690948486328\n",
      "G loss: 4.168390274047852\n",
      "Iteration 351\n",
      "D loss: -47.25939178466797\n",
      "GP: 5.386316299438477\n",
      "Gradient norm: 1.1696178913116455\n",
      "G loss: 4.681433200836182\n",
      "\n",
      "Epoch 62\n",
      "Iteration 1\n",
      "D loss: -45.779666900634766\n",
      "GP: 6.3183794021606445\n",
      "Gradient norm: 1.1998611688613892\n",
      "G loss: 4.346905708312988\n",
      "Iteration 51\n",
      "D loss: -45.975162506103516\n",
      "GP: 8.025956153869629\n",
      "Gradient norm: 1.2027033567428589\n",
      "G loss: 4.417182922363281\n",
      "Iteration 101\n",
      "D loss: -45.653961181640625\n",
      "GP: 5.659692764282227\n",
      "Gradient norm: 1.1779040098190308\n",
      "G loss: 3.473261833190918\n",
      "Iteration 151\n",
      "D loss: -45.34062957763672\n",
      "GP: 6.3322649002075195\n",
      "Gradient norm: 1.1968629360198975\n",
      "G loss: 4.184386253356934\n",
      "Iteration 201\n",
      "D loss: -49.739688873291016\n",
      "GP: 6.561934947967529\n",
      "Gradient norm: 1.2014329433441162\n",
      "G loss: 3.5230095386505127\n",
      "Iteration 251\n",
      "D loss: -48.63637924194336\n",
      "GP: 7.013922214508057\n",
      "Gradient norm: 1.2104147672653198\n",
      "G loss: 4.1297101974487305\n",
      "Iteration 301\n",
      "D loss: -46.872047424316406\n",
      "GP: 6.207701683044434\n",
      "Gradient norm: 1.1938139200210571\n",
      "G loss: 4.2760491371154785\n",
      "Iteration 351\n",
      "D loss: -47.83953857421875\n",
      "GP: 7.3484907150268555\n",
      "Gradient norm: 1.2055045366287231\n",
      "G loss: 2.842833995819092\n",
      "\n",
      "Epoch 63\n",
      "Iteration 1\n",
      "D loss: -46.477909088134766\n",
      "GP: 8.00277042388916\n",
      "Gradient norm: 1.2248928546905518\n",
      "G loss: 2.661632537841797\n",
      "Iteration 51\n",
      "D loss: -47.540809631347656\n",
      "GP: 6.401457786560059\n",
      "Gradient norm: 1.200395941734314\n",
      "G loss: 3.9432342052459717\n",
      "Iteration 101\n",
      "D loss: -45.5369987487793\n",
      "GP: 7.5255937576293945\n",
      "Gradient norm: 1.2129426002502441\n",
      "G loss: 4.409183979034424\n",
      "Iteration 151\n",
      "D loss: -46.91193771362305\n",
      "GP: 8.259830474853516\n",
      "Gradient norm: 1.215864658355713\n",
      "G loss: 2.4672746658325195\n",
      "Iteration 201\n",
      "D loss: -45.26186752319336\n",
      "GP: 7.848663330078125\n",
      "Gradient norm: 1.2228425741195679\n",
      "G loss: 5.061154842376709\n",
      "Iteration 251\n",
      "D loss: -46.032318115234375\n",
      "GP: 7.048022747039795\n",
      "Gradient norm: 1.2098045349121094\n",
      "G loss: 3.275836944580078\n",
      "Iteration 301\n",
      "D loss: -46.67632293701172\n",
      "GP: 8.576593399047852\n",
      "Gradient norm: 1.2191520929336548\n",
      "G loss: 1.8702329397201538\n",
      "Iteration 351\n",
      "D loss: -49.257259368896484\n",
      "GP: 6.249279975891113\n",
      "Gradient norm: 1.1831923723220825\n",
      "G loss: 1.8550286293029785\n",
      "\n",
      "Epoch 64\n",
      "Iteration 1\n",
      "D loss: -47.33518600463867\n",
      "GP: 6.194664001464844\n",
      "Gradient norm: 1.191345453262329\n",
      "G loss: 1.7420399188995361\n",
      "Iteration 51\n",
      "D loss: -44.490108489990234\n",
      "GP: 6.172882080078125\n",
      "Gradient norm: 1.1843600273132324\n",
      "G loss: 2.611377000808716\n",
      "Iteration 101\n",
      "D loss: -45.232173919677734\n",
      "GP: 6.63595724105835\n",
      "Gradient norm: 1.195517897605896\n",
      "G loss: 1.3849109411239624\n",
      "Iteration 151\n",
      "D loss: -46.315940856933594\n",
      "GP: 6.504302501678467\n",
      "Gradient norm: 1.1836198568344116\n",
      "G loss: 1.4090994596481323\n",
      "Iteration 201\n",
      "D loss: -47.853973388671875\n",
      "GP: 6.724298477172852\n",
      "Gradient norm: 1.20883309841156\n",
      "G loss: 1.3421597480773926\n",
      "Iteration 251\n",
      "D loss: -45.35700225830078\n",
      "GP: 7.458479404449463\n",
      "Gradient norm: 1.2230802774429321\n",
      "G loss: 2.0064404010772705\n",
      "Iteration 301\n",
      "D loss: -47.301753997802734\n",
      "GP: 6.756365776062012\n",
      "Gradient norm: 1.1877678632736206\n",
      "G loss: 2.5074853897094727\n",
      "Iteration 351\n",
      "D loss: -47.50551223754883\n",
      "GP: 5.793881893157959\n",
      "Gradient norm: 1.1784396171569824\n",
      "G loss: 4.343172550201416\n",
      "\n",
      "Epoch 65\n",
      "Iteration 1\n",
      "D loss: -46.79645919799805\n",
      "GP: 7.151847839355469\n",
      "Gradient norm: 1.201912760734558\n",
      "G loss: 2.7914702892303467\n",
      "Iteration 51\n",
      "D loss: -46.55219268798828\n",
      "GP: 6.706738471984863\n",
      "Gradient norm: 1.1949776411056519\n",
      "G loss: 2.713327646255493\n",
      "Iteration 101\n",
      "D loss: -46.13560485839844\n",
      "GP: 9.540407180786133\n",
      "Gradient norm: 1.2332967519760132\n",
      "G loss: 2.216219186782837\n",
      "Iteration 151\n",
      "D loss: -48.031951904296875\n",
      "GP: 5.894654273986816\n",
      "Gradient norm: 1.1657627820968628\n",
      "G loss: 2.717876672744751\n",
      "Iteration 201\n",
      "D loss: -48.84983825683594\n",
      "GP: 5.433126449584961\n",
      "Gradient norm: 1.1767486333847046\n",
      "G loss: 2.1326725482940674\n",
      "Iteration 251\n",
      "D loss: -45.04952621459961\n",
      "GP: 6.547615051269531\n",
      "Gradient norm: 1.1981313228607178\n",
      "G loss: 3.8381168842315674\n",
      "Iteration 301\n",
      "D loss: -45.99146270751953\n",
      "GP: 7.355546951293945\n",
      "Gradient norm: 1.2136976718902588\n",
      "G loss: 3.047618865966797\n",
      "Iteration 351\n",
      "D loss: -45.99938201904297\n",
      "GP: 4.9713850021362305\n",
      "Gradient norm: 1.1556288003921509\n",
      "G loss: 1.7537668943405151\n",
      "\n",
      "Epoch 66\n",
      "Iteration 1\n",
      "D loss: -45.10822296142578\n",
      "GP: 6.66785192489624\n",
      "Gradient norm: 1.2065812349319458\n",
      "G loss: 2.7451350688934326\n",
      "Iteration 51\n",
      "D loss: -46.94000244140625\n",
      "GP: 7.635225296020508\n",
      "Gradient norm: 1.2129420042037964\n",
      "G loss: 2.5361697673797607\n",
      "Iteration 101\n",
      "D loss: -43.69614791870117\n",
      "GP: 6.210739612579346\n",
      "Gradient norm: 1.1798845529556274\n",
      "G loss: 2.247279644012451\n",
      "Iteration 151\n",
      "D loss: -46.137943267822266\n",
      "GP: 7.201457500457764\n",
      "Gradient norm: 1.2022794485092163\n",
      "G loss: 2.429783582687378\n",
      "Iteration 201\n",
      "D loss: -47.0362548828125\n",
      "GP: 5.89948844909668\n",
      "Gradient norm: 1.1803388595581055\n",
      "G loss: 3.1500744819641113\n",
      "Iteration 251\n",
      "D loss: -44.7974967956543\n",
      "GP: 6.222034454345703\n",
      "Gradient norm: 1.1821095943450928\n",
      "G loss: 2.9685261249542236\n",
      "Iteration 301\n",
      "D loss: -44.822322845458984\n",
      "GP: 6.131603240966797\n",
      "Gradient norm: 1.1748260259628296\n",
      "G loss: 4.204995632171631\n",
      "Iteration 351\n",
      "D loss: -48.427982330322266\n",
      "GP: 7.7034807205200195\n",
      "Gradient norm: 1.2024928331375122\n",
      "G loss: 3.052417755126953\n",
      "\n",
      "Epoch 67\n",
      "Iteration 1\n",
      "D loss: -48.124698638916016\n",
      "GP: 6.585635185241699\n",
      "Gradient norm: 1.1772162914276123\n",
      "G loss: 3.222778558731079\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 51\n",
      "D loss: -44.197021484375\n",
      "GP: 7.561365127563477\n",
      "Gradient norm: 1.2104806900024414\n",
      "G loss: 3.365861415863037\n",
      "Iteration 101\n",
      "D loss: -47.13005828857422\n",
      "GP: 9.344172477722168\n",
      "Gradient norm: 1.2529011964797974\n",
      "G loss: 3.895535707473755\n",
      "Iteration 151\n",
      "D loss: -45.86112594604492\n",
      "GP: 6.471617221832275\n",
      "Gradient norm: 1.1941090822219849\n",
      "G loss: 4.396480560302734\n",
      "Iteration 201\n",
      "D loss: -47.159400939941406\n",
      "GP: 7.486318588256836\n",
      "Gradient norm: 1.216558814048767\n",
      "G loss: 4.7996039390563965\n",
      "Iteration 251\n",
      "D loss: -44.231414794921875\n",
      "GP: 5.9488420486450195\n",
      "Gradient norm: 1.1617985963821411\n",
      "G loss: 5.136054039001465\n",
      "Iteration 301\n",
      "D loss: -48.04821014404297\n",
      "GP: 6.586202621459961\n",
      "Gradient norm: 1.186863899230957\n",
      "G loss: 4.851585865020752\n",
      "Iteration 351\n",
      "D loss: -46.83155059814453\n",
      "GP: 8.681811332702637\n",
      "Gradient norm: 1.2437736988067627\n",
      "G loss: 5.123680114746094\n",
      "\n",
      "Epoch 68\n",
      "Iteration 1\n",
      "D loss: -45.879981994628906\n",
      "GP: 7.145645618438721\n",
      "Gradient norm: 1.1828975677490234\n",
      "G loss: 5.247659206390381\n",
      "Iteration 51\n",
      "D loss: -44.1134033203125\n",
      "GP: 5.009484767913818\n",
      "Gradient norm: 1.1581040620803833\n",
      "G loss: 5.164888381958008\n",
      "Iteration 101\n",
      "D loss: -46.632728576660156\n",
      "GP: 8.009263038635254\n",
      "Gradient norm: 1.2221343517303467\n",
      "G loss: 5.395533561706543\n",
      "Iteration 151\n",
      "D loss: -45.30833435058594\n",
      "GP: 7.0892181396484375\n",
      "Gradient norm: 1.2063877582550049\n",
      "G loss: 3.6228792667388916\n",
      "Iteration 201\n",
      "D loss: -46.917232513427734\n",
      "GP: 7.637215614318848\n",
      "Gradient norm: 1.2096601724624634\n",
      "G loss: 5.832248687744141\n",
      "Iteration 251\n",
      "D loss: -45.801570892333984\n",
      "GP: 5.629014015197754\n",
      "Gradient norm: 1.1900463104248047\n",
      "G loss: 4.2811808586120605\n",
      "Iteration 301\n",
      "D loss: -45.99463653564453\n",
      "GP: 8.339715957641602\n",
      "Gradient norm: 1.2217785120010376\n",
      "G loss: 4.83477783203125\n",
      "Iteration 351\n",
      "D loss: -44.88029479980469\n",
      "GP: 5.799624443054199\n",
      "Gradient norm: 1.1710398197174072\n",
      "G loss: 4.975945472717285\n",
      "\n",
      "Epoch 69\n",
      "Iteration 1\n",
      "D loss: -47.11339569091797\n",
      "GP: 7.247230529785156\n",
      "Gradient norm: 1.202147126197815\n",
      "G loss: 5.024211883544922\n",
      "Iteration 51\n",
      "D loss: -44.88505554199219\n",
      "GP: 7.727027893066406\n",
      "Gradient norm: 1.2227258682250977\n",
      "G loss: 5.75953483581543\n",
      "Iteration 101\n",
      "D loss: -47.44512939453125\n",
      "GP: 6.202167987823486\n",
      "Gradient norm: 1.190346598625183\n",
      "G loss: 6.310091972351074\n",
      "Iteration 151\n",
      "D loss: -47.77033996582031\n",
      "GP: 8.160322189331055\n",
      "Gradient norm: 1.218793511390686\n",
      "G loss: 5.625129699707031\n",
      "Iteration 201\n",
      "D loss: -46.64075469970703\n",
      "GP: 6.923235893249512\n",
      "Gradient norm: 1.192551851272583\n",
      "G loss: 6.157749176025391\n",
      "Iteration 251\n",
      "D loss: -44.801727294921875\n",
      "GP: 6.198954105377197\n",
      "Gradient norm: 1.171633005142212\n",
      "G loss: 5.010344982147217\n",
      "Iteration 301\n",
      "D loss: -46.15411376953125\n",
      "GP: 8.06770133972168\n",
      "Gradient norm: 1.215773582458496\n",
      "G loss: 5.4362263679504395\n",
      "Iteration 351\n",
      "D loss: -47.15052032470703\n",
      "GP: 6.1411285400390625\n",
      "Gradient norm: 1.1813712120056152\n",
      "G loss: 5.347027778625488\n",
      "\n",
      "Epoch 70\n",
      "Iteration 1\n",
      "D loss: -47.6197395324707\n",
      "GP: 5.196071147918701\n",
      "Gradient norm: 1.1610785722732544\n",
      "G loss: 5.1670308113098145\n",
      "Iteration 51\n",
      "D loss: -46.04644012451172\n",
      "GP: 7.815728187561035\n",
      "Gradient norm: 1.226234793663025\n",
      "G loss: 5.480053424835205\n",
      "Iteration 101\n",
      "D loss: -44.44530487060547\n",
      "GP: 6.731962203979492\n",
      "Gradient norm: 1.1872376203536987\n",
      "G loss: 4.138239860534668\n",
      "Iteration 151\n",
      "D loss: -42.230201721191406\n",
      "GP: 7.102675437927246\n",
      "Gradient norm: 1.1982370615005493\n",
      "G loss: 3.9037654399871826\n",
      "Iteration 201\n",
      "D loss: -45.28910827636719\n",
      "GP: 6.399477958679199\n",
      "Gradient norm: 1.187301516532898\n",
      "G loss: 5.756385326385498\n",
      "Iteration 251\n",
      "D loss: -46.03961181640625\n",
      "GP: 7.499765396118164\n",
      "Gradient norm: 1.2022844552993774\n",
      "G loss: 5.705276966094971\n",
      "Iteration 301\n",
      "D loss: -47.898170471191406\n",
      "GP: 6.739297866821289\n",
      "Gradient norm: 1.190071702003479\n",
      "G loss: 5.010985851287842\n",
      "Iteration 351\n",
      "D loss: -45.4620361328125\n",
      "GP: 5.5667924880981445\n",
      "Gradient norm: 1.181456208229065\n",
      "G loss: 4.831444263458252\n",
      "\n",
      "Epoch 71\n",
      "Iteration 1\n",
      "D loss: -46.67776107788086\n",
      "GP: 6.908231258392334\n",
      "Gradient norm: 1.190009355545044\n",
      "G loss: 5.651397705078125\n",
      "Iteration 51\n",
      "D loss: -45.639747619628906\n",
      "GP: 5.648179054260254\n",
      "Gradient norm: 1.1609649658203125\n",
      "G loss: 4.2234883308410645\n",
      "Iteration 101\n",
      "D loss: -44.89044189453125\n",
      "GP: 6.112170219421387\n",
      "Gradient norm: 1.1889870166778564\n",
      "G loss: 3.9677531719207764\n",
      "Iteration 151\n",
      "D loss: -46.363868713378906\n",
      "GP: 6.051227569580078\n",
      "Gradient norm: 1.18317449092865\n",
      "G loss: 4.803187370300293\n",
      "Iteration 201\n",
      "D loss: -48.33040237426758\n",
      "GP: 6.513739585876465\n",
      "Gradient norm: 1.183961272239685\n",
      "G loss: 6.16776180267334\n",
      "Iteration 251\n",
      "D loss: -46.87143325805664\n",
      "GP: 7.4298834800720215\n",
      "Gradient norm: 1.2092536687850952\n",
      "G loss: 4.556966304779053\n",
      "Iteration 301\n",
      "D loss: -43.752685546875\n",
      "GP: 6.372528076171875\n",
      "Gradient norm: 1.1955280303955078\n",
      "G loss: 4.622959136962891\n",
      "Iteration 351\n",
      "D loss: -47.02153015136719\n",
      "GP: 7.144245624542236\n",
      "Gradient norm: 1.1967768669128418\n",
      "G loss: 4.00772762298584\n",
      "\n",
      "Epoch 72\n",
      "Iteration 1\n",
      "D loss: -46.167518615722656\n",
      "GP: 6.711263656616211\n",
      "Gradient norm: 1.1956911087036133\n",
      "G loss: 5.17609167098999\n",
      "Iteration 51\n",
      "D loss: -45.23209762573242\n",
      "GP: 7.9071245193481445\n",
      "Gradient norm: 1.2198399305343628\n",
      "G loss: 4.762998580932617\n",
      "Iteration 101\n",
      "D loss: -47.213130950927734\n",
      "GP: 7.939969062805176\n",
      "Gradient norm: 1.217551350593567\n",
      "G loss: 5.915006637573242\n",
      "Iteration 151\n",
      "D loss: -46.38496780395508\n",
      "GP: 7.685863494873047\n",
      "Gradient norm: 1.2232903242111206\n",
      "G loss: 4.405673027038574\n",
      "Iteration 201\n",
      "D loss: -44.092350006103516\n",
      "GP: 6.039102077484131\n",
      "Gradient norm: 1.1837592124938965\n",
      "G loss: 5.249671459197998\n",
      "Iteration 251\n",
      "D loss: -49.52187728881836\n",
      "GP: 7.61688232421875\n",
      "Gradient norm: 1.2015784978866577\n",
      "G loss: 5.418038845062256\n",
      "Iteration 301\n",
      "D loss: -45.982276916503906\n",
      "GP: 8.373554229736328\n",
      "Gradient norm: 1.2263498306274414\n",
      "G loss: 6.157547950744629\n",
      "Iteration 351\n",
      "D loss: -46.453006744384766\n",
      "GP: 7.220249652862549\n",
      "Gradient norm: 1.1992546319961548\n",
      "G loss: 5.495206356048584\n",
      "\n",
      "Epoch 73\n",
      "Iteration 1\n",
      "D loss: -44.095069885253906\n",
      "GP: 6.906026840209961\n",
      "Gradient norm: 1.204591989517212\n",
      "G loss: 6.639685153961182\n",
      "Iteration 51\n",
      "D loss: -46.44488525390625\n",
      "GP: 6.576669692993164\n",
      "Gradient norm: 1.193011999130249\n",
      "G loss: 5.612962245941162\n",
      "Iteration 101\n",
      "D loss: -45.05949401855469\n",
      "GP: 8.12511920928955\n",
      "Gradient norm: 1.219219446182251\n",
      "G loss: 5.656919956207275\n",
      "Iteration 151\n",
      "D loss: -44.00368881225586\n",
      "GP: 4.772627830505371\n",
      "Gradient norm: 1.1439168453216553\n",
      "G loss: 5.823770999908447\n",
      "Iteration 201\n",
      "D loss: -45.746219635009766\n",
      "GP: 7.075408935546875\n",
      "Gradient norm: 1.1841773986816406\n",
      "G loss: 6.437525749206543\n",
      "Iteration 251\n",
      "D loss: -46.391563415527344\n",
      "GP: 7.6399335861206055\n",
      "Gradient norm: 1.2001839876174927\n",
      "G loss: 5.768476486206055\n",
      "Iteration 301\n",
      "D loss: -45.96451187133789\n",
      "GP: 6.281124114990234\n",
      "Gradient norm: 1.1872092485427856\n",
      "G loss: 7.339904308319092\n",
      "Iteration 351\n",
      "D loss: -44.969207763671875\n",
      "GP: 7.493488788604736\n",
      "Gradient norm: 1.201613426208496\n",
      "G loss: 6.3014960289001465\n",
      "\n",
      "Epoch 74\n",
      "Iteration 1\n",
      "D loss: -47.882530212402344\n",
      "GP: 6.596085548400879\n",
      "Gradient norm: 1.1910165548324585\n",
      "G loss: 6.74135160446167\n",
      "Iteration 51\n",
      "D loss: -45.4061393737793\n",
      "GP: 8.040508270263672\n",
      "Gradient norm: 1.2128751277923584\n",
      "G loss: 6.308822154998779\n",
      "Iteration 101\n",
      "D loss: -43.554561614990234\n",
      "GP: 7.29489803314209\n",
      "Gradient norm: 1.2008711099624634\n",
      "G loss: 6.559739589691162\n",
      "Iteration 151\n",
      "D loss: -46.4586181640625\n",
      "GP: 6.894613265991211\n",
      "Gradient norm: 1.1953785419464111\n",
      "G loss: 6.650250434875488\n",
      "Iteration 201\n",
      "D loss: -46.21627426147461\n",
      "GP: 7.549227714538574\n",
      "Gradient norm: 1.2229734659194946\n",
      "G loss: 6.757818222045898\n",
      "Iteration 251\n",
      "D loss: -44.15879821777344\n",
      "GP: 6.30107307434082\n",
      "Gradient norm: 1.1831374168395996\n",
      "G loss: 6.489404678344727\n",
      "Iteration 301\n",
      "D loss: -46.75865173339844\n",
      "GP: 6.075552940368652\n",
      "Gradient norm: 1.1712181568145752\n",
      "G loss: 8.247930526733398\n",
      "Iteration 351\n",
      "D loss: -47.884552001953125\n",
      "GP: 8.198936462402344\n",
      "Gradient norm: 1.2202054262161255\n",
      "G loss: 7.79646635055542\n",
      "\n",
      "Epoch 75\n",
      "Iteration 1\n",
      "D loss: -44.86101150512695\n",
      "GP: 6.560330867767334\n",
      "Gradient norm: 1.1951931715011597\n",
      "G loss: 6.718258857727051\n",
      "Iteration 51\n",
      "D loss: -45.0263786315918\n",
      "GP: 8.275161743164062\n",
      "Gradient norm: 1.2187533378601074\n",
      "G loss: 6.036637306213379\n",
      "Iteration 101\n",
      "D loss: -45.38189697265625\n",
      "GP: 5.691605567932129\n",
      "Gradient norm: 1.1723237037658691\n",
      "G loss: 6.656651973724365\n",
      "Iteration 151\n",
      "D loss: -48.869117736816406\n",
      "GP: 6.740322589874268\n",
      "Gradient norm: 1.1993573904037476\n",
      "G loss: 7.787210464477539\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 201\n",
      "D loss: -47.936927795410156\n",
      "GP: 7.297834873199463\n",
      "Gradient norm: 1.2126613855361938\n",
      "G loss: 6.851300239562988\n",
      "Iteration 251\n",
      "D loss: -46.96739196777344\n",
      "GP: 6.917398452758789\n",
      "Gradient norm: 1.1967356204986572\n",
      "G loss: 8.157224655151367\n",
      "Iteration 301\n",
      "D loss: -44.510467529296875\n",
      "GP: 6.747026443481445\n",
      "Gradient norm: 1.1883633136749268\n",
      "G loss: 8.086764335632324\n",
      "Iteration 351\n",
      "D loss: -46.43756103515625\n",
      "GP: 6.9767608642578125\n",
      "Gradient norm: 1.1932075023651123\n",
      "G loss: 7.245584964752197\n",
      "\n",
      "Epoch 76\n",
      "Iteration 1\n",
      "D loss: -45.38705062866211\n",
      "GP: 8.404267311096191\n",
      "Gradient norm: 1.2167800664901733\n",
      "G loss: 7.145129680633545\n",
      "Iteration 51\n",
      "D loss: -43.9930419921875\n",
      "GP: 6.710768699645996\n",
      "Gradient norm: 1.1856746673583984\n",
      "G loss: 6.96753454208374\n",
      "Iteration 101\n",
      "D loss: -45.18671798706055\n",
      "GP: 5.3817291259765625\n",
      "Gradient norm: 1.1642223596572876\n",
      "G loss: 7.205475807189941\n",
      "Iteration 151\n",
      "D loss: -45.31471252441406\n",
      "GP: 6.133709907531738\n",
      "Gradient norm: 1.1899869441986084\n",
      "G loss: 9.955531120300293\n",
      "Iteration 201\n",
      "D loss: -45.42625427246094\n",
      "GP: 6.392009258270264\n",
      "Gradient norm: 1.1998058557510376\n",
      "G loss: 8.877370834350586\n",
      "Iteration 251\n",
      "D loss: -44.4709587097168\n",
      "GP: 7.9979329109191895\n",
      "Gradient norm: 1.2230002880096436\n",
      "G loss: 8.989214897155762\n",
      "Iteration 301\n",
      "D loss: -48.15751647949219\n",
      "GP: 5.929342269897461\n",
      "Gradient norm: 1.1810113191604614\n",
      "G loss: 7.509098529815674\n",
      "Iteration 351\n",
      "D loss: -43.79829788208008\n",
      "GP: 7.127444744110107\n",
      "Gradient norm: 1.1965219974517822\n",
      "G loss: 8.571398735046387\n",
      "\n",
      "Epoch 77\n",
      "Iteration 1\n",
      "D loss: -47.50613021850586\n",
      "GP: 7.739073753356934\n",
      "Gradient norm: 1.2124780416488647\n",
      "G loss: 8.96199893951416\n",
      "Iteration 51\n",
      "D loss: -44.49945831298828\n",
      "GP: 6.750654697418213\n",
      "Gradient norm: 1.1999531984329224\n",
      "G loss: 8.497369766235352\n",
      "Iteration 101\n",
      "D loss: -47.36405944824219\n",
      "GP: 7.076818466186523\n",
      "Gradient norm: 1.1968014240264893\n",
      "G loss: 8.536093711853027\n",
      "Iteration 151\n",
      "D loss: -44.58708190917969\n",
      "GP: 7.411051273345947\n",
      "Gradient norm: 1.2091041803359985\n",
      "G loss: 9.405769348144531\n",
      "Iteration 201\n",
      "D loss: -44.26786804199219\n",
      "GP: 7.419519424438477\n",
      "Gradient norm: 1.2068639993667603\n",
      "G loss: 7.4969024658203125\n",
      "Iteration 251\n",
      "D loss: -45.111778259277344\n",
      "GP: 5.80486536026001\n",
      "Gradient norm: 1.1617003679275513\n",
      "G loss: 9.314746856689453\n",
      "Iteration 301\n",
      "D loss: -44.46587371826172\n",
      "GP: 7.523834228515625\n",
      "Gradient norm: 1.2062678337097168\n",
      "G loss: 9.477558135986328\n",
      "Iteration 351\n",
      "D loss: -44.83501434326172\n",
      "GP: 7.005499839782715\n",
      "Gradient norm: 1.1986043453216553\n",
      "G loss: 9.002130508422852\n",
      "\n",
      "Epoch 78\n",
      "Iteration 1\n",
      "D loss: -46.247718811035156\n",
      "GP: 7.187095642089844\n",
      "Gradient norm: 1.1852036714553833\n",
      "G loss: 7.906615734100342\n",
      "Iteration 51\n",
      "D loss: -45.687538146972656\n",
      "GP: 7.298125267028809\n",
      "Gradient norm: 1.1994255781173706\n",
      "G loss: 8.622344017028809\n",
      "Iteration 101\n",
      "D loss: -44.93317413330078\n",
      "GP: 8.304704666137695\n",
      "Gradient norm: 1.2210421562194824\n",
      "G loss: 8.84045124053955\n",
      "Iteration 151\n",
      "D loss: -45.791114807128906\n",
      "GP: 5.196832656860352\n",
      "Gradient norm: 1.1736445426940918\n",
      "G loss: 8.549554824829102\n",
      "Iteration 201\n",
      "D loss: -48.230926513671875\n",
      "GP: 6.583952903747559\n",
      "Gradient norm: 1.1977605819702148\n",
      "G loss: 7.717716693878174\n",
      "Iteration 251\n",
      "D loss: -45.95809555053711\n",
      "GP: 8.555267333984375\n",
      "Gradient norm: 1.2299352884292603\n",
      "G loss: 8.675539016723633\n",
      "Iteration 301\n",
      "D loss: -49.970340728759766\n",
      "GP: 6.593125343322754\n",
      "Gradient norm: 1.1963165998458862\n",
      "G loss: 8.91036319732666\n",
      "Iteration 351\n",
      "D loss: -45.287681579589844\n",
      "GP: 6.93635892868042\n",
      "Gradient norm: 1.1970802545547485\n",
      "G loss: 10.134950637817383\n",
      "\n",
      "Epoch 79\n",
      "Iteration 1\n",
      "D loss: -46.892547607421875\n",
      "GP: 6.591261863708496\n",
      "Gradient norm: 1.1884456872940063\n",
      "G loss: 9.641121864318848\n",
      "Iteration 51\n",
      "D loss: -42.88711929321289\n",
      "GP: 6.002379417419434\n",
      "Gradient norm: 1.1731550693511963\n",
      "G loss: 8.782379150390625\n",
      "Iteration 101\n",
      "D loss: -44.80520248413086\n",
      "GP: 8.264403343200684\n",
      "Gradient norm: 1.2203296422958374\n",
      "G loss: 9.46861457824707\n",
      "Iteration 151\n",
      "D loss: -44.8414421081543\n",
      "GP: 5.991997718811035\n",
      "Gradient norm: 1.1735475063323975\n",
      "G loss: 8.454362869262695\n",
      "Iteration 201\n",
      "D loss: -45.72148895263672\n",
      "GP: 7.694838047027588\n",
      "Gradient norm: 1.2149806022644043\n",
      "G loss: 9.105701446533203\n",
      "Iteration 251\n",
      "D loss: -45.42933654785156\n",
      "GP: 5.7012410163879395\n",
      "Gradient norm: 1.1773111820220947\n",
      "G loss: 9.400458335876465\n",
      "Iteration 301\n",
      "D loss: -45.1065673828125\n",
      "GP: 6.933444023132324\n",
      "Gradient norm: 1.2026777267456055\n",
      "G loss: 8.834147453308105\n",
      "Iteration 351\n",
      "D loss: -45.56352615356445\n",
      "GP: 6.161134243011475\n",
      "Gradient norm: 1.1844145059585571\n",
      "G loss: 9.707953453063965\n",
      "\n",
      "Epoch 80\n",
      "Iteration 1\n",
      "D loss: -42.30316925048828\n",
      "GP: 5.901269912719727\n",
      "Gradient norm: 1.1757484674453735\n",
      "G loss: 9.646282196044922\n",
      "Iteration 51\n",
      "D loss: -47.712646484375\n",
      "GP: 6.210029602050781\n",
      "Gradient norm: 1.169263243675232\n",
      "G loss: 9.31940746307373\n",
      "Iteration 101\n",
      "D loss: -46.04062271118164\n",
      "GP: 7.539427757263184\n",
      "Gradient norm: 1.2098227739334106\n",
      "G loss: 9.618143081665039\n",
      "Iteration 151\n",
      "D loss: -45.5606575012207\n",
      "GP: 7.136633396148682\n",
      "Gradient norm: 1.2025532722473145\n",
      "G loss: 9.773611068725586\n",
      "Iteration 201\n",
      "D loss: -48.25290298461914\n",
      "GP: 7.984820365905762\n",
      "Gradient norm: 1.2209248542785645\n",
      "G loss: 10.033090591430664\n",
      "Iteration 251\n",
      "D loss: -43.78971862792969\n",
      "GP: 6.465703964233398\n",
      "Gradient norm: 1.1904048919677734\n",
      "G loss: 10.16505241394043\n",
      "Iteration 301\n",
      "D loss: -43.852073669433594\n",
      "GP: 4.972824573516846\n",
      "Gradient norm: 1.1474804878234863\n",
      "G loss: 11.153335571289062\n",
      "Iteration 351\n",
      "D loss: -47.97749328613281\n",
      "GP: 7.9851813316345215\n",
      "Gradient norm: 1.2212679386138916\n",
      "G loss: 10.986615180969238\n",
      "\n",
      "Epoch 81\n",
      "Iteration 1\n",
      "D loss: -45.00979995727539\n",
      "GP: 6.120481967926025\n",
      "Gradient norm: 1.1830836534500122\n",
      "G loss: 11.090429306030273\n",
      "Iteration 51\n",
      "D loss: -47.152950286865234\n",
      "GP: 6.96801233291626\n",
      "Gradient norm: 1.1908999681472778\n",
      "G loss: 10.955339431762695\n",
      "Iteration 101\n",
      "D loss: -47.24190139770508\n",
      "GP: 8.114483833312988\n",
      "Gradient norm: 1.2236918210983276\n",
      "G loss: 10.525217056274414\n",
      "Iteration 151\n",
      "D loss: -44.36537170410156\n",
      "GP: 7.429157257080078\n",
      "Gradient norm: 1.2134188413619995\n",
      "G loss: 10.43455982208252\n",
      "Iteration 201\n",
      "D loss: -44.7059211730957\n",
      "GP: 6.315113067626953\n",
      "Gradient norm: 1.193533182144165\n",
      "G loss: 10.987266540527344\n",
      "Iteration 251\n",
      "D loss: -44.74946212768555\n",
      "GP: 6.181118488311768\n",
      "Gradient norm: 1.1820752620697021\n",
      "G loss: 9.91490364074707\n",
      "Iteration 301\n",
      "D loss: -45.83507537841797\n",
      "GP: 7.182348251342773\n",
      "Gradient norm: 1.200038194656372\n",
      "G loss: 9.959168434143066\n",
      "Iteration 351\n",
      "D loss: -43.86319351196289\n",
      "GP: 6.098740100860596\n",
      "Gradient norm: 1.1786566972732544\n",
      "G loss: 10.202072143554688\n",
      "\n",
      "Epoch 82\n",
      "Iteration 1\n",
      "D loss: -44.486236572265625\n",
      "GP: 7.49826192855835\n",
      "Gradient norm: 1.2058241367340088\n",
      "G loss: 10.942033767700195\n",
      "Iteration 51\n",
      "D loss: -44.092323303222656\n",
      "GP: 7.389616012573242\n",
      "Gradient norm: 1.2187074422836304\n",
      "G loss: 10.546223640441895\n",
      "Iteration 101\n",
      "D loss: -44.17552947998047\n",
      "GP: 6.5960540771484375\n",
      "Gradient norm: 1.189389944076538\n",
      "G loss: 11.434549331665039\n",
      "Iteration 151\n",
      "D loss: -45.57183074951172\n",
      "GP: 6.683038711547852\n",
      "Gradient norm: 1.1892396211624146\n",
      "G loss: 10.10107135772705\n",
      "Iteration 201\n",
      "D loss: -44.816322326660156\n",
      "GP: 6.567966938018799\n",
      "Gradient norm: 1.1803109645843506\n",
      "G loss: 9.946434020996094\n",
      "Iteration 251\n",
      "D loss: -46.374549865722656\n",
      "GP: 8.451314926147461\n",
      "Gradient norm: 1.2226300239562988\n",
      "G loss: 10.508512496948242\n",
      "Iteration 301\n",
      "D loss: -43.92562484741211\n",
      "GP: 5.859631538391113\n",
      "Gradient norm: 1.1622779369354248\n",
      "G loss: 10.619152069091797\n",
      "Iteration 351\n",
      "D loss: -44.156646728515625\n",
      "GP: 6.498814582824707\n",
      "Gradient norm: 1.1860464811325073\n",
      "G loss: 11.37655258178711\n",
      "\n",
      "Epoch 83\n",
      "Iteration 1\n",
      "D loss: -46.086727142333984\n",
      "GP: 6.323570251464844\n",
      "Gradient norm: 1.1818591356277466\n",
      "G loss: 10.573702812194824\n",
      "Iteration 51\n",
      "D loss: -45.34309387207031\n",
      "GP: 5.725142955780029\n",
      "Gradient norm: 1.1754229068756104\n",
      "G loss: 11.206271171569824\n",
      "Iteration 101\n",
      "D loss: -42.7598991394043\n",
      "GP: 7.172138214111328\n",
      "Gradient norm: 1.202413558959961\n",
      "G loss: 11.120898246765137\n",
      "Iteration 151\n",
      "D loss: -46.17018127441406\n",
      "GP: 6.946298599243164\n",
      "Gradient norm: 1.1968469619750977\n",
      "G loss: 9.4623384475708\n",
      "Iteration 201\n",
      "D loss: -45.60965347290039\n",
      "GP: 5.89370059967041\n",
      "Gradient norm: 1.1746034622192383\n",
      "G loss: 10.2323637008667\n",
      "Iteration 251\n",
      "D loss: -45.318328857421875\n",
      "GP: 6.232515335083008\n",
      "Gradient norm: 1.1783242225646973\n",
      "G loss: 10.210217475891113\n",
      "Iteration 301\n",
      "D loss: -44.859195709228516\n",
      "GP: 7.282095432281494\n",
      "Gradient norm: 1.2047301530838013\n",
      "G loss: 10.93575382232666\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 351\n",
      "D loss: -45.498653411865234\n",
      "GP: 4.96498966217041\n",
      "Gradient norm: 1.1608198881149292\n",
      "G loss: 10.543366432189941\n",
      "\n",
      "Epoch 84\n",
      "Iteration 1\n",
      "D loss: -44.61565399169922\n",
      "GP: 8.53077507019043\n",
      "Gradient norm: 1.2232370376586914\n",
      "G loss: 10.269874572753906\n",
      "Iteration 51\n",
      "D loss: -45.26087188720703\n",
      "GP: 8.43883228302002\n",
      "Gradient norm: 1.2213486433029175\n",
      "G loss: 10.496065139770508\n",
      "Iteration 101\n",
      "D loss: -46.58463668823242\n",
      "GP: 5.883950233459473\n",
      "Gradient norm: 1.1811636686325073\n",
      "G loss: 10.234750747680664\n",
      "Iteration 151\n",
      "D loss: -45.646705627441406\n",
      "GP: 6.35365104675293\n",
      "Gradient norm: 1.1843671798706055\n",
      "G loss: 9.330766677856445\n",
      "Iteration 201\n",
      "D loss: -43.7445068359375\n",
      "GP: 6.228899002075195\n",
      "Gradient norm: 1.1782581806182861\n",
      "G loss: 9.254018783569336\n",
      "Iteration 251\n",
      "D loss: -45.19313430786133\n",
      "GP: 6.0490312576293945\n",
      "Gradient norm: 1.1742606163024902\n",
      "G loss: 9.365147590637207\n",
      "Iteration 301\n",
      "D loss: -45.87175750732422\n",
      "GP: 7.033581256866455\n",
      "Gradient norm: 1.2038490772247314\n",
      "G loss: 9.583724021911621\n",
      "Iteration 351\n",
      "D loss: -45.17273712158203\n",
      "GP: 8.369319915771484\n",
      "Gradient norm: 1.2193282842636108\n",
      "G loss: 11.375274658203125\n",
      "\n",
      "Epoch 85\n",
      "Iteration 1\n",
      "D loss: -43.43798065185547\n",
      "GP: 5.783143997192383\n",
      "Gradient norm: 1.1779918670654297\n",
      "G loss: 9.287842750549316\n",
      "Iteration 51\n",
      "D loss: -44.2081184387207\n",
      "GP: 5.511822700500488\n",
      "Gradient norm: 1.1631485223770142\n",
      "G loss: 10.851905822753906\n",
      "Iteration 101\n",
      "D loss: -44.71339797973633\n",
      "GP: 5.051709175109863\n",
      "Gradient norm: 1.1656211614608765\n",
      "G loss: 9.828461647033691\n",
      "Iteration 151\n",
      "D loss: -45.00175857543945\n",
      "GP: 6.6080498695373535\n",
      "Gradient norm: 1.1816877126693726\n",
      "G loss: 9.94828987121582\n",
      "Iteration 201\n",
      "D loss: -45.048500061035156\n",
      "GP: 5.224318981170654\n",
      "Gradient norm: 1.1611392498016357\n",
      "G loss: 11.308497428894043\n",
      "Iteration 251\n",
      "D loss: -43.755577087402344\n",
      "GP: 7.129510879516602\n",
      "Gradient norm: 1.2066826820373535\n",
      "G loss: 10.148229598999023\n",
      "Iteration 301\n",
      "D loss: -43.989158630371094\n",
      "GP: 6.846678733825684\n",
      "Gradient norm: 1.1937155723571777\n",
      "G loss: 9.122663497924805\n",
      "Iteration 351\n",
      "D loss: -46.17023468017578\n",
      "GP: 6.083528995513916\n",
      "Gradient norm: 1.181886911392212\n",
      "G loss: 10.55298900604248\n",
      "\n",
      "Epoch 86\n",
      "Iteration 1\n",
      "D loss: -43.99135971069336\n",
      "GP: 6.356785297393799\n",
      "Gradient norm: 1.1900025606155396\n",
      "G loss: 10.461383819580078\n",
      "Iteration 51\n",
      "D loss: -45.698631286621094\n",
      "GP: 5.771858215332031\n",
      "Gradient norm: 1.1764615774154663\n",
      "G loss: 8.996849060058594\n",
      "Iteration 101\n",
      "D loss: -46.47767639160156\n",
      "GP: 7.626893997192383\n",
      "Gradient norm: 1.2187590599060059\n",
      "G loss: 9.936479568481445\n",
      "Iteration 151\n",
      "D loss: -45.32051086425781\n",
      "GP: 6.923918724060059\n",
      "Gradient norm: 1.1903563737869263\n",
      "G loss: 9.70569133758545\n",
      "Iteration 201\n",
      "D loss: -46.49361038208008\n",
      "GP: 7.042686939239502\n",
      "Gradient norm: 1.1842955350875854\n",
      "G loss: 10.421409606933594\n",
      "Iteration 251\n",
      "D loss: -44.79298400878906\n",
      "GP: 6.959671974182129\n",
      "Gradient norm: 1.2079073190689087\n",
      "G loss: 10.933414459228516\n",
      "Iteration 301\n",
      "D loss: -46.705589294433594\n",
      "GP: 6.818204879760742\n",
      "Gradient norm: 1.1796687841415405\n",
      "G loss: 10.619181632995605\n",
      "Iteration 351\n",
      "D loss: -44.644989013671875\n",
      "GP: 6.129332542419434\n",
      "Gradient norm: 1.1870499849319458\n",
      "G loss: 9.393207550048828\n",
      "\n",
      "Epoch 87\n",
      "Iteration 1\n",
      "D loss: -46.891868591308594\n",
      "GP: 4.802947044372559\n",
      "Gradient norm: 1.154654622077942\n",
      "G loss: 10.907159805297852\n",
      "Iteration 51\n",
      "D loss: -45.16646194458008\n",
      "GP: 7.130298614501953\n",
      "Gradient norm: 1.196752667427063\n",
      "G loss: 9.47275447845459\n",
      "Iteration 101\n",
      "D loss: -43.25084686279297\n",
      "GP: 6.283137798309326\n",
      "Gradient norm: 1.182296872138977\n",
      "G loss: 10.231775283813477\n",
      "Iteration 151\n",
      "D loss: -44.46923065185547\n",
      "GP: 5.890505313873291\n",
      "Gradient norm: 1.172414779663086\n",
      "G loss: 9.042287826538086\n",
      "Iteration 201\n",
      "D loss: -47.24725341796875\n",
      "GP: 7.008599281311035\n",
      "Gradient norm: 1.1854089498519897\n",
      "G loss: 10.59434700012207\n",
      "Iteration 251\n",
      "D loss: -46.291908264160156\n",
      "GP: 6.384099960327148\n",
      "Gradient norm: 1.1801172494888306\n",
      "G loss: 9.565352439880371\n",
      "Iteration 301\n",
      "D loss: -47.40192413330078\n",
      "GP: 6.640573501586914\n",
      "Gradient norm: 1.174835205078125\n",
      "G loss: 10.344048500061035\n",
      "Iteration 351\n",
      "D loss: -46.024410247802734\n",
      "GP: 7.627301216125488\n",
      "Gradient norm: 1.1991591453552246\n",
      "G loss: 10.338736534118652\n",
      "\n",
      "Epoch 88\n",
      "Iteration 1\n",
      "D loss: -43.97173309326172\n",
      "GP: 6.622857570648193\n",
      "Gradient norm: 1.1769013404846191\n",
      "G loss: 10.561395645141602\n",
      "Iteration 51\n",
      "D loss: -44.28082275390625\n",
      "GP: 6.163061141967773\n",
      "Gradient norm: 1.1791290044784546\n",
      "G loss: 10.83871841430664\n",
      "Iteration 101\n",
      "D loss: -46.69811248779297\n",
      "GP: 6.855535507202148\n",
      "Gradient norm: 1.1904003620147705\n",
      "G loss: 11.847310066223145\n",
      "Iteration 151\n",
      "D loss: -45.26666259765625\n",
      "GP: 7.142393589019775\n",
      "Gradient norm: 1.1991535425186157\n",
      "G loss: 11.201215744018555\n",
      "Iteration 201\n",
      "D loss: -45.899105072021484\n",
      "GP: 6.527424335479736\n",
      "Gradient norm: 1.1850016117095947\n",
      "G loss: 12.236127853393555\n",
      "Iteration 251\n",
      "D loss: -44.737586975097656\n",
      "GP: 5.74919319152832\n",
      "Gradient norm: 1.167108416557312\n",
      "G loss: 11.241351127624512\n",
      "Iteration 301\n",
      "D loss: -46.875732421875\n",
      "GP: 6.865958213806152\n",
      "Gradient norm: 1.1849818229675293\n",
      "G loss: 11.255412101745605\n",
      "Iteration 351\n",
      "D loss: -46.50339126586914\n",
      "GP: 7.2255730628967285\n",
      "Gradient norm: 1.2025854587554932\n",
      "G loss: 11.198243141174316\n",
      "\n",
      "Epoch 89\n",
      "Iteration 1\n",
      "D loss: -44.71446990966797\n",
      "GP: 6.455326080322266\n",
      "Gradient norm: 1.1814411878585815\n",
      "G loss: 11.983380317687988\n",
      "Iteration 51\n",
      "D loss: -44.23580551147461\n",
      "GP: 6.944119930267334\n",
      "Gradient norm: 1.1854444742202759\n",
      "G loss: 11.961664199829102\n",
      "Iteration 101\n",
      "D loss: -45.39830780029297\n",
      "GP: 6.872785568237305\n",
      "Gradient norm: 1.1924524307250977\n",
      "G loss: 11.429769515991211\n",
      "Iteration 151\n",
      "D loss: -45.38982009887695\n",
      "GP: 6.387433052062988\n",
      "Gradient norm: 1.1807781457901\n",
      "G loss: 10.469558715820312\n",
      "Iteration 201\n",
      "D loss: -43.68354415893555\n",
      "GP: 8.962296485900879\n",
      "Gradient norm: 1.2295223474502563\n",
      "G loss: 10.828473091125488\n",
      "Iteration 251\n",
      "D loss: -46.14896011352539\n",
      "GP: 6.767273902893066\n",
      "Gradient norm: 1.197031021118164\n",
      "G loss: 9.861732482910156\n",
      "Iteration 301\n",
      "D loss: -45.308475494384766\n",
      "GP: 7.840221405029297\n",
      "Gradient norm: 1.2121442556381226\n",
      "G loss: 11.80367374420166\n",
      "Iteration 351\n",
      "D loss: -44.640380859375\n",
      "GP: 7.6185197830200195\n",
      "Gradient norm: 1.21062171459198\n",
      "G loss: 10.257295608520508\n",
      "\n",
      "Epoch 90\n",
      "Iteration 1\n",
      "D loss: -46.85753631591797\n",
      "GP: 8.137908935546875\n",
      "Gradient norm: 1.2007641792297363\n",
      "G loss: 12.21683406829834\n",
      "Iteration 51\n",
      "D loss: -44.916141510009766\n",
      "GP: 6.959429740905762\n",
      "Gradient norm: 1.1970916986465454\n",
      "G loss: 12.07741641998291\n",
      "Iteration 101\n",
      "D loss: -45.098724365234375\n",
      "GP: 7.17255163192749\n",
      "Gradient norm: 1.191824197769165\n",
      "G loss: 11.312536239624023\n",
      "Iteration 151\n",
      "D loss: -45.599037170410156\n",
      "GP: 6.997627258300781\n",
      "Gradient norm: 1.1900430917739868\n",
      "G loss: 11.29477596282959\n",
      "Iteration 201\n",
      "D loss: -45.27256393432617\n",
      "GP: 8.155194282531738\n",
      "Gradient norm: 1.2165849208831787\n",
      "G loss: 11.125661849975586\n",
      "Iteration 251\n",
      "D loss: -45.737361907958984\n",
      "GP: 5.817765712738037\n",
      "Gradient norm: 1.1590369939804077\n",
      "G loss: 11.233352661132812\n",
      "Iteration 301\n",
      "D loss: -45.43525314331055\n",
      "GP: 6.624747276306152\n",
      "Gradient norm: 1.187307357788086\n",
      "G loss: 10.624714851379395\n",
      "Iteration 351\n",
      "D loss: -42.34145736694336\n",
      "GP: 5.613288879394531\n",
      "Gradient norm: 1.1591837406158447\n",
      "G loss: 11.033153533935547\n",
      "\n",
      "Epoch 91\n",
      "Iteration 1\n",
      "D loss: -45.46200180053711\n",
      "GP: 6.090174198150635\n",
      "Gradient norm: 1.1738941669464111\n",
      "G loss: 9.734586715698242\n",
      "Iteration 51\n",
      "D loss: -45.791725158691406\n",
      "GP: 7.223283767700195\n",
      "Gradient norm: 1.1900871992111206\n",
      "G loss: 9.100278854370117\n",
      "Iteration 101\n",
      "D loss: -44.28724670410156\n",
      "GP: 6.003391742706299\n",
      "Gradient norm: 1.1765944957733154\n",
      "G loss: 10.407316207885742\n",
      "Iteration 151\n",
      "D loss: -43.73281478881836\n",
      "GP: 6.820633411407471\n",
      "Gradient norm: 1.2009133100509644\n",
      "G loss: 10.356557846069336\n",
      "Iteration 201\n",
      "D loss: -44.465030670166016\n",
      "GP: 7.2356672286987305\n",
      "Gradient norm: 1.2042948007583618\n",
      "G loss: 10.550724983215332\n",
      "Iteration 251\n",
      "D loss: -43.056365966796875\n",
      "GP: 7.354280948638916\n",
      "Gradient norm: 1.2033978700637817\n",
      "G loss: 9.353241920471191\n",
      "Iteration 301\n",
      "D loss: -46.06946563720703\n",
      "GP: 6.0393385887146\n",
      "Gradient norm: 1.1664708852767944\n",
      "G loss: 10.913368225097656\n",
      "Iteration 351\n",
      "D loss: -46.000911712646484\n",
      "GP: 6.746633052825928\n",
      "Gradient norm: 1.1853100061416626\n",
      "G loss: 10.19318675994873\n",
      "\n",
      "Epoch 92\n",
      "Iteration 1\n",
      "D loss: -46.586814880371094\n",
      "GP: 5.492245674133301\n",
      "Gradient norm: 1.1616275310516357\n",
      "G loss: 8.876579284667969\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 51\n",
      "D loss: -46.555843353271484\n",
      "GP: 7.206559658050537\n",
      "Gradient norm: 1.2008787393569946\n",
      "G loss: 9.609491348266602\n",
      "Iteration 101\n",
      "D loss: -45.684722900390625\n",
      "GP: 7.326320648193359\n",
      "Gradient norm: 1.2047631740570068\n",
      "G loss: 8.66071605682373\n",
      "Iteration 151\n",
      "D loss: -46.5526237487793\n",
      "GP: 7.371777534484863\n",
      "Gradient norm: 1.1993918418884277\n",
      "G loss: 10.014782905578613\n",
      "Iteration 201\n",
      "D loss: -44.93294143676758\n",
      "GP: 6.869602680206299\n",
      "Gradient norm: 1.1895967721939087\n",
      "G loss: 10.744987487792969\n",
      "Iteration 251\n",
      "D loss: -46.51668167114258\n",
      "GP: 5.945766448974609\n",
      "Gradient norm: 1.1647393703460693\n",
      "G loss: 9.834447860717773\n",
      "Iteration 301\n",
      "D loss: -46.68667221069336\n",
      "GP: 6.562548637390137\n",
      "Gradient norm: 1.177834153175354\n",
      "G loss: 8.856273651123047\n",
      "Iteration 351\n",
      "D loss: -45.331077575683594\n",
      "GP: 7.883969306945801\n",
      "Gradient norm: 1.2034409046173096\n",
      "G loss: 7.777762413024902\n",
      "\n",
      "Epoch 93\n",
      "Iteration 1\n",
      "D loss: -41.79301071166992\n",
      "GP: 7.75674295425415\n",
      "Gradient norm: 1.2065577507019043\n",
      "G loss: 8.98299503326416\n",
      "Iteration 51\n",
      "D loss: -43.26322937011719\n",
      "GP: 8.196743965148926\n",
      "Gradient norm: 1.2025896310806274\n",
      "G loss: 9.866174697875977\n",
      "Iteration 101\n",
      "D loss: -47.07034683227539\n",
      "GP: 7.936779022216797\n",
      "Gradient norm: 1.2035971879959106\n",
      "G loss: 10.064630508422852\n",
      "Iteration 151\n",
      "D loss: -43.415218353271484\n",
      "GP: 6.996083736419678\n",
      "Gradient norm: 1.1937698125839233\n",
      "G loss: 8.61116886138916\n",
      "Iteration 201\n",
      "D loss: -44.13207244873047\n",
      "GP: 6.563656806945801\n",
      "Gradient norm: 1.1738835573196411\n",
      "G loss: 9.078495979309082\n",
      "Iteration 251\n",
      "D loss: -44.116676330566406\n",
      "GP: 7.895571708679199\n",
      "Gradient norm: 1.2029179334640503\n",
      "G loss: 8.794797897338867\n",
      "Iteration 301\n",
      "D loss: -46.18892288208008\n",
      "GP: 6.763744354248047\n",
      "Gradient norm: 1.1975306272506714\n",
      "G loss: 7.7966485023498535\n",
      "Iteration 351\n",
      "D loss: -43.767242431640625\n",
      "GP: 5.794233322143555\n",
      "Gradient norm: 1.1654632091522217\n",
      "G loss: 9.465744972229004\n",
      "\n",
      "Epoch 94\n",
      "Iteration 1\n",
      "D loss: -46.468841552734375\n",
      "GP: 7.853081703186035\n",
      "Gradient norm: 1.1996140480041504\n",
      "G loss: 10.075387001037598\n",
      "Iteration 51\n",
      "D loss: -46.61412811279297\n",
      "GP: 6.087336540222168\n",
      "Gradient norm: 1.1867425441741943\n",
      "G loss: 7.882245063781738\n",
      "Iteration 101\n",
      "D loss: -44.90658187866211\n",
      "GP: 8.036177635192871\n",
      "Gradient norm: 1.2161409854888916\n",
      "G loss: 9.857791900634766\n",
      "Iteration 151\n",
      "D loss: -48.3775520324707\n",
      "GP: 7.147826194763184\n",
      "Gradient norm: 1.196564793586731\n",
      "G loss: 9.005587577819824\n",
      "Iteration 201\n",
      "D loss: -46.30045700073242\n",
      "GP: 5.145237922668457\n",
      "Gradient norm: 1.1491880416870117\n",
      "G loss: 10.137879371643066\n",
      "Iteration 251\n",
      "D loss: -45.772979736328125\n",
      "GP: 8.069844245910645\n",
      "Gradient norm: 1.2035471200942993\n",
      "G loss: 9.026519775390625\n",
      "Iteration 301\n",
      "D loss: -46.51661682128906\n",
      "GP: 8.378952980041504\n",
      "Gradient norm: 1.205818772315979\n",
      "G loss: 9.591911315917969\n",
      "Iteration 351\n",
      "D loss: -45.51143264770508\n",
      "GP: 6.541062355041504\n",
      "Gradient norm: 1.1825428009033203\n",
      "G loss: 9.912151336669922\n",
      "\n",
      "Epoch 95\n",
      "Iteration 1\n",
      "D loss: -45.006168365478516\n",
      "GP: 7.066107749938965\n",
      "Gradient norm: 1.1949851512908936\n",
      "G loss: 9.097128868103027\n",
      "Iteration 51\n",
      "D loss: -44.41602325439453\n",
      "GP: 5.2167887687683105\n",
      "Gradient norm: 1.1557643413543701\n",
      "G loss: 8.319500923156738\n",
      "Iteration 101\n",
      "D loss: -46.71611022949219\n",
      "GP: 7.398040294647217\n",
      "Gradient norm: 1.1924172639846802\n",
      "G loss: 8.7672700881958\n",
      "Iteration 151\n",
      "D loss: -44.44449996948242\n",
      "GP: 6.752490997314453\n",
      "Gradient norm: 1.1663435697555542\n",
      "G loss: 8.433780670166016\n",
      "Iteration 201\n",
      "D loss: -43.814064025878906\n",
      "GP: 9.725797653198242\n",
      "Gradient norm: 1.2242605686187744\n",
      "G loss: 8.433520317077637\n",
      "Iteration 251\n",
      "D loss: -45.611942291259766\n",
      "GP: 7.920003414154053\n",
      "Gradient norm: 1.2061049938201904\n",
      "G loss: 8.488724708557129\n",
      "Iteration 301\n",
      "D loss: -45.76879119873047\n",
      "GP: 6.9947190284729\n",
      "Gradient norm: 1.1978169679641724\n",
      "G loss: 9.690092086791992\n",
      "Iteration 351\n",
      "D loss: -45.744876861572266\n",
      "GP: 6.286470890045166\n",
      "Gradient norm: 1.180748701095581\n",
      "G loss: 7.652944087982178\n",
      "\n",
      "Epoch 96\n",
      "Iteration 1\n",
      "D loss: -45.87383270263672\n",
      "GP: 7.528017997741699\n",
      "Gradient norm: 1.2062112092971802\n",
      "G loss: 9.150674819946289\n",
      "Iteration 51\n",
      "D loss: -44.4676513671875\n",
      "GP: 7.145265579223633\n",
      "Gradient norm: 1.1948022842407227\n",
      "G loss: 8.50171184539795\n",
      "Iteration 101\n",
      "D loss: -43.9389533996582\n",
      "GP: 6.293791770935059\n",
      "Gradient norm: 1.1746405363082886\n",
      "G loss: 8.018478393554688\n",
      "Iteration 151\n",
      "D loss: -45.98426055908203\n",
      "GP: 6.499147891998291\n",
      "Gradient norm: 1.1956002712249756\n",
      "G loss: 8.339896202087402\n",
      "Iteration 201\n",
      "D loss: -43.735740661621094\n",
      "GP: 6.956502914428711\n",
      "Gradient norm: 1.1779512166976929\n",
      "G loss: 8.306831359863281\n",
      "Iteration 251\n",
      "D loss: -47.92958068847656\n",
      "GP: 6.175313949584961\n",
      "Gradient norm: 1.1785953044891357\n",
      "G loss: 8.753702163696289\n",
      "Iteration 301\n",
      "D loss: -46.93103790283203\n",
      "GP: 5.463162422180176\n",
      "Gradient norm: 1.1674094200134277\n",
      "G loss: 9.500651359558105\n",
      "Iteration 351\n",
      "D loss: -47.022945404052734\n",
      "GP: 6.0370917320251465\n",
      "Gradient norm: 1.1705560684204102\n",
      "G loss: 9.45473575592041\n",
      "\n",
      "Epoch 97\n",
      "Iteration 1\n",
      "D loss: -43.631202697753906\n",
      "GP: 6.873907089233398\n",
      "Gradient norm: 1.2035762071609497\n",
      "G loss: 9.303945541381836\n",
      "Iteration 51\n",
      "D loss: -42.387718200683594\n",
      "GP: 6.48822546005249\n",
      "Gradient norm: 1.1740418672561646\n",
      "G loss: 8.32229232788086\n",
      "Iteration 101\n",
      "D loss: -44.0666618347168\n",
      "GP: 6.941745281219482\n",
      "Gradient norm: 1.1926220655441284\n",
      "G loss: 9.418458938598633\n",
      "Iteration 151\n",
      "D loss: -44.21656036376953\n",
      "GP: 5.998344421386719\n",
      "Gradient norm: 1.1768112182617188\n",
      "G loss: 8.978816986083984\n",
      "Iteration 201\n",
      "D loss: -45.95142364501953\n",
      "GP: 6.282984733581543\n",
      "Gradient norm: 1.1827915906906128\n",
      "G loss: 8.228568077087402\n",
      "Iteration 251\n",
      "D loss: -43.73904037475586\n",
      "GP: 6.491148471832275\n",
      "Gradient norm: 1.1866909265518188\n",
      "G loss: 9.241307258605957\n",
      "Iteration 301\n",
      "D loss: -44.932132720947266\n",
      "GP: 8.68170166015625\n",
      "Gradient norm: 1.2200396060943604\n",
      "G loss: 9.10315227508545\n",
      "Iteration 351\n",
      "D loss: -44.42808151245117\n",
      "GP: 6.3959269523620605\n",
      "Gradient norm: 1.179770588874817\n",
      "G loss: 8.872873306274414\n",
      "\n",
      "Epoch 98\n",
      "Iteration 1\n",
      "D loss: -43.82258987426758\n",
      "GP: 8.025981903076172\n",
      "Gradient norm: 1.1983741521835327\n",
      "G loss: 8.505424499511719\n",
      "Iteration 51\n",
      "D loss: -47.86111831665039\n",
      "GP: 6.569218158721924\n",
      "Gradient norm: 1.1879005432128906\n",
      "G loss: 9.200879096984863\n",
      "Iteration 101\n",
      "D loss: -45.22590255737305\n",
      "GP: 5.527697563171387\n",
      "Gradient norm: 1.163920283317566\n",
      "G loss: 8.804171562194824\n",
      "Iteration 151\n",
      "D loss: -43.972171783447266\n",
      "GP: 8.510588645935059\n",
      "Gradient norm: 1.2285184860229492\n",
      "G loss: 10.173386573791504\n",
      "Iteration 201\n",
      "D loss: -44.846900939941406\n",
      "GP: 7.487974166870117\n",
      "Gradient norm: 1.1884292364120483\n",
      "G loss: 9.585874557495117\n",
      "Iteration 251\n",
      "D loss: -46.42327880859375\n",
      "GP: 6.323518753051758\n",
      "Gradient norm: 1.1888025999069214\n",
      "G loss: 9.791580200195312\n",
      "Iteration 301\n",
      "D loss: -44.95806884765625\n",
      "GP: 7.00359582901001\n",
      "Gradient norm: 1.19206702709198\n",
      "G loss: 10.836848258972168\n",
      "Iteration 351\n",
      "D loss: -46.36532974243164\n",
      "GP: 6.775345802307129\n",
      "Gradient norm: 1.192677617073059\n",
      "G loss: 9.818323135375977\n",
      "\n",
      "Epoch 99\n",
      "Iteration 1\n",
      "D loss: -45.634613037109375\n",
      "GP: 9.217283248901367\n",
      "Gradient norm: 1.2123618125915527\n",
      "G loss: 10.568000793457031\n",
      "Iteration 51\n",
      "D loss: -43.14258575439453\n",
      "GP: 6.191389083862305\n",
      "Gradient norm: 1.1684722900390625\n",
      "G loss: 10.81981086730957\n",
      "Iteration 101\n",
      "D loss: -43.40541076660156\n",
      "GP: 6.378454208374023\n",
      "Gradient norm: 1.1778377294540405\n",
      "G loss: 9.943617820739746\n",
      "Iteration 151\n",
      "D loss: -45.95358657836914\n",
      "GP: 7.778165340423584\n",
      "Gradient norm: 1.1929852962493896\n",
      "G loss: 9.773933410644531\n",
      "Iteration 201\n",
      "D loss: -47.074527740478516\n",
      "GP: 7.0584940910339355\n",
      "Gradient norm: 1.1995431184768677\n",
      "G loss: 9.823827743530273\n",
      "Iteration 251\n",
      "D loss: -44.085426330566406\n",
      "GP: 6.598463535308838\n",
      "Gradient norm: 1.1908224821090698\n",
      "G loss: 8.378839492797852\n",
      "Iteration 301\n",
      "D loss: -43.93692398071289\n",
      "GP: 6.7159929275512695\n",
      "Gradient norm: 1.1794081926345825\n",
      "G loss: 9.424118041992188\n",
      "Iteration 351\n",
      "D loss: -45.11882781982422\n",
      "GP: 6.844130039215088\n",
      "Gradient norm: 1.1901283264160156\n",
      "G loss: 9.396780014038086\n",
      "\n",
      "Epoch 100\n",
      "Iteration 1\n",
      "D loss: -45.734092712402344\n",
      "GP: 7.525873184204102\n",
      "Gradient norm: 1.1983885765075684\n",
      "G loss: 9.635703086853027\n",
      "Iteration 51\n",
      "D loss: -44.80666732788086\n",
      "GP: 6.254244804382324\n",
      "Gradient norm: 1.1752287149429321\n",
      "G loss: 9.768134117126465\n",
      "Iteration 101\n",
      "D loss: -43.072174072265625\n",
      "GP: 6.907739639282227\n",
      "Gradient norm: 1.1924233436584473\n",
      "G loss: 8.874161720275879\n",
      "Iteration 151\n",
      "D loss: -42.884437561035156\n",
      "GP: 7.316644191741943\n",
      "Gradient norm: 1.1888113021850586\n",
      "G loss: 9.367177963256836\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 201\n",
      "D loss: -46.504295349121094\n",
      "GP: 7.563918113708496\n",
      "Gradient norm: 1.2140663862228394\n",
      "G loss: 9.776175498962402\n",
      "Iteration 251\n",
      "D loss: -46.510047912597656\n",
      "GP: 7.430845260620117\n",
      "Gradient norm: 1.1979830265045166\n",
      "G loss: 10.059003829956055\n",
      "Iteration 301\n",
      "D loss: -45.41716766357422\n",
      "GP: 7.241462707519531\n",
      "Gradient norm: 1.1807852983474731\n",
      "G loss: 9.067401885986328\n",
      "Iteration 351\n",
      "D loss: -45.758304595947266\n",
      "GP: 7.45175838470459\n",
      "Gradient norm: 1.208736777305603\n",
      "G loss: 7.553319454193115\n",
      "\n",
      "Epoch 101\n",
      "Iteration 1\n",
      "D loss: -45.685081481933594\n",
      "GP: 7.723259925842285\n",
      "Gradient norm: 1.199373483657837\n",
      "G loss: 8.210722923278809\n",
      "Iteration 51\n",
      "D loss: -45.36469268798828\n",
      "GP: 5.979841232299805\n",
      "Gradient norm: 1.1815402507781982\n",
      "G loss: 9.201471328735352\n",
      "Iteration 101\n",
      "D loss: -45.348350524902344\n",
      "GP: 8.78470516204834\n",
      "Gradient norm: 1.2244235277175903\n",
      "G loss: 9.742027282714844\n",
      "Iteration 151\n",
      "D loss: -44.392738342285156\n",
      "GP: 7.682276725769043\n",
      "Gradient norm: 1.1961874961853027\n",
      "G loss: 10.433331489562988\n",
      "Iteration 201\n",
      "D loss: -41.55099105834961\n",
      "GP: 8.430882453918457\n",
      "Gradient norm: 1.214967966079712\n",
      "G loss: 9.893973350524902\n",
      "Iteration 251\n",
      "D loss: -43.86677551269531\n",
      "GP: 6.893542289733887\n",
      "Gradient norm: 1.2060242891311646\n",
      "G loss: 7.591614246368408\n",
      "Iteration 301\n",
      "D loss: -45.17748260498047\n",
      "GP: 7.30921745300293\n",
      "Gradient norm: 1.1923649311065674\n",
      "G loss: 9.539920806884766\n",
      "Iteration 351\n",
      "D loss: -46.165950775146484\n",
      "GP: 5.9774274826049805\n",
      "Gradient norm: 1.182153582572937\n",
      "G loss: 8.190948486328125\n",
      "\n",
      "Epoch 102\n",
      "Iteration 1\n",
      "D loss: -47.961299896240234\n",
      "GP: 7.2798004150390625\n",
      "Gradient norm: 1.1923140287399292\n",
      "G loss: 9.410451889038086\n",
      "Iteration 51\n",
      "D loss: -47.108619689941406\n",
      "GP: 5.23744010925293\n",
      "Gradient norm: 1.1632534265518188\n",
      "G loss: 10.393041610717773\n",
      "Iteration 101\n",
      "D loss: -45.01890182495117\n",
      "GP: 7.294458866119385\n",
      "Gradient norm: 1.2057582139968872\n",
      "G loss: 8.628859519958496\n",
      "Iteration 151\n",
      "D loss: -44.77589416503906\n",
      "GP: 7.8689374923706055\n",
      "Gradient norm: 1.2064640522003174\n",
      "G loss: 9.085491180419922\n",
      "Iteration 201\n",
      "D loss: -47.29689407348633\n",
      "GP: 9.14214038848877\n",
      "Gradient norm: 1.2375271320343018\n",
      "G loss: 9.074250221252441\n",
      "Iteration 251\n",
      "D loss: -43.75320053100586\n",
      "GP: 7.5290350914001465\n",
      "Gradient norm: 1.2022919654846191\n",
      "G loss: 9.22920036315918\n",
      "Iteration 301\n",
      "D loss: -45.79344177246094\n",
      "GP: 6.59684419631958\n",
      "Gradient norm: 1.1864036321640015\n",
      "G loss: 9.517465591430664\n",
      "Iteration 351\n",
      "D loss: -45.326114654541016\n",
      "GP: 6.3327956199646\n",
      "Gradient norm: 1.175689935684204\n",
      "G loss: 9.204450607299805\n",
      "\n",
      "Epoch 103\n",
      "Iteration 1\n",
      "D loss: -44.18257141113281\n",
      "GP: 6.473054885864258\n",
      "Gradient norm: 1.1818370819091797\n",
      "G loss: 9.93161392211914\n",
      "Iteration 51\n",
      "D loss: -46.866458892822266\n",
      "GP: 6.092793941497803\n",
      "Gradient norm: 1.1843559741973877\n",
      "G loss: 9.772339820861816\n",
      "Iteration 101\n",
      "D loss: -45.03803253173828\n",
      "GP: 8.086699485778809\n",
      "Gradient norm: 1.1905269622802734\n",
      "G loss: 8.785798072814941\n",
      "Iteration 151\n",
      "D loss: -44.52140426635742\n",
      "GP: 8.021218299865723\n",
      "Gradient norm: 1.2147928476333618\n",
      "G loss: 9.842177391052246\n",
      "Iteration 201\n",
      "D loss: -42.74162673950195\n",
      "GP: 6.619372367858887\n",
      "Gradient norm: 1.1833035945892334\n",
      "G loss: 10.046830177307129\n",
      "Iteration 251\n",
      "D loss: -45.87784194946289\n",
      "GP: 8.315596580505371\n",
      "Gradient norm: 1.2213400602340698\n",
      "G loss: 8.68130874633789\n",
      "Iteration 301\n",
      "D loss: -43.74932861328125\n",
      "GP: 8.554980278015137\n",
      "Gradient norm: 1.220015287399292\n",
      "G loss: 11.078770637512207\n",
      "Iteration 351\n",
      "D loss: -43.26176452636719\n",
      "GP: 5.486650466918945\n",
      "Gradient norm: 1.151276707649231\n",
      "G loss: 9.887901306152344\n",
      "\n",
      "Epoch 104\n",
      "Iteration 1\n",
      "D loss: -42.79385757446289\n",
      "GP: 8.368595123291016\n",
      "Gradient norm: 1.216655969619751\n",
      "G loss: 9.486364364624023\n",
      "Iteration 51\n",
      "D loss: -44.311546325683594\n",
      "GP: 7.2149224281311035\n",
      "Gradient norm: 1.1935484409332275\n",
      "G loss: 9.373493194580078\n",
      "Iteration 101\n",
      "D loss: -47.63229751586914\n",
      "GP: 7.734897136688232\n",
      "Gradient norm: 1.2028859853744507\n",
      "G loss: 9.333710670471191\n",
      "Iteration 151\n",
      "D loss: -42.79209518432617\n",
      "GP: 6.755659580230713\n",
      "Gradient norm: 1.1756305694580078\n",
      "G loss: 9.719764709472656\n",
      "Iteration 201\n",
      "D loss: -46.56223678588867\n",
      "GP: 7.748332500457764\n",
      "Gradient norm: 1.2067108154296875\n",
      "G loss: 8.79661750793457\n",
      "Iteration 251\n",
      "D loss: -45.761558532714844\n",
      "GP: 8.208642959594727\n",
      "Gradient norm: 1.2160837650299072\n",
      "G loss: 9.193635940551758\n",
      "Iteration 301\n",
      "D loss: -46.18512725830078\n",
      "GP: 7.629055976867676\n",
      "Gradient norm: 1.2025314569473267\n",
      "G loss: 10.111112594604492\n",
      "Iteration 351\n",
      "D loss: -44.34606170654297\n",
      "GP: 8.206335067749023\n",
      "Gradient norm: 1.219225525856018\n",
      "G loss: 9.55140209197998\n",
      "\n",
      "Epoch 105\n",
      "Iteration 1\n",
      "D loss: -44.65514373779297\n",
      "GP: 7.540900707244873\n",
      "Gradient norm: 1.197089433670044\n",
      "G loss: 11.254621505737305\n",
      "Iteration 51\n",
      "D loss: -46.441253662109375\n",
      "GP: 5.222341537475586\n",
      "Gradient norm: 1.1481044292449951\n",
      "G loss: 8.74440860748291\n",
      "Iteration 101\n",
      "D loss: -43.71949768066406\n",
      "GP: 6.784400939941406\n",
      "Gradient norm: 1.1931469440460205\n",
      "G loss: 9.581555366516113\n",
      "Iteration 151\n",
      "D loss: -46.15251922607422\n",
      "GP: 8.245046615600586\n",
      "Gradient norm: 1.2129038572311401\n",
      "G loss: 8.432604789733887\n",
      "Iteration 201\n",
      "D loss: -45.368717193603516\n",
      "GP: 8.705998420715332\n",
      "Gradient norm: 1.2264344692230225\n",
      "G loss: 10.157628059387207\n",
      "Iteration 251\n",
      "D loss: -45.79928207397461\n",
      "GP: 6.566312313079834\n",
      "Gradient norm: 1.188066005706787\n",
      "G loss: 8.019678115844727\n",
      "Iteration 301\n",
      "D loss: -44.273094177246094\n",
      "GP: 6.485944747924805\n",
      "Gradient norm: 1.1679283380508423\n",
      "G loss: 8.47260856628418\n",
      "Iteration 351\n",
      "D loss: -42.828399658203125\n",
      "GP: 7.416791915893555\n",
      "Gradient norm: 1.2009581327438354\n",
      "G loss: 8.872415542602539\n",
      "\n",
      "Epoch 106\n",
      "Iteration 1\n",
      "D loss: -45.784149169921875\n",
      "GP: 7.957525253295898\n",
      "Gradient norm: 1.2074401378631592\n",
      "G loss: 8.69406509399414\n",
      "Iteration 51\n",
      "D loss: -47.16947555541992\n",
      "GP: 6.201554298400879\n",
      "Gradient norm: 1.1895480155944824\n",
      "G loss: 9.340989112854004\n",
      "Iteration 101\n",
      "D loss: -43.653892517089844\n",
      "GP: 6.979935169219971\n",
      "Gradient norm: 1.191403865814209\n",
      "G loss: 10.504096031188965\n",
      "Iteration 151\n",
      "D loss: -44.44700622558594\n",
      "GP: 6.902009963989258\n",
      "Gradient norm: 1.1986360549926758\n",
      "G loss: 10.697303771972656\n",
      "Iteration 201\n",
      "D loss: -46.700340270996094\n",
      "GP: 9.614895820617676\n",
      "Gradient norm: 1.2438318729400635\n",
      "G loss: 9.791162490844727\n",
      "Iteration 251\n",
      "D loss: -44.657676696777344\n",
      "GP: 7.764884948730469\n",
      "Gradient norm: 1.2071722745895386\n",
      "G loss: 9.807792663574219\n",
      "Iteration 301\n",
      "D loss: -41.81338882446289\n",
      "GP: 6.9702043533325195\n",
      "Gradient norm: 1.1881084442138672\n",
      "G loss: 9.104482650756836\n",
      "Iteration 351\n",
      "D loss: -43.87773132324219\n",
      "GP: 5.666341781616211\n",
      "Gradient norm: 1.1727349758148193\n",
      "G loss: 8.87202262878418\n",
      "\n",
      "Epoch 107\n",
      "Iteration 1\n",
      "D loss: -45.315101623535156\n",
      "GP: 6.736619472503662\n",
      "Gradient norm: 1.1811630725860596\n",
      "G loss: 10.650601387023926\n",
      "Iteration 51\n",
      "D loss: -44.90795135498047\n",
      "GP: 8.025750160217285\n",
      "Gradient norm: 1.210838794708252\n",
      "G loss: 10.048393249511719\n",
      "Iteration 101\n",
      "D loss: -47.86244583129883\n",
      "GP: 7.329536437988281\n",
      "Gradient norm: 1.1870579719543457\n",
      "G loss: 10.073768615722656\n",
      "Iteration 151\n",
      "D loss: -44.48362731933594\n",
      "GP: 7.780239582061768\n",
      "Gradient norm: 1.213511347770691\n",
      "G loss: 8.506928443908691\n",
      "Iteration 201\n",
      "D loss: -44.720333099365234\n",
      "GP: 5.926441192626953\n",
      "Gradient norm: 1.1604313850402832\n",
      "G loss: 9.304448127746582\n",
      "Iteration 251\n",
      "D loss: -44.30729293823242\n",
      "GP: 6.5107574462890625\n",
      "Gradient norm: 1.1686478853225708\n",
      "G loss: 9.27726936340332\n",
      "Iteration 301\n",
      "D loss: -44.666847229003906\n",
      "GP: 6.955972671508789\n",
      "Gradient norm: 1.1866525411605835\n",
      "G loss: 9.460193634033203\n",
      "Iteration 351\n",
      "D loss: -44.79353332519531\n",
      "GP: 8.124185562133789\n",
      "Gradient norm: 1.2132368087768555\n",
      "G loss: 9.979477882385254\n",
      "\n",
      "Epoch 108\n",
      "Iteration 1\n",
      "D loss: -43.3579216003418\n",
      "GP: 6.182159423828125\n",
      "Gradient norm: 1.1689093112945557\n",
      "G loss: 9.889260292053223\n",
      "Iteration 51\n",
      "D loss: -40.137474060058594\n",
      "GP: 5.190176963806152\n",
      "Gradient norm: 1.142749547958374\n",
      "G loss: 8.918508529663086\n",
      "Iteration 101\n",
      "D loss: -44.16279220581055\n",
      "GP: 7.7125935554504395\n",
      "Gradient norm: 1.201593279838562\n",
      "G loss: 10.266060829162598\n",
      "Iteration 151\n",
      "D loss: -42.52809143066406\n",
      "GP: 7.33989143371582\n",
      "Gradient norm: 1.1833739280700684\n",
      "G loss: 9.22612476348877\n",
      "Iteration 201\n",
      "D loss: -42.83741760253906\n",
      "GP: 5.095129013061523\n",
      "Gradient norm: 1.1476465463638306\n",
      "G loss: 9.720129013061523\n",
      "Iteration 251\n",
      "D loss: -45.15772247314453\n",
      "GP: 6.901656150817871\n",
      "Gradient norm: 1.187078595161438\n",
      "G loss: 9.828195571899414\n",
      "Iteration 301\n",
      "D loss: -43.19921112060547\n",
      "GP: 5.161811828613281\n",
      "Gradient norm: 1.1445469856262207\n",
      "G loss: 10.804630279541016\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 351\n",
      "D loss: -44.746673583984375\n",
      "GP: 7.268763065338135\n",
      "Gradient norm: 1.2005029916763306\n",
      "G loss: 9.635912895202637\n",
      "\n",
      "Epoch 109\n",
      "Iteration 1\n",
      "D loss: -45.69835662841797\n",
      "GP: 6.686895370483398\n",
      "Gradient norm: 1.1862571239471436\n",
      "G loss: 10.351316452026367\n",
      "Iteration 51\n",
      "D loss: -44.310028076171875\n",
      "GP: 6.979966640472412\n",
      "Gradient norm: 1.167422890663147\n",
      "G loss: 8.918109893798828\n",
      "Iteration 101\n",
      "D loss: -46.76129913330078\n",
      "GP: 7.188240051269531\n",
      "Gradient norm: 1.1942434310913086\n",
      "G loss: 10.53144645690918\n",
      "Iteration 151\n",
      "D loss: -43.25886917114258\n",
      "GP: 6.227840900421143\n",
      "Gradient norm: 1.177178144454956\n",
      "G loss: 9.303386688232422\n",
      "Iteration 201\n",
      "D loss: -45.15656280517578\n",
      "GP: 5.08408784866333\n",
      "Gradient norm: 1.167891263961792\n",
      "G loss: 10.318650245666504\n",
      "Iteration 251\n",
      "D loss: -43.16193389892578\n",
      "GP: 8.095462799072266\n",
      "Gradient norm: 1.2095528841018677\n",
      "G loss: 10.56124496459961\n",
      "Iteration 301\n",
      "D loss: -44.26552963256836\n",
      "GP: 6.455214023590088\n",
      "Gradient norm: 1.1891968250274658\n",
      "G loss: 9.858526229858398\n",
      "Iteration 351\n",
      "D loss: -43.521854400634766\n",
      "GP: 5.468327522277832\n",
      "Gradient norm: 1.1446725130081177\n",
      "G loss: 9.955294609069824\n",
      "\n",
      "Epoch 110\n",
      "Iteration 1\n",
      "D loss: -45.36709976196289\n",
      "GP: 6.685299396514893\n",
      "Gradient norm: 1.1653026342391968\n",
      "G loss: 9.866931915283203\n",
      "Iteration 51\n",
      "D loss: -45.37974548339844\n",
      "GP: 6.8043622970581055\n",
      "Gradient norm: 1.1865103244781494\n",
      "G loss: 10.340328216552734\n",
      "Iteration 101\n",
      "D loss: -43.68021774291992\n",
      "GP: 7.456843852996826\n",
      "Gradient norm: 1.2095798254013062\n",
      "G loss: 10.156286239624023\n",
      "Iteration 151\n",
      "D loss: -43.65094757080078\n",
      "GP: 6.613997459411621\n",
      "Gradient norm: 1.1805509328842163\n",
      "G loss: 10.678781509399414\n",
      "Iteration 201\n",
      "D loss: -43.4357795715332\n",
      "GP: 6.669283390045166\n",
      "Gradient norm: 1.1886478662490845\n",
      "G loss: 11.075882911682129\n",
      "Iteration 251\n",
      "D loss: -45.88667297363281\n",
      "GP: 8.41350269317627\n",
      "Gradient norm: 1.2139772176742554\n",
      "G loss: 10.837570190429688\n",
      "Iteration 301\n",
      "D loss: -43.37492752075195\n",
      "GP: 7.7787089347839355\n",
      "Gradient norm: 1.2165268659591675\n",
      "G loss: 10.245743751525879\n",
      "Iteration 351\n",
      "D loss: -43.118289947509766\n",
      "GP: 8.51924991607666\n",
      "Gradient norm: 1.2076436281204224\n",
      "G loss: 10.084391593933105\n",
      "\n",
      "Epoch 111\n",
      "Iteration 1\n",
      "D loss: -44.91738510131836\n",
      "GP: 6.9090118408203125\n",
      "Gradient norm: 1.1911793947219849\n",
      "G loss: 10.676265716552734\n",
      "Iteration 51\n",
      "D loss: -43.37523651123047\n",
      "GP: 6.238645553588867\n",
      "Gradient norm: 1.179895281791687\n",
      "G loss: 11.745807647705078\n",
      "Iteration 101\n",
      "D loss: -44.94696044921875\n",
      "GP: 6.804872512817383\n",
      "Gradient norm: 1.1762003898620605\n",
      "G loss: 9.803611755371094\n",
      "Iteration 151\n",
      "D loss: -45.22096252441406\n",
      "GP: 6.354253768920898\n",
      "Gradient norm: 1.1755727529525757\n",
      "G loss: 10.350150108337402\n",
      "Iteration 201\n",
      "D loss: -45.2386589050293\n",
      "GP: 6.644284248352051\n",
      "Gradient norm: 1.1818376779556274\n",
      "G loss: 11.504556655883789\n",
      "Iteration 251\n",
      "D loss: -43.3026123046875\n",
      "GP: 7.238199710845947\n",
      "Gradient norm: 1.1859487295150757\n",
      "G loss: 11.007710456848145\n",
      "Iteration 301\n",
      "D loss: -45.315826416015625\n",
      "GP: 6.966156959533691\n",
      "Gradient norm: 1.184715747833252\n",
      "G loss: 11.732205390930176\n",
      "Iteration 351\n",
      "D loss: -41.51328659057617\n",
      "GP: 7.197329998016357\n",
      "Gradient norm: 1.2066755294799805\n",
      "G loss: 9.590638160705566\n",
      "\n",
      "Epoch 112\n",
      "Iteration 1\n",
      "D loss: -42.89424133300781\n",
      "GP: 8.977737426757812\n",
      "Gradient norm: 1.219763994216919\n",
      "G loss: 9.448783874511719\n",
      "Iteration 51\n",
      "D loss: -42.37442398071289\n",
      "GP: 6.667625904083252\n",
      "Gradient norm: 1.188128113746643\n",
      "G loss: 11.701078414916992\n",
      "Iteration 101\n",
      "D loss: -45.390647888183594\n",
      "GP: 8.104973793029785\n",
      "Gradient norm: 1.1953160762786865\n",
      "G loss: 10.849246978759766\n",
      "Iteration 151\n",
      "D loss: -44.9223747253418\n",
      "GP: 6.264028072357178\n",
      "Gradient norm: 1.192940592765808\n",
      "G loss: 11.213593482971191\n",
      "Iteration 201\n",
      "D loss: -43.18301773071289\n",
      "GP: 6.2308197021484375\n",
      "Gradient norm: 1.167029619216919\n",
      "G loss: 10.89282512664795\n",
      "Iteration 251\n",
      "D loss: -43.29773712158203\n",
      "GP: 7.356672763824463\n",
      "Gradient norm: 1.2026314735412598\n",
      "G loss: 12.713364601135254\n",
      "Iteration 301\n",
      "D loss: -45.66609191894531\n",
      "GP: 6.721829891204834\n",
      "Gradient norm: 1.1632212400436401\n",
      "G loss: 12.222370147705078\n",
      "Iteration 351\n",
      "D loss: -42.908103942871094\n",
      "GP: 5.2950005531311035\n",
      "Gradient norm: 1.1619799137115479\n",
      "G loss: 10.982186317443848\n",
      "\n",
      "Epoch 113\n",
      "Iteration 1\n",
      "D loss: -43.17827224731445\n",
      "GP: 6.003909587860107\n",
      "Gradient norm: 1.1753931045532227\n",
      "G loss: 10.824468612670898\n",
      "Iteration 51\n",
      "D loss: -43.47478103637695\n",
      "GP: 7.702785491943359\n",
      "Gradient norm: 1.1925662755966187\n",
      "G loss: 11.157928466796875\n",
      "Iteration 101\n",
      "D loss: -42.271156311035156\n",
      "GP: 7.122564792633057\n",
      "Gradient norm: 1.1873230934143066\n",
      "G loss: 11.794845581054688\n",
      "Iteration 151\n",
      "D loss: -45.00978469848633\n",
      "GP: 6.229784965515137\n",
      "Gradient norm: 1.174156665802002\n",
      "G loss: 10.748298645019531\n",
      "Iteration 201\n",
      "D loss: -45.288612365722656\n",
      "GP: 7.282957077026367\n",
      "Gradient norm: 1.1984901428222656\n",
      "G loss: 11.440038681030273\n",
      "Iteration 251\n",
      "D loss: -43.326148986816406\n",
      "GP: 7.4623494148254395\n",
      "Gradient norm: 1.1882444620132446\n",
      "G loss: 11.881996154785156\n",
      "Iteration 301\n",
      "D loss: -40.51676940917969\n",
      "GP: 6.200024604797363\n",
      "Gradient norm: 1.1641814708709717\n",
      "G loss: 11.67456340789795\n",
      "Iteration 351\n",
      "D loss: -44.79197311401367\n",
      "GP: 7.069880485534668\n",
      "Gradient norm: 1.1843504905700684\n",
      "G loss: 10.888260841369629\n",
      "\n",
      "Epoch 114\n",
      "Iteration 1\n",
      "D loss: -42.18059158325195\n",
      "GP: 7.4882025718688965\n",
      "Gradient norm: 1.1723597049713135\n",
      "G loss: 11.82040786743164\n",
      "Iteration 51\n",
      "D loss: -41.67106246948242\n",
      "GP: 6.354182243347168\n",
      "Gradient norm: 1.1760776042938232\n",
      "G loss: 11.058307647705078\n",
      "Iteration 101\n",
      "D loss: -43.414833068847656\n",
      "GP: 6.647468090057373\n",
      "Gradient norm: 1.1740360260009766\n",
      "G loss: 11.160513877868652\n",
      "Iteration 151\n",
      "D loss: -43.57131576538086\n",
      "GP: 7.859310626983643\n",
      "Gradient norm: 1.2110084295272827\n",
      "G loss: 11.857792854309082\n",
      "Iteration 201\n",
      "D loss: -44.1623649597168\n",
      "GP: 6.525784015655518\n",
      "Gradient norm: 1.1810375452041626\n",
      "G loss: 10.866424560546875\n",
      "Iteration 251\n",
      "D loss: -44.746986389160156\n",
      "GP: 7.060802459716797\n",
      "Gradient norm: 1.1964884996414185\n",
      "G loss: 11.041661262512207\n",
      "Iteration 301\n",
      "D loss: -44.639827728271484\n",
      "GP: 5.5527215003967285\n",
      "Gradient norm: 1.1551594734191895\n",
      "G loss: 10.961183547973633\n",
      "Iteration 351\n",
      "D loss: -44.61275100708008\n",
      "GP: 5.492079734802246\n",
      "Gradient norm: 1.1596193313598633\n",
      "G loss: 12.607039451599121\n",
      "\n",
      "Epoch 115\n",
      "Iteration 1\n",
      "D loss: -43.67845153808594\n",
      "GP: 7.321481704711914\n",
      "Gradient norm: 1.2016407251358032\n",
      "G loss: 11.780941009521484\n",
      "Iteration 51\n",
      "D loss: -42.14318084716797\n",
      "GP: 8.39290714263916\n",
      "Gradient norm: 1.2169345617294312\n",
      "G loss: 10.642923355102539\n",
      "Iteration 101\n",
      "D loss: -43.975650787353516\n",
      "GP: 6.404271125793457\n",
      "Gradient norm: 1.1819279193878174\n",
      "G loss: 12.62557315826416\n",
      "Iteration 151\n",
      "D loss: -43.014122009277344\n",
      "GP: 7.3384199142456055\n",
      "Gradient norm: 1.1966391801834106\n",
      "G loss: 11.243175506591797\n",
      "Iteration 201\n",
      "D loss: -44.40099334716797\n",
      "GP: 6.205925941467285\n",
      "Gradient norm: 1.1701958179473877\n",
      "G loss: 12.807594299316406\n",
      "Iteration 251\n",
      "D loss: -45.684844970703125\n",
      "GP: 8.355793952941895\n",
      "Gradient norm: 1.2073307037353516\n",
      "G loss: 11.767878532409668\n",
      "Iteration 301\n",
      "D loss: -44.197505950927734\n",
      "GP: 6.437004089355469\n",
      "Gradient norm: 1.194792628288269\n",
      "G loss: 12.192068099975586\n",
      "Iteration 351\n",
      "D loss: -46.0418815612793\n",
      "GP: 8.618389129638672\n",
      "Gradient norm: 1.2307482957839966\n",
      "G loss: 11.91955852508545\n",
      "\n",
      "Epoch 116\n",
      "Iteration 1\n",
      "D loss: -42.22174072265625\n",
      "GP: 7.00295352935791\n",
      "Gradient norm: 1.2038965225219727\n",
      "G loss: 12.33532428741455\n",
      "Iteration 51\n",
      "D loss: -42.56488037109375\n",
      "GP: 7.006507873535156\n",
      "Gradient norm: 1.1898162364959717\n",
      "G loss: 12.149018287658691\n",
      "Iteration 101\n",
      "D loss: -44.08906555175781\n",
      "GP: 8.538323402404785\n",
      "Gradient norm: 1.2263718843460083\n",
      "G loss: 12.631023406982422\n",
      "Iteration 151\n",
      "D loss: -43.16364669799805\n",
      "GP: 8.236832618713379\n",
      "Gradient norm: 1.203038215637207\n",
      "G loss: 12.475205421447754\n",
      "Iteration 201\n",
      "D loss: -43.41985321044922\n",
      "GP: 6.784092903137207\n",
      "Gradient norm: 1.1883113384246826\n",
      "G loss: 15.10833740234375\n",
      "Iteration 251\n",
      "D loss: -43.773738861083984\n",
      "GP: 7.104016304016113\n",
      "Gradient norm: 1.187286376953125\n",
      "G loss: 15.435037612915039\n",
      "Iteration 301\n",
      "D loss: -43.09878158569336\n",
      "GP: 7.071187496185303\n",
      "Gradient norm: 1.1864327192306519\n",
      "G loss: 14.491326332092285\n",
      "Iteration 351\n",
      "D loss: -44.93095779418945\n",
      "GP: 6.166760444641113\n",
      "Gradient norm: 1.174288272857666\n",
      "G loss: 15.781964302062988\n",
      "\n",
      "Epoch 117\n",
      "Iteration 1\n",
      "D loss: -43.83623123168945\n",
      "GP: 5.536531925201416\n",
      "Gradient norm: 1.1629271507263184\n",
      "G loss: 15.826223373413086\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 51\n",
      "D loss: -44.796485900878906\n",
      "GP: 6.692670822143555\n",
      "Gradient norm: 1.1913654804229736\n",
      "G loss: 15.703472137451172\n",
      "Iteration 101\n",
      "D loss: -42.771724700927734\n",
      "GP: 7.469284534454346\n",
      "Gradient norm: 1.1837302446365356\n",
      "G loss: 15.30087947845459\n",
      "Iteration 151\n",
      "D loss: -45.398826599121094\n",
      "GP: 6.739487648010254\n",
      "Gradient norm: 1.1902005672454834\n",
      "G loss: 12.654278755187988\n",
      "Iteration 201\n",
      "D loss: -43.5601921081543\n",
      "GP: 7.48258113861084\n",
      "Gradient norm: 1.1947473287582397\n",
      "G loss: 13.993301391601562\n",
      "Iteration 251\n",
      "D loss: -43.683719635009766\n",
      "GP: 5.397091865539551\n",
      "Gradient norm: 1.1524208784103394\n",
      "G loss: 14.341686248779297\n",
      "Iteration 301\n",
      "D loss: -43.685829162597656\n",
      "GP: 6.638650894165039\n",
      "Gradient norm: 1.1881599426269531\n",
      "G loss: 15.994348526000977\n",
      "Iteration 351\n",
      "D loss: -44.098785400390625\n",
      "GP: 5.342956066131592\n",
      "Gradient norm: 1.1611653566360474\n",
      "G loss: 15.32783031463623\n",
      "\n",
      "Epoch 118\n",
      "Iteration 1\n",
      "D loss: -44.28699493408203\n",
      "GP: 7.386373519897461\n",
      "Gradient norm: 1.2087732553482056\n",
      "G loss: 15.319580078125\n",
      "Iteration 51\n",
      "D loss: -43.26909637451172\n",
      "GP: 5.942906379699707\n",
      "Gradient norm: 1.1647858619689941\n",
      "G loss: 16.388225555419922\n",
      "Iteration 101\n",
      "D loss: -41.80786895751953\n",
      "GP: 4.96938943862915\n",
      "Gradient norm: 1.1508240699768066\n",
      "G loss: 15.870524406433105\n",
      "Iteration 151\n",
      "D loss: -45.769287109375\n",
      "GP: 6.283998966217041\n",
      "Gradient norm: 1.1787463426589966\n",
      "G loss: 14.394792556762695\n",
      "Iteration 201\n",
      "D loss: -45.28691864013672\n",
      "GP: 7.787690162658691\n",
      "Gradient norm: 1.1898324489593506\n",
      "G loss: 15.760799407958984\n",
      "Iteration 251\n",
      "D loss: -43.75178527832031\n",
      "GP: 7.17642068862915\n",
      "Gradient norm: 1.203099250793457\n",
      "G loss: 15.693611145019531\n",
      "Iteration 301\n",
      "D loss: -43.355716705322266\n",
      "GP: 6.457453727722168\n",
      "Gradient norm: 1.1900339126586914\n",
      "G loss: 15.63589096069336\n",
      "Iteration 351\n",
      "D loss: -42.20863342285156\n",
      "GP: 5.042123794555664\n",
      "Gradient norm: 1.129841685295105\n",
      "G loss: 15.959887504577637\n",
      "\n",
      "Epoch 119\n",
      "Iteration 1\n",
      "D loss: -44.26301956176758\n",
      "GP: 5.76928186416626\n",
      "Gradient norm: 1.1653096675872803\n",
      "G loss: 16.156187057495117\n",
      "Iteration 51\n",
      "D loss: -47.068809509277344\n",
      "GP: 6.399818420410156\n",
      "Gradient norm: 1.1752530336380005\n",
      "G loss: 14.711140632629395\n",
      "Iteration 101\n",
      "D loss: -42.79248046875\n",
      "GP: 7.0737810134887695\n",
      "Gradient norm: 1.20150887966156\n",
      "G loss: 15.914674758911133\n",
      "Iteration 151\n",
      "D loss: -44.38140106201172\n",
      "GP: 6.324514865875244\n",
      "Gradient norm: 1.1809529066085815\n",
      "G loss: 15.796150207519531\n",
      "Iteration 201\n",
      "D loss: -42.37626266479492\n",
      "GP: 5.617265224456787\n",
      "Gradient norm: 1.152242660522461\n",
      "G loss: 15.403741836547852\n",
      "Iteration 251\n",
      "D loss: -42.923545837402344\n",
      "GP: 8.784111976623535\n",
      "Gradient norm: 1.228735089302063\n",
      "G loss: 15.4916353225708\n",
      "Iteration 301\n",
      "D loss: -45.519691467285156\n",
      "GP: 6.817498683929443\n",
      "Gradient norm: 1.1735228300094604\n",
      "G loss: 14.16690444946289\n",
      "Iteration 351\n",
      "D loss: -41.29728698730469\n",
      "GP: 6.897085189819336\n",
      "Gradient norm: 1.189116358757019\n",
      "G loss: 14.694318771362305\n",
      "\n",
      "Epoch 120\n",
      "Iteration 1\n",
      "D loss: -42.94731140136719\n",
      "GP: 7.461442947387695\n",
      "Gradient norm: 1.1923047304153442\n",
      "G loss: 15.064096450805664\n",
      "Iteration 51\n",
      "D loss: -42.14997863769531\n",
      "GP: 5.986680030822754\n",
      "Gradient norm: 1.1564106941223145\n",
      "G loss: 15.56469440460205\n",
      "Iteration 101\n",
      "D loss: -44.16947555541992\n",
      "GP: 5.933751106262207\n",
      "Gradient norm: 1.163468837738037\n",
      "G loss: 15.258220672607422\n",
      "Iteration 151\n",
      "D loss: -41.657798767089844\n",
      "GP: 6.884993553161621\n",
      "Gradient norm: 1.1941202878952026\n",
      "G loss: 14.051228523254395\n",
      "Iteration 201\n",
      "D loss: -44.83077621459961\n",
      "GP: 6.85077428817749\n",
      "Gradient norm: 1.1961965560913086\n",
      "G loss: 15.452651977539062\n",
      "Iteration 251\n",
      "D loss: -41.83647155761719\n",
      "GP: 6.9452433586120605\n",
      "Gradient norm: 1.1894419193267822\n",
      "G loss: 14.867478370666504\n",
      "Iteration 301\n",
      "D loss: -44.95772171020508\n",
      "GP: 6.483730316162109\n",
      "Gradient norm: 1.182167410850525\n",
      "G loss: 13.875116348266602\n",
      "Iteration 351\n",
      "D loss: -44.81314468383789\n",
      "GP: 6.012642860412598\n",
      "Gradient norm: 1.1730809211730957\n",
      "G loss: 13.818922996520996\n",
      "\n",
      "Epoch 121\n",
      "Iteration 1\n",
      "D loss: -43.19685745239258\n",
      "GP: 7.024689674377441\n",
      "Gradient norm: 1.1979303359985352\n",
      "G loss: 13.669496536254883\n",
      "Iteration 51\n",
      "D loss: -45.562965393066406\n",
      "GP: 6.5835723876953125\n",
      "Gradient norm: 1.1872690916061401\n",
      "G loss: 14.734212875366211\n",
      "Iteration 101\n",
      "D loss: -42.7392692565918\n",
      "GP: 5.741976737976074\n",
      "Gradient norm: 1.1683706045150757\n",
      "G loss: 15.381643295288086\n",
      "Iteration 151\n",
      "D loss: -44.409061431884766\n",
      "GP: 5.544424533843994\n",
      "Gradient norm: 1.1667850017547607\n",
      "G loss: 14.769529342651367\n",
      "Iteration 201\n",
      "D loss: -43.902915954589844\n",
      "GP: 7.775116443634033\n",
      "Gradient norm: 1.2056816816329956\n",
      "G loss: 14.396757125854492\n",
      "Iteration 251\n",
      "D loss: -43.7889404296875\n",
      "GP: 7.640313625335693\n",
      "Gradient norm: 1.1810743808746338\n",
      "G loss: 13.975095748901367\n",
      "Iteration 301\n",
      "D loss: -43.3740348815918\n",
      "GP: 6.515200138092041\n",
      "Gradient norm: 1.1816200017929077\n",
      "G loss: 14.150921821594238\n",
      "Iteration 351\n",
      "D loss: -44.508872985839844\n",
      "GP: 6.8233795166015625\n",
      "Gradient norm: 1.1753833293914795\n",
      "G loss: 14.397688865661621\n",
      "\n",
      "Epoch 122\n",
      "Iteration 1\n",
      "D loss: -42.95729446411133\n",
      "GP: 6.935748100280762\n",
      "Gradient norm: 1.1807578802108765\n",
      "G loss: 13.749639511108398\n",
      "Iteration 51\n",
      "D loss: -44.20732116699219\n",
      "GP: 6.010869979858398\n",
      "Gradient norm: 1.1773627996444702\n",
      "G loss: 14.009950637817383\n",
      "Iteration 101\n",
      "D loss: -44.89066696166992\n",
      "GP: 5.378008842468262\n",
      "Gradient norm: 1.1501283645629883\n",
      "G loss: 15.24815845489502\n",
      "Iteration 151\n",
      "D loss: -44.42765808105469\n",
      "GP: 5.5231523513793945\n",
      "Gradient norm: 1.1660934686660767\n",
      "G loss: 13.396702766418457\n",
      "Iteration 201\n",
      "D loss: -43.522552490234375\n",
      "GP: 7.134100914001465\n",
      "Gradient norm: 1.1931644678115845\n",
      "G loss: 14.529950141906738\n",
      "Iteration 251\n",
      "D loss: -44.14373016357422\n",
      "GP: 6.459908485412598\n",
      "Gradient norm: 1.1753933429718018\n",
      "G loss: 13.726019859313965\n",
      "Iteration 301\n",
      "D loss: -43.735233306884766\n",
      "GP: 7.347687721252441\n",
      "Gradient norm: 1.1895078420639038\n",
      "G loss: 13.343507766723633\n",
      "Iteration 351\n",
      "D loss: -40.52729034423828\n",
      "GP: 5.563408851623535\n",
      "Gradient norm: 1.1545754671096802\n",
      "G loss: 14.984705924987793\n",
      "\n",
      "Epoch 123\n",
      "Iteration 1\n",
      "D loss: -44.9798583984375\n",
      "GP: 6.536904335021973\n",
      "Gradient norm: 1.1780381202697754\n",
      "G loss: 15.317530632019043\n",
      "Iteration 51\n",
      "D loss: -41.95145034790039\n",
      "GP: 7.762650489807129\n",
      "Gradient norm: 1.2007105350494385\n",
      "G loss: 14.386584281921387\n",
      "Iteration 101\n",
      "D loss: -41.23183059692383\n",
      "GP: 7.006817817687988\n",
      "Gradient norm: 1.1714167594909668\n",
      "G loss: 13.096284866333008\n",
      "Iteration 151\n",
      "D loss: -43.644187927246094\n",
      "GP: 6.906607627868652\n",
      "Gradient norm: 1.1884270906448364\n",
      "G loss: 14.345331192016602\n",
      "Iteration 201\n",
      "D loss: -43.91049575805664\n",
      "GP: 10.12380599975586\n",
      "Gradient norm: 1.2322639226913452\n",
      "G loss: 13.879609107971191\n",
      "Iteration 251\n",
      "D loss: -43.90132141113281\n",
      "GP: 6.509637832641602\n",
      "Gradient norm: 1.1761016845703125\n",
      "G loss: 14.052438735961914\n",
      "Iteration 301\n",
      "D loss: -42.884220123291016\n",
      "GP: 8.677950859069824\n",
      "Gradient norm: 1.2207618951797485\n",
      "G loss: 15.005102157592773\n",
      "Iteration 351\n",
      "D loss: -44.63559341430664\n",
      "GP: 6.2022013664245605\n",
      "Gradient norm: 1.1753180027008057\n",
      "G loss: 14.556588172912598\n",
      "\n",
      "Epoch 124\n",
      "Iteration 1\n",
      "D loss: -42.675804138183594\n",
      "GP: 8.047715187072754\n",
      "Gradient norm: 1.2031776905059814\n",
      "G loss: 13.064002990722656\n",
      "Iteration 51\n",
      "D loss: -43.5367431640625\n",
      "GP: 9.55081558227539\n",
      "Gradient norm: 1.2267345190048218\n",
      "G loss: 13.548048973083496\n",
      "Iteration 101\n",
      "D loss: -44.902339935302734\n",
      "GP: 7.420397758483887\n",
      "Gradient norm: 1.2003799676895142\n",
      "G loss: 13.208542823791504\n",
      "Iteration 151\n",
      "D loss: -42.886070251464844\n",
      "GP: 8.946788787841797\n",
      "Gradient norm: 1.220482349395752\n",
      "G loss: 14.373549461364746\n",
      "Iteration 201\n",
      "D loss: -44.09769058227539\n",
      "GP: 7.840385913848877\n",
      "Gradient norm: 1.1996350288391113\n",
      "G loss: 14.203702926635742\n",
      "Iteration 251\n",
      "D loss: -44.08824920654297\n",
      "GP: 5.62160062789917\n",
      "Gradient norm: 1.1564017534255981\n",
      "G loss: 14.253915786743164\n",
      "Iteration 301\n",
      "D loss: -43.683258056640625\n",
      "GP: 7.641956806182861\n",
      "Gradient norm: 1.1995748281478882\n",
      "G loss: 13.55594539642334\n",
      "Iteration 351\n",
      "D loss: -44.36623764038086\n",
      "GP: 7.2258148193359375\n",
      "Gradient norm: 1.1900190114974976\n",
      "G loss: 14.705733299255371\n",
      "\n",
      "Epoch 125\n",
      "Iteration 1\n",
      "D loss: -44.95591735839844\n",
      "GP: 7.402402877807617\n",
      "Gradient norm: 1.194145917892456\n",
      "G loss: 14.60427188873291\n",
      "Iteration 51\n",
      "D loss: -44.57928466796875\n",
      "GP: 8.046991348266602\n",
      "Gradient norm: 1.2206240892410278\n",
      "G loss: 13.917713165283203\n",
      "Iteration 101\n",
      "D loss: -42.280296325683594\n",
      "GP: 8.250187873840332\n",
      "Gradient norm: 1.2236367464065552\n",
      "G loss: 14.460078239440918\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 151\n",
      "D loss: -44.060184478759766\n",
      "GP: 5.751197338104248\n",
      "Gradient norm: 1.1562994718551636\n",
      "G loss: 13.906384468078613\n",
      "Iteration 201\n",
      "D loss: -44.27886962890625\n",
      "GP: 7.46833610534668\n",
      "Gradient norm: 1.2096830606460571\n",
      "G loss: 14.028465270996094\n",
      "Iteration 251\n",
      "D loss: -42.1854362487793\n",
      "GP: 6.876728057861328\n",
      "Gradient norm: 1.1835204362869263\n",
      "G loss: 13.870996475219727\n",
      "Iteration 301\n",
      "D loss: -44.162620544433594\n",
      "GP: 7.232251167297363\n",
      "Gradient norm: 1.1847130060195923\n",
      "G loss: 12.106827735900879\n",
      "Iteration 351\n",
      "D loss: -42.846893310546875\n",
      "GP: 4.702751159667969\n",
      "Gradient norm: 1.1479805707931519\n",
      "G loss: 12.252951622009277\n",
      "\n",
      "Epoch 126\n",
      "Iteration 1\n",
      "D loss: -44.084407806396484\n",
      "GP: 7.551555633544922\n",
      "Gradient norm: 1.1885199546813965\n",
      "G loss: 12.614680290222168\n",
      "Iteration 51\n",
      "D loss: -45.30681228637695\n",
      "GP: 7.499185085296631\n",
      "Gradient norm: 1.209152102470398\n",
      "G loss: 14.237648963928223\n",
      "Iteration 101\n",
      "D loss: -43.86582565307617\n",
      "GP: 8.14165210723877\n",
      "Gradient norm: 1.2052409648895264\n",
      "G loss: 14.330562591552734\n",
      "Iteration 151\n",
      "D loss: -45.371517181396484\n",
      "GP: 6.152310371398926\n",
      "Gradient norm: 1.1760181188583374\n",
      "G loss: 13.602438926696777\n",
      "Iteration 201\n",
      "D loss: -44.59835433959961\n",
      "GP: 6.551197052001953\n",
      "Gradient norm: 1.185624599456787\n",
      "G loss: 14.417301177978516\n",
      "Iteration 251\n",
      "D loss: -43.42655944824219\n",
      "GP: 6.636453151702881\n",
      "Gradient norm: 1.1810921430587769\n",
      "G loss: 13.032379150390625\n",
      "Iteration 301\n",
      "D loss: -47.391761779785156\n",
      "GP: 8.590719223022461\n",
      "Gradient norm: 1.2128558158874512\n",
      "G loss: 14.011055946350098\n",
      "Iteration 351\n",
      "D loss: -43.093040466308594\n",
      "GP: 8.247343063354492\n",
      "Gradient norm: 1.2169914245605469\n",
      "G loss: 14.577555656433105\n",
      "\n",
      "Epoch 127\n",
      "Iteration 1\n",
      "D loss: -42.49043655395508\n",
      "GP: 7.252924919128418\n",
      "Gradient norm: 1.2046971321105957\n",
      "G loss: 13.353266716003418\n",
      "Iteration 51\n",
      "D loss: -41.36003875732422\n",
      "GP: 5.998438835144043\n",
      "Gradient norm: 1.1658689975738525\n",
      "G loss: 13.133111000061035\n",
      "Iteration 101\n",
      "D loss: -43.27035140991211\n",
      "GP: 7.340950965881348\n",
      "Gradient norm: 1.2074700593948364\n",
      "G loss: 12.721341133117676\n",
      "Iteration 151\n",
      "D loss: -44.035186767578125\n",
      "GP: 6.717878341674805\n",
      "Gradient norm: 1.1855605840682983\n",
      "G loss: 13.652839660644531\n",
      "Iteration 201\n",
      "D loss: -45.038333892822266\n",
      "GP: 7.141761779785156\n",
      "Gradient norm: 1.1861311197280884\n",
      "G loss: 14.1378755569458\n",
      "Iteration 251\n",
      "D loss: -41.04757308959961\n",
      "GP: 6.825990200042725\n",
      "Gradient norm: 1.1862778663635254\n",
      "G loss: 13.952156066894531\n",
      "Iteration 301\n",
      "D loss: -45.39760208129883\n",
      "GP: 8.735568046569824\n",
      "Gradient norm: 1.221262812614441\n",
      "G loss: 13.231216430664062\n",
      "Iteration 351\n",
      "D loss: -43.46015167236328\n",
      "GP: 6.36812686920166\n",
      "Gradient norm: 1.158906102180481\n",
      "G loss: 14.369854927062988\n",
      "\n",
      "Epoch 128\n",
      "Iteration 1\n",
      "D loss: -42.53989028930664\n",
      "GP: 8.522473335266113\n",
      "Gradient norm: 1.2113077640533447\n",
      "G loss: 13.556705474853516\n",
      "Iteration 51\n",
      "D loss: -41.26288986206055\n",
      "GP: 7.695904731750488\n",
      "Gradient norm: 1.1983920335769653\n",
      "G loss: 13.990458488464355\n",
      "Iteration 101\n",
      "D loss: -41.602996826171875\n",
      "GP: 7.301080226898193\n",
      "Gradient norm: 1.1866858005523682\n",
      "G loss: 13.348803520202637\n",
      "Iteration 151\n",
      "D loss: -43.07948303222656\n",
      "GP: 6.048582077026367\n",
      "Gradient norm: 1.1674838066101074\n",
      "G loss: 13.555927276611328\n",
      "Iteration 201\n",
      "D loss: -44.89323425292969\n",
      "GP: 6.818098068237305\n",
      "Gradient norm: 1.188628077507019\n",
      "G loss: 14.083556175231934\n",
      "Iteration 251\n",
      "D loss: -43.60591506958008\n",
      "GP: 6.434643268585205\n",
      "Gradient norm: 1.1880203485488892\n",
      "G loss: 13.391487121582031\n",
      "Iteration 301\n",
      "D loss: -47.28146743774414\n",
      "GP: 7.15266752243042\n",
      "Gradient norm: 1.1763908863067627\n",
      "G loss: 14.134153366088867\n",
      "Iteration 351\n",
      "D loss: -43.047061920166016\n",
      "GP: 6.893625736236572\n",
      "Gradient norm: 1.1841768026351929\n",
      "G loss: 13.633158683776855\n",
      "\n",
      "Epoch 129\n",
      "Iteration 1\n",
      "D loss: -44.269447326660156\n",
      "GP: 6.098685264587402\n",
      "Gradient norm: 1.1690924167633057\n",
      "G loss: 14.846166610717773\n",
      "Iteration 51\n",
      "D loss: -43.052486419677734\n",
      "GP: 7.882055759429932\n",
      "Gradient norm: 1.1920021772384644\n",
      "G loss: 12.681915283203125\n",
      "Iteration 101\n",
      "D loss: -44.02056884765625\n",
      "GP: 8.642698287963867\n",
      "Gradient norm: 1.2216222286224365\n",
      "G loss: 12.84688949584961\n",
      "Iteration 151\n",
      "D loss: -42.705047607421875\n",
      "GP: 7.210751533508301\n",
      "Gradient norm: 1.188217043876648\n",
      "G loss: 12.983041763305664\n",
      "Iteration 201\n",
      "D loss: -44.087039947509766\n",
      "GP: 6.236065864562988\n",
      "Gradient norm: 1.1691383123397827\n",
      "G loss: 14.276799201965332\n",
      "Iteration 251\n",
      "D loss: -44.68876647949219\n",
      "GP: 6.762422561645508\n",
      "Gradient norm: 1.181130290031433\n",
      "G loss: 14.318914413452148\n",
      "Iteration 301\n",
      "D loss: -43.588008880615234\n",
      "GP: 7.363926887512207\n",
      "Gradient norm: 1.1949859857559204\n",
      "G loss: 13.657514572143555\n",
      "Iteration 351\n",
      "D loss: -40.88041305541992\n",
      "GP: 6.690777778625488\n",
      "Gradient norm: 1.1988110542297363\n",
      "G loss: 14.789365768432617\n",
      "\n",
      "Epoch 130\n",
      "Iteration 1\n",
      "D loss: -45.35022735595703\n",
      "GP: 6.359381675720215\n",
      "Gradient norm: 1.1616712808609009\n",
      "G loss: 12.784778594970703\n",
      "Iteration 51\n",
      "D loss: -41.456939697265625\n",
      "GP: 6.473380088806152\n",
      "Gradient norm: 1.182086706161499\n",
      "G loss: 12.989924430847168\n",
      "Iteration 101\n",
      "D loss: -42.91291046142578\n",
      "GP: 8.58338737487793\n",
      "Gradient norm: 1.2131564617156982\n",
      "G loss: 12.872740745544434\n",
      "Iteration 151\n",
      "D loss: -43.78929901123047\n",
      "GP: 6.75537109375\n",
      "Gradient norm: 1.178121566772461\n",
      "G loss: 13.496079444885254\n",
      "Iteration 201\n",
      "D loss: -41.85265350341797\n",
      "GP: 6.530600547790527\n",
      "Gradient norm: 1.185523509979248\n",
      "G loss: 13.693435668945312\n",
      "Iteration 251\n",
      "D loss: -44.77217483520508\n",
      "GP: 6.960933208465576\n",
      "Gradient norm: 1.1895828247070312\n",
      "G loss: 13.656623840332031\n",
      "Iteration 301\n",
      "D loss: -44.312095642089844\n",
      "GP: 6.735045909881592\n",
      "Gradient norm: 1.1811524629592896\n",
      "G loss: 12.845703125\n",
      "Iteration 351\n",
      "D loss: -43.0890998840332\n",
      "GP: 6.94579553604126\n",
      "Gradient norm: 1.1864240169525146\n",
      "G loss: 12.635530471801758\n",
      "\n",
      "Epoch 131\n",
      "Iteration 1\n",
      "D loss: -42.23619079589844\n",
      "GP: 8.698721885681152\n",
      "Gradient norm: 1.217170238494873\n",
      "G loss: 12.514423370361328\n",
      "Iteration 51\n",
      "D loss: -43.725563049316406\n",
      "GP: 7.564877986907959\n",
      "Gradient norm: 1.1959320306777954\n",
      "G loss: 12.956328392028809\n",
      "Iteration 101\n",
      "D loss: -42.88796615600586\n",
      "GP: 7.501399040222168\n",
      "Gradient norm: 1.1976169347763062\n",
      "G loss: 12.479848861694336\n",
      "Iteration 151\n",
      "D loss: -42.35869598388672\n",
      "GP: 8.545952796936035\n",
      "Gradient norm: 1.2097467184066772\n",
      "G loss: 12.567044258117676\n",
      "Iteration 201\n",
      "D loss: -43.18292236328125\n",
      "GP: 6.854134559631348\n",
      "Gradient norm: 1.1810624599456787\n",
      "G loss: 13.037073135375977\n",
      "Iteration 251\n",
      "D loss: -45.45809555053711\n",
      "GP: 5.886860370635986\n",
      "Gradient norm: 1.1658191680908203\n",
      "G loss: 13.14732551574707\n",
      "Iteration 301\n",
      "D loss: -45.271583557128906\n",
      "GP: 6.693510055541992\n",
      "Gradient norm: 1.1782441139221191\n",
      "G loss: 13.006439208984375\n",
      "Iteration 351\n",
      "D loss: -43.987213134765625\n",
      "GP: 7.148141384124756\n",
      "Gradient norm: 1.186998963356018\n",
      "G loss: 12.558171272277832\n",
      "\n",
      "Epoch 132\n",
      "Iteration 1\n",
      "D loss: -44.80873107910156\n",
      "GP: 5.732831954956055\n",
      "Gradient norm: 1.1675188541412354\n",
      "G loss: 12.61568832397461\n",
      "Iteration 51\n",
      "D loss: -42.364532470703125\n",
      "GP: 6.659050941467285\n",
      "Gradient norm: 1.1907728910446167\n",
      "G loss: 12.366539001464844\n",
      "Iteration 101\n",
      "D loss: -41.537498474121094\n",
      "GP: 8.206306457519531\n",
      "Gradient norm: 1.2127138376235962\n",
      "G loss: 12.917978286743164\n",
      "Iteration 151\n",
      "D loss: -43.63730239868164\n",
      "GP: 7.685863494873047\n",
      "Gradient norm: 1.209083914756775\n",
      "G loss: 12.616132736206055\n",
      "Iteration 201\n",
      "D loss: -43.88943099975586\n",
      "GP: 6.0396223068237305\n",
      "Gradient norm: 1.1870085000991821\n",
      "G loss: 11.495134353637695\n",
      "Iteration 251\n",
      "D loss: -45.58370590209961\n",
      "GP: 6.0686235427856445\n",
      "Gradient norm: 1.175032377243042\n",
      "G loss: 12.076910972595215\n",
      "Iteration 301\n",
      "D loss: -42.8227653503418\n",
      "GP: 6.993251800537109\n",
      "Gradient norm: 1.1962146759033203\n",
      "G loss: 12.230644226074219\n",
      "Iteration 351\n",
      "D loss: -43.5542106628418\n",
      "GP: 6.489277362823486\n",
      "Gradient norm: 1.1756279468536377\n",
      "G loss: 12.025323867797852\n",
      "\n",
      "Epoch 133\n",
      "Iteration 1\n",
      "D loss: -43.196929931640625\n",
      "GP: 7.03836727142334\n",
      "Gradient norm: 1.179019808769226\n",
      "G loss: 12.769244194030762\n",
      "Iteration 51\n",
      "D loss: -44.926429748535156\n",
      "GP: 5.380692005157471\n",
      "Gradient norm: 1.1481257677078247\n",
      "G loss: 13.026740074157715\n",
      "Iteration 101\n",
      "D loss: -42.8175048828125\n",
      "GP: 6.445688724517822\n",
      "Gradient norm: 1.181546688079834\n",
      "G loss: 13.13070011138916\n",
      "Iteration 151\n",
      "D loss: -44.21355438232422\n",
      "GP: 6.7435808181762695\n",
      "Gradient norm: 1.187098741531372\n",
      "G loss: 12.587970733642578\n",
      "Iteration 201\n",
      "D loss: -44.567623138427734\n",
      "GP: 6.507072448730469\n",
      "Gradient norm: 1.1711336374282837\n",
      "G loss: 11.294464111328125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 251\n",
      "D loss: -42.586429595947266\n",
      "GP: 6.824198246002197\n",
      "Gradient norm: 1.1825255155563354\n",
      "G loss: 13.105388641357422\n",
      "Iteration 301\n",
      "D loss: -42.154117584228516\n",
      "GP: 5.7118940353393555\n",
      "Gradient norm: 1.1572356224060059\n",
      "G loss: 11.698134422302246\n",
      "Iteration 351\n",
      "D loss: -43.40816879272461\n",
      "GP: 5.986575126647949\n",
      "Gradient norm: 1.164681077003479\n",
      "G loss: 11.768999099731445\n",
      "\n",
      "Epoch 134\n",
      "Iteration 1\n",
      "D loss: -40.58103561401367\n",
      "GP: 5.348635196685791\n",
      "Gradient norm: 1.1485528945922852\n",
      "G loss: 13.729216575622559\n",
      "Iteration 51\n",
      "D loss: -42.355003356933594\n",
      "GP: 6.923911094665527\n",
      "Gradient norm: 1.182230830192566\n",
      "G loss: 12.172435760498047\n",
      "Iteration 101\n",
      "D loss: -43.97642517089844\n",
      "GP: 7.550546646118164\n",
      "Gradient norm: 1.1913986206054688\n",
      "G loss: 12.005532264709473\n",
      "Iteration 151\n",
      "D loss: -44.16096115112305\n",
      "GP: 6.4320969581604\n",
      "Gradient norm: 1.162250280380249\n",
      "G loss: 12.875521659851074\n",
      "Iteration 201\n",
      "D loss: -42.640289306640625\n",
      "GP: 6.454530715942383\n",
      "Gradient norm: 1.1847108602523804\n",
      "G loss: 13.406373977661133\n",
      "Iteration 251\n",
      "D loss: -41.76081848144531\n",
      "GP: 9.0554838180542\n",
      "Gradient norm: 1.2188286781311035\n",
      "G loss: 12.70212459564209\n",
      "Iteration 301\n",
      "D loss: -46.47423553466797\n",
      "GP: 7.19097375869751\n",
      "Gradient norm: 1.1938567161560059\n",
      "G loss: 11.51302719116211\n",
      "Iteration 351\n",
      "D loss: -42.915679931640625\n",
      "GP: 5.800009727478027\n",
      "Gradient norm: 1.163365125656128\n",
      "G loss: 12.8397798538208\n",
      "\n",
      "Epoch 135\n",
      "Iteration 1\n",
      "D loss: -43.50190734863281\n",
      "GP: 6.724323272705078\n",
      "Gradient norm: 1.1823318004608154\n",
      "G loss: 12.251495361328125\n",
      "Iteration 51\n",
      "D loss: -42.302978515625\n",
      "GP: 8.298439025878906\n",
      "Gradient norm: 1.212382197380066\n",
      "G loss: 12.974761009216309\n",
      "Iteration 101\n",
      "D loss: -42.950592041015625\n",
      "GP: 6.130022048950195\n",
      "Gradient norm: 1.1635820865631104\n",
      "G loss: 13.107274055480957\n",
      "Iteration 151\n",
      "D loss: -44.86050033569336\n",
      "GP: 6.20556116104126\n",
      "Gradient norm: 1.167494535446167\n",
      "G loss: 12.418426513671875\n",
      "Iteration 201\n",
      "D loss: -44.66908264160156\n",
      "GP: 6.34309196472168\n",
      "Gradient norm: 1.1827584505081177\n",
      "G loss: 11.737940788269043\n",
      "Iteration 251\n",
      "D loss: -43.562660217285156\n",
      "GP: 5.225671768188477\n",
      "Gradient norm: 1.152191400527954\n",
      "G loss: 12.705792427062988\n",
      "Iteration 301\n",
      "D loss: -42.46622848510742\n",
      "GP: 7.738781452178955\n",
      "Gradient norm: 1.2020808458328247\n",
      "G loss: 12.231443405151367\n",
      "Iteration 351\n",
      "D loss: -44.16676330566406\n",
      "GP: 5.398862361907959\n",
      "Gradient norm: 1.1598271131515503\n",
      "G loss: 12.975099563598633\n",
      "\n",
      "Epoch 136\n",
      "Iteration 1\n",
      "D loss: -42.20261764526367\n",
      "GP: 8.02361011505127\n",
      "Gradient norm: 1.1885859966278076\n",
      "G loss: 13.368544578552246\n",
      "Iteration 51\n",
      "D loss: -42.92893600463867\n",
      "GP: 6.917782783508301\n",
      "Gradient norm: 1.1936970949172974\n",
      "G loss: 11.691469192504883\n",
      "Iteration 101\n",
      "D loss: -44.858863830566406\n",
      "GP: 6.275530815124512\n",
      "Gradient norm: 1.1728780269622803\n",
      "G loss: 12.765036582946777\n",
      "Iteration 151\n",
      "D loss: -45.737247467041016\n",
      "GP: 6.472451210021973\n",
      "Gradient norm: 1.178821325302124\n",
      "G loss: 11.824936866760254\n",
      "Iteration 201\n",
      "D loss: -45.0667610168457\n",
      "GP: 6.811707496643066\n",
      "Gradient norm: 1.1782633066177368\n",
      "G loss: 11.684076309204102\n",
      "Iteration 251\n",
      "D loss: -43.39976501464844\n",
      "GP: 5.246811389923096\n",
      "Gradient norm: 1.1544630527496338\n",
      "G loss: 12.94907283782959\n",
      "Iteration 301\n",
      "D loss: -41.757694244384766\n",
      "GP: 6.687542915344238\n",
      "Gradient norm: 1.180474877357483\n",
      "G loss: 13.791744232177734\n",
      "Iteration 351\n",
      "D loss: -43.68840408325195\n",
      "GP: 9.30501937866211\n",
      "Gradient norm: 1.2252187728881836\n",
      "G loss: 12.870841979980469\n",
      "\n",
      "Epoch 137\n",
      "Iteration 1\n",
      "D loss: -43.219303131103516\n",
      "GP: 7.194111347198486\n",
      "Gradient norm: 1.1824538707733154\n",
      "G loss: 13.061826705932617\n",
      "Iteration 51\n",
      "D loss: -41.176246643066406\n",
      "GP: 8.222762107849121\n",
      "Gradient norm: 1.2146416902542114\n",
      "G loss: 12.60247802734375\n",
      "Iteration 101\n",
      "D loss: -45.27029037475586\n",
      "GP: 6.921565055847168\n",
      "Gradient norm: 1.1874985694885254\n",
      "G loss: 12.779130935668945\n",
      "Iteration 151\n",
      "D loss: -44.243255615234375\n",
      "GP: 6.118423938751221\n",
      "Gradient norm: 1.1693345308303833\n",
      "G loss: 13.323280334472656\n",
      "Iteration 201\n",
      "D loss: -42.9068603515625\n",
      "GP: 6.600717544555664\n",
      "Gradient norm: 1.183173656463623\n",
      "G loss: 13.639320373535156\n",
      "Iteration 251\n",
      "D loss: -43.10887908935547\n",
      "GP: 7.879866600036621\n",
      "Gradient norm: 1.2050166130065918\n",
      "G loss: 11.921762466430664\n",
      "Iteration 301\n",
      "D loss: -44.05335998535156\n",
      "GP: 6.614363193511963\n",
      "Gradient norm: 1.1788830757141113\n",
      "G loss: 11.874394416809082\n",
      "Iteration 351\n",
      "D loss: -41.55216598510742\n",
      "GP: 6.300581932067871\n",
      "Gradient norm: 1.174123764038086\n",
      "G loss: 12.049551963806152\n",
      "\n",
      "Epoch 138\n",
      "Iteration 1\n",
      "D loss: -42.38032531738281\n",
      "GP: 6.725319862365723\n",
      "Gradient norm: 1.198305606842041\n",
      "G loss: 12.465384483337402\n",
      "Iteration 51\n",
      "D loss: -42.54235076904297\n",
      "GP: 7.902770042419434\n",
      "Gradient norm: 1.1923381090164185\n",
      "G loss: 13.316580772399902\n",
      "Iteration 101\n",
      "D loss: -43.16557312011719\n",
      "GP: 6.7568230628967285\n",
      "Gradient norm: 1.1823644638061523\n",
      "G loss: 12.972372055053711\n",
      "Iteration 151\n",
      "D loss: -41.2117919921875\n",
      "GP: 8.343756675720215\n",
      "Gradient norm: 1.2119762897491455\n",
      "G loss: 12.468387603759766\n",
      "Iteration 201\n",
      "D loss: -43.055171966552734\n",
      "GP: 7.303938865661621\n",
      "Gradient norm: 1.1831756830215454\n",
      "G loss: 12.571551322937012\n",
      "Iteration 251\n",
      "D loss: -44.21748352050781\n",
      "GP: 5.829324722290039\n",
      "Gradient norm: 1.173166275024414\n",
      "G loss: 13.969198226928711\n",
      "Iteration 301\n",
      "D loss: -43.13298034667969\n",
      "GP: 6.471078872680664\n",
      "Gradient norm: 1.17054283618927\n",
      "G loss: 13.556757926940918\n",
      "Iteration 351\n",
      "D loss: -44.57780456542969\n",
      "GP: 6.422894477844238\n",
      "Gradient norm: 1.1806213855743408\n",
      "G loss: 13.026962280273438\n",
      "\n",
      "Epoch 139\n",
      "Iteration 1\n",
      "D loss: -44.71685791015625\n",
      "GP: 7.95929479598999\n",
      "Gradient norm: 1.1989277601242065\n",
      "G loss: 13.34033203125\n",
      "Iteration 51\n",
      "D loss: -44.33241653442383\n",
      "GP: 5.741893768310547\n",
      "Gradient norm: 1.1539174318313599\n",
      "G loss: 13.6819486618042\n",
      "Iteration 101\n",
      "D loss: -43.689796447753906\n",
      "GP: 6.901411056518555\n",
      "Gradient norm: 1.1798384189605713\n",
      "G loss: 13.287281036376953\n",
      "Iteration 151\n",
      "D loss: -44.646209716796875\n",
      "GP: 7.934162616729736\n",
      "Gradient norm: 1.2159757614135742\n",
      "G loss: 13.119922637939453\n",
      "Iteration 201\n",
      "D loss: -44.19645690917969\n",
      "GP: 7.349862098693848\n",
      "Gradient norm: 1.1939314603805542\n",
      "G loss: 12.010990142822266\n",
      "Iteration 251\n",
      "D loss: -43.85734939575195\n",
      "GP: 5.6603474617004395\n",
      "Gradient norm: 1.1586018800735474\n",
      "G loss: 13.167451858520508\n",
      "Iteration 301\n",
      "D loss: -41.44337463378906\n",
      "GP: 7.261434555053711\n",
      "Gradient norm: 1.1935399770736694\n",
      "G loss: 13.006598472595215\n",
      "Iteration 351\n",
      "D loss: -43.75460433959961\n",
      "GP: 6.408629417419434\n",
      "Gradient norm: 1.174775242805481\n",
      "G loss: 12.473586082458496\n",
      "\n",
      "Epoch 140\n",
      "Iteration 1\n",
      "D loss: -42.71288299560547\n",
      "GP: 9.270357131958008\n",
      "Gradient norm: 1.217819333076477\n",
      "G loss: 12.700236320495605\n",
      "Iteration 51\n",
      "D loss: -45.99677658081055\n",
      "GP: 7.024994373321533\n",
      "Gradient norm: 1.1942095756530762\n",
      "G loss: 12.656108856201172\n",
      "Iteration 101\n",
      "D loss: -45.07288360595703\n",
      "GP: 6.718349456787109\n",
      "Gradient norm: 1.1957803964614868\n",
      "G loss: 12.837751388549805\n",
      "Iteration 151\n",
      "D loss: -41.61128234863281\n",
      "GP: 7.189445972442627\n",
      "Gradient norm: 1.1890851259231567\n",
      "G loss: 12.778892517089844\n",
      "Iteration 201\n",
      "D loss: -45.68677520751953\n",
      "GP: 8.336516380310059\n",
      "Gradient norm: 1.1934244632720947\n",
      "G loss: 12.182692527770996\n",
      "Iteration 251\n",
      "D loss: -43.617820739746094\n",
      "GP: 6.998371601104736\n",
      "Gradient norm: 1.1917449235916138\n",
      "G loss: 13.249950408935547\n",
      "Iteration 301\n",
      "D loss: -43.26710510253906\n",
      "GP: 7.2168498039245605\n",
      "Gradient norm: 1.1884191036224365\n",
      "G loss: 13.274081230163574\n",
      "Iteration 351\n",
      "D loss: -44.268714904785156\n",
      "GP: 7.2383551597595215\n",
      "Gradient norm: 1.1883327960968018\n",
      "G loss: 13.79218864440918\n",
      "\n",
      "Epoch 141\n",
      "Iteration 1\n",
      "D loss: -44.39181900024414\n",
      "GP: 6.719605922698975\n",
      "Gradient norm: 1.1825124025344849\n",
      "G loss: 14.229626655578613\n",
      "Iteration 51\n",
      "D loss: -43.16575622558594\n",
      "GP: 7.168846607208252\n",
      "Gradient norm: 1.1903388500213623\n",
      "G loss: 14.237902641296387\n",
      "Iteration 101\n",
      "D loss: -45.284088134765625\n",
      "GP: 6.200271129608154\n",
      "Gradient norm: 1.177618384361267\n",
      "G loss: 12.945063591003418\n",
      "Iteration 151\n",
      "D loss: -43.73497772216797\n",
      "GP: 7.5986833572387695\n",
      "Gradient norm: 1.214458703994751\n",
      "G loss: 12.505345344543457\n",
      "Iteration 201\n",
      "D loss: -44.1154899597168\n",
      "GP: 6.852120876312256\n",
      "Gradient norm: 1.1838940382003784\n",
      "G loss: 11.322996139526367\n",
      "Iteration 251\n",
      "D loss: -43.08845520019531\n",
      "GP: 7.652589797973633\n",
      "Gradient norm: 1.1865322589874268\n",
      "G loss: 12.641243934631348\n",
      "Iteration 301\n",
      "D loss: -42.755130767822266\n",
      "GP: 7.084748268127441\n",
      "Gradient norm: 1.1837165355682373\n",
      "G loss: 14.404937744140625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 351\n",
      "D loss: -43.896141052246094\n",
      "GP: 6.768002510070801\n",
      "Gradient norm: 1.1835358142852783\n",
      "G loss: 13.512419700622559\n",
      "\n",
      "Epoch 142\n",
      "Iteration 1\n",
      "D loss: -44.2362174987793\n",
      "GP: 8.494396209716797\n",
      "Gradient norm: 1.2079439163208008\n",
      "G loss: 13.066059112548828\n",
      "Iteration 51\n",
      "D loss: -44.19919204711914\n",
      "GP: 6.889443874359131\n",
      "Gradient norm: 1.179559588432312\n",
      "G loss: 13.378074645996094\n",
      "Iteration 101\n",
      "D loss: -43.676063537597656\n",
      "GP: 7.751102447509766\n",
      "Gradient norm: 1.1998038291931152\n",
      "G loss: 12.801794052124023\n",
      "Iteration 151\n",
      "D loss: -43.301631927490234\n",
      "GP: 5.712279796600342\n",
      "Gradient norm: 1.1631063222885132\n",
      "G loss: 12.945999145507812\n",
      "Iteration 201\n",
      "D loss: -42.18597412109375\n",
      "GP: 5.557661056518555\n",
      "Gradient norm: 1.1652772426605225\n",
      "G loss: 13.356422424316406\n",
      "Iteration 251\n",
      "D loss: -40.792057037353516\n",
      "GP: 6.481245994567871\n",
      "Gradient norm: 1.1752511262893677\n",
      "G loss: 12.766314506530762\n",
      "Iteration 301\n",
      "D loss: -42.091796875\n",
      "GP: 5.338927268981934\n",
      "Gradient norm: 1.154735803604126\n",
      "G loss: 13.741214752197266\n",
      "Iteration 351\n",
      "D loss: -41.36243438720703\n",
      "GP: 8.744504928588867\n",
      "Gradient norm: 1.237860918045044\n",
      "G loss: 13.239473342895508\n",
      "\n",
      "Epoch 143\n",
      "Iteration 1\n",
      "D loss: -46.02583694458008\n",
      "GP: 5.807406902313232\n",
      "Gradient norm: 1.1712089776992798\n",
      "G loss: 13.823575973510742\n",
      "Iteration 51\n",
      "D loss: -44.803279876708984\n",
      "GP: 5.582749366760254\n",
      "Gradient norm: 1.157038927078247\n",
      "G loss: 13.049943923950195\n",
      "Iteration 101\n",
      "D loss: -43.78208541870117\n",
      "GP: 5.386844635009766\n",
      "Gradient norm: 1.1650553941726685\n",
      "G loss: 13.704566955566406\n",
      "Iteration 151\n",
      "D loss: -41.38525390625\n",
      "GP: 7.609453201293945\n",
      "Gradient norm: 1.195525884628296\n",
      "G loss: 13.655984878540039\n",
      "Iteration 201\n",
      "D loss: -43.096885681152344\n",
      "GP: 7.50679874420166\n",
      "Gradient norm: 1.198466420173645\n",
      "G loss: 13.463766098022461\n",
      "Iteration 251\n",
      "D loss: -40.272212982177734\n",
      "GP: 7.449012756347656\n",
      "Gradient norm: 1.196193814277649\n",
      "G loss: 12.195891380310059\n",
      "Iteration 301\n",
      "D loss: -42.1352653503418\n",
      "GP: 7.707247734069824\n",
      "Gradient norm: 1.1980302333831787\n",
      "G loss: 11.592713356018066\n",
      "Iteration 351\n",
      "D loss: -43.90163803100586\n",
      "GP: 5.9011640548706055\n",
      "Gradient norm: 1.1650809049606323\n",
      "G loss: 11.998760223388672\n",
      "\n",
      "Epoch 144\n",
      "Iteration 1\n",
      "D loss: -44.581974029541016\n",
      "GP: 6.037283897399902\n",
      "Gradient norm: 1.194342017173767\n",
      "G loss: 13.141806602478027\n",
      "Iteration 51\n",
      "D loss: -42.934654235839844\n",
      "GP: 5.8611016273498535\n",
      "Gradient norm: 1.1637861728668213\n",
      "G loss: 13.173030853271484\n",
      "Iteration 101\n",
      "D loss: -41.47758102416992\n",
      "GP: 6.568112373352051\n",
      "Gradient norm: 1.1561554670333862\n",
      "G loss: 13.199195861816406\n",
      "Iteration 151\n",
      "D loss: -41.591365814208984\n",
      "GP: 6.538279056549072\n",
      "Gradient norm: 1.199724793434143\n",
      "G loss: 13.502655982971191\n",
      "Iteration 201\n",
      "D loss: -44.270633697509766\n",
      "GP: 6.511208534240723\n",
      "Gradient norm: 1.1707199811935425\n",
      "G loss: 13.629591941833496\n",
      "Iteration 251\n",
      "D loss: -44.637081146240234\n",
      "GP: 6.7528557777404785\n",
      "Gradient norm: 1.190764307975769\n",
      "G loss: 12.066139221191406\n",
      "Iteration 301\n",
      "D loss: -42.930274963378906\n",
      "GP: 6.087229251861572\n",
      "Gradient norm: 1.1675916910171509\n",
      "G loss: 12.658804893493652\n",
      "Iteration 351\n",
      "D loss: -44.041412353515625\n",
      "GP: 7.640330791473389\n",
      "Gradient norm: 1.1957862377166748\n",
      "G loss: 13.102316856384277\n",
      "\n",
      "Epoch 145\n",
      "Iteration 1\n",
      "D loss: -44.36165237426758\n",
      "GP: 7.3804612159729\n",
      "Gradient norm: 1.1980148553848267\n",
      "G loss: 13.582653999328613\n",
      "Iteration 51\n",
      "D loss: -45.23301315307617\n",
      "GP: 6.331830024719238\n",
      "Gradient norm: 1.1722978353500366\n",
      "G loss: 12.251115798950195\n",
      "Iteration 101\n",
      "D loss: -41.21516799926758\n",
      "GP: 5.5690412521362305\n",
      "Gradient norm: 1.149600625038147\n",
      "G loss: 12.623961448669434\n",
      "Iteration 151\n",
      "D loss: -42.27341842651367\n",
      "GP: 6.760231018066406\n",
      "Gradient norm: 1.1912047863006592\n",
      "G loss: 12.356844902038574\n",
      "Iteration 201\n",
      "D loss: -43.222434997558594\n",
      "GP: 7.12808895111084\n",
      "Gradient norm: 1.1756970882415771\n",
      "G loss: 13.369471549987793\n",
      "Iteration 251\n",
      "D loss: -45.436824798583984\n",
      "GP: 7.183618068695068\n",
      "Gradient norm: 1.198240876197815\n",
      "G loss: 13.667659759521484\n",
      "Iteration 301\n",
      "D loss: -43.787620544433594\n",
      "GP: 6.099293231964111\n",
      "Gradient norm: 1.172245979309082\n",
      "G loss: 11.881057739257812\n",
      "Iteration 351\n",
      "D loss: -42.760520935058594\n",
      "GP: 8.048306465148926\n",
      "Gradient norm: 1.1942826509475708\n",
      "G loss: 12.572259902954102\n",
      "\n",
      "Epoch 146\n",
      "Iteration 1\n",
      "D loss: -42.66193389892578\n",
      "GP: 6.638960838317871\n",
      "Gradient norm: 1.182072639465332\n",
      "G loss: 12.689191818237305\n",
      "Iteration 51\n",
      "D loss: -46.269039154052734\n",
      "GP: 6.780251502990723\n",
      "Gradient norm: 1.2002960443496704\n",
      "G loss: 12.489728927612305\n",
      "Iteration 101\n",
      "D loss: -41.48900604248047\n",
      "GP: 6.639265060424805\n",
      "Gradient norm: 1.1610411405563354\n",
      "G loss: 13.170454978942871\n",
      "Iteration 151\n",
      "D loss: -45.876949310302734\n",
      "GP: 7.837318420410156\n",
      "Gradient norm: 1.1970036029815674\n",
      "G loss: 11.987959861755371\n",
      "Iteration 201\n",
      "D loss: -44.79413604736328\n",
      "GP: 7.5676984786987305\n",
      "Gradient norm: 1.190581202507019\n",
      "G loss: 12.105030059814453\n",
      "Iteration 251\n",
      "D loss: -43.75429916381836\n",
      "GP: 7.8426513671875\n",
      "Gradient norm: 1.1993941068649292\n",
      "G loss: 10.805872917175293\n",
      "Iteration 301\n",
      "D loss: -45.436248779296875\n",
      "GP: 7.379854202270508\n",
      "Gradient norm: 1.1765668392181396\n",
      "G loss: 10.649065971374512\n",
      "Iteration 351\n",
      "D loss: -43.828731536865234\n",
      "GP: 7.673227787017822\n",
      "Gradient norm: 1.192325234413147\n",
      "G loss: 12.311325073242188\n",
      "\n",
      "Epoch 147\n",
      "Iteration 1\n",
      "D loss: -43.48802947998047\n",
      "GP: 7.454916954040527\n",
      "Gradient norm: 1.203061580657959\n",
      "G loss: 11.530415534973145\n",
      "Iteration 51\n",
      "D loss: -44.15821838378906\n",
      "GP: 5.55286169052124\n",
      "Gradient norm: 1.1641618013381958\n",
      "G loss: 11.893627166748047\n",
      "Iteration 101\n",
      "D loss: -40.22397994995117\n",
      "GP: 5.0109968185424805\n",
      "Gradient norm: 1.1439872980117798\n",
      "G loss: 11.290643692016602\n",
      "Iteration 151\n",
      "D loss: -44.51788330078125\n",
      "GP: 6.861496925354004\n",
      "Gradient norm: 1.1917192935943604\n",
      "G loss: 11.607839584350586\n",
      "Iteration 201\n",
      "D loss: -41.128089904785156\n",
      "GP: 7.374603748321533\n",
      "Gradient norm: 1.1965487003326416\n",
      "G loss: 11.892863273620605\n",
      "Iteration 251\n",
      "D loss: -43.65008544921875\n",
      "GP: 6.307201862335205\n",
      "Gradient norm: 1.1594043970108032\n",
      "G loss: 11.619340896606445\n",
      "Iteration 301\n",
      "D loss: -44.303863525390625\n",
      "GP: 6.337499618530273\n",
      "Gradient norm: 1.1858118772506714\n",
      "G loss: 12.13770866394043\n",
      "Iteration 351\n",
      "D loss: -43.424156188964844\n",
      "GP: 6.5612030029296875\n",
      "Gradient norm: 1.1492667198181152\n",
      "G loss: 11.469733238220215\n",
      "\n",
      "Epoch 148\n",
      "Iteration 1\n",
      "D loss: -42.681007385253906\n",
      "GP: 6.224111557006836\n",
      "Gradient norm: 1.1857191324234009\n",
      "G loss: 12.150787353515625\n",
      "Iteration 51\n",
      "D loss: -43.46323013305664\n",
      "GP: 8.941609382629395\n",
      "Gradient norm: 1.207758903503418\n",
      "G loss: 12.111093521118164\n",
      "Iteration 101\n",
      "D loss: -42.386627197265625\n",
      "GP: 7.318666458129883\n",
      "Gradient norm: 1.189562439918518\n",
      "G loss: 11.035107612609863\n",
      "Iteration 151\n",
      "D loss: -45.112403869628906\n",
      "GP: 8.396549224853516\n",
      "Gradient norm: 1.2127163410186768\n",
      "G loss: 11.570096969604492\n",
      "Iteration 201\n",
      "D loss: -42.8984489440918\n",
      "GP: 6.300158500671387\n",
      "Gradient norm: 1.1716078519821167\n",
      "G loss: 10.338294982910156\n",
      "Iteration 251\n",
      "D loss: -41.901161193847656\n",
      "GP: 5.416274547576904\n",
      "Gradient norm: 1.1419353485107422\n",
      "G loss: 12.697966575622559\n",
      "Iteration 301\n",
      "D loss: -43.561317443847656\n",
      "GP: 8.084183692932129\n",
      "Gradient norm: 1.211901307106018\n",
      "G loss: 11.83344554901123\n",
      "Iteration 351\n",
      "D loss: -43.53107452392578\n",
      "GP: 7.34032678604126\n",
      "Gradient norm: 1.1875579357147217\n",
      "G loss: 11.460118293762207\n",
      "\n",
      "Epoch 149\n",
      "Iteration 1\n",
      "D loss: -41.6793212890625\n",
      "GP: 8.48957633972168\n",
      "Gradient norm: 1.208365559577942\n",
      "G loss: 11.025769233703613\n",
      "Iteration 51\n",
      "D loss: -43.64138412475586\n",
      "GP: 7.631070613861084\n",
      "Gradient norm: 1.203202486038208\n",
      "G loss: 11.230632781982422\n",
      "Iteration 101\n",
      "D loss: -43.232330322265625\n",
      "GP: 8.82473087310791\n",
      "Gradient norm: 1.2201974391937256\n",
      "G loss: 10.837930679321289\n",
      "Iteration 151\n",
      "D loss: -43.99200439453125\n",
      "GP: 6.574049472808838\n",
      "Gradient norm: 1.1771320104599\n",
      "G loss: 12.124300003051758\n",
      "Iteration 201\n",
      "D loss: -43.313377380371094\n",
      "GP: 7.55161190032959\n",
      "Gradient norm: 1.192448377609253\n",
      "G loss: 12.147958755493164\n",
      "Iteration 251\n",
      "D loss: -43.58136749267578\n",
      "GP: 8.047533988952637\n",
      "Gradient norm: 1.1979806423187256\n",
      "G loss: 12.088111877441406\n",
      "Iteration 301\n",
      "D loss: -44.87824249267578\n",
      "GP: 6.655656814575195\n",
      "Gradient norm: 1.1786366701126099\n",
      "G loss: 11.960624694824219\n",
      "Iteration 351\n",
      "D loss: -43.943870544433594\n",
      "GP: 7.534135818481445\n",
      "Gradient norm: 1.1900759935379028\n",
      "G loss: 10.634483337402344\n",
      "\n",
      "Epoch 150\n",
      "Iteration 1\n",
      "D loss: -41.53221130371094\n",
      "GP: 7.699274063110352\n",
      "Gradient norm: 1.1836192607879639\n",
      "G loss: 12.412755966186523\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 51\n",
      "D loss: -43.726871490478516\n",
      "GP: 7.570952892303467\n",
      "Gradient norm: 1.2010254859924316\n",
      "G loss: 10.971199989318848\n",
      "Iteration 101\n",
      "D loss: -43.88225555419922\n",
      "GP: 8.373825073242188\n",
      "Gradient norm: 1.1911568641662598\n",
      "G loss: 12.142539024353027\n",
      "Iteration 151\n",
      "D loss: -42.71653366088867\n",
      "GP: 6.54921293258667\n",
      "Gradient norm: 1.1758754253387451\n",
      "G loss: 11.09337043762207\n",
      "Iteration 201\n",
      "D loss: -43.066402435302734\n",
      "GP: 5.936441898345947\n",
      "Gradient norm: 1.1727434396743774\n",
      "G loss: 11.523509979248047\n",
      "Iteration 251\n",
      "D loss: -43.510135650634766\n",
      "GP: 7.254218578338623\n",
      "Gradient norm: 1.2111883163452148\n",
      "G loss: 12.209293365478516\n",
      "Iteration 301\n",
      "D loss: -44.827239990234375\n",
      "GP: 5.485247611999512\n",
      "Gradient norm: 1.1628113985061646\n",
      "G loss: 11.038372039794922\n",
      "Iteration 351\n",
      "D loss: -42.290122985839844\n",
      "GP: 7.064105033874512\n",
      "Gradient norm: 1.1892415285110474\n",
      "G loss: 11.627623558044434\n",
      "\n",
      "Epoch 151\n",
      "Iteration 1\n",
      "D loss: -43.069732666015625\n",
      "GP: 5.470117568969727\n",
      "Gradient norm: 1.150686264038086\n",
      "G loss: 12.823827743530273\n",
      "Iteration 51\n",
      "D loss: -44.322872161865234\n",
      "GP: 7.089832305908203\n",
      "Gradient norm: 1.191237449645996\n",
      "G loss: 11.10169506072998\n",
      "Iteration 101\n",
      "D loss: -43.65910339355469\n",
      "GP: 5.818208694458008\n",
      "Gradient norm: 1.159596562385559\n",
      "G loss: 11.212726593017578\n",
      "Iteration 151\n",
      "D loss: -43.572715759277344\n",
      "GP: 6.561180114746094\n",
      "Gradient norm: 1.1732261180877686\n",
      "G loss: 10.960869789123535\n",
      "Iteration 201\n",
      "D loss: -42.90772247314453\n",
      "GP: 5.9522624015808105\n",
      "Gradient norm: 1.1652497053146362\n",
      "G loss: 10.917362213134766\n",
      "Iteration 251\n",
      "D loss: -44.869022369384766\n",
      "GP: 8.10684871673584\n",
      "Gradient norm: 1.213029146194458\n",
      "G loss: 11.851069450378418\n",
      "Iteration 301\n",
      "D loss: -43.189144134521484\n",
      "GP: 9.040326118469238\n",
      "Gradient norm: 1.2218719720840454\n",
      "G loss: 10.646092414855957\n",
      "Iteration 351\n",
      "D loss: -42.03232955932617\n",
      "GP: 7.5191330909729\n",
      "Gradient norm: 1.19917631149292\n",
      "G loss: 12.098291397094727\n",
      "\n",
      "Epoch 152\n",
      "Iteration 1\n",
      "D loss: -44.288818359375\n",
      "GP: 6.355222702026367\n",
      "Gradient norm: 1.1757208108901978\n",
      "G loss: 11.797637939453125\n",
      "Iteration 51\n",
      "D loss: -43.402076721191406\n",
      "GP: 6.693553924560547\n",
      "Gradient norm: 1.179938793182373\n",
      "G loss: 11.807234764099121\n",
      "Iteration 101\n",
      "D loss: -42.16419982910156\n",
      "GP: 5.912004470825195\n",
      "Gradient norm: 1.1688531637191772\n",
      "G loss: 12.679097175598145\n",
      "Iteration 151\n",
      "D loss: -43.116310119628906\n",
      "GP: 6.665048122406006\n",
      "Gradient norm: 1.1938140392303467\n",
      "G loss: 13.34475040435791\n",
      "Iteration 201\n",
      "D loss: -42.744049072265625\n",
      "GP: 8.002965927124023\n",
      "Gradient norm: 1.204628825187683\n",
      "G loss: 12.712541580200195\n",
      "Iteration 251\n",
      "D loss: -42.49110794067383\n",
      "GP: 6.4755353927612305\n",
      "Gradient norm: 1.1762113571166992\n",
      "G loss: 9.197002410888672\n",
      "Iteration 301\n",
      "D loss: -44.144290924072266\n",
      "GP: 6.327769756317139\n",
      "Gradient norm: 1.1768226623535156\n",
      "G loss: 11.076973915100098\n",
      "Iteration 351\n",
      "D loss: -43.99129867553711\n",
      "GP: 8.44874095916748\n",
      "Gradient norm: 1.2213128805160522\n",
      "G loss: 11.924726486206055\n",
      "\n",
      "Epoch 153\n",
      "Iteration 1\n",
      "D loss: -43.292388916015625\n",
      "GP: 6.625608444213867\n",
      "Gradient norm: 1.1719528436660767\n",
      "G loss: 11.350615501403809\n",
      "Iteration 51\n",
      "D loss: -43.89556884765625\n",
      "GP: 6.183626651763916\n",
      "Gradient norm: 1.1659414768218994\n",
      "G loss: 11.320074081420898\n",
      "Iteration 101\n",
      "D loss: -44.68046569824219\n",
      "GP: 5.048957347869873\n",
      "Gradient norm: 1.1459455490112305\n",
      "G loss: 11.628767013549805\n",
      "Iteration 151\n",
      "D loss: -43.1429443359375\n",
      "GP: 7.4202799797058105\n",
      "Gradient norm: 1.2014577388763428\n",
      "G loss: 11.669856071472168\n",
      "Iteration 201\n",
      "D loss: -43.84284973144531\n",
      "GP: 7.247292995452881\n",
      "Gradient norm: 1.1920746564865112\n",
      "G loss: 10.998519897460938\n",
      "Iteration 251\n",
      "D loss: -45.12513732910156\n",
      "GP: 7.125395774841309\n",
      "Gradient norm: 1.1903655529022217\n",
      "G loss: 11.52013111114502\n",
      "Iteration 301\n",
      "D loss: -45.869712829589844\n",
      "GP: 6.814521789550781\n",
      "Gradient norm: 1.1740225553512573\n",
      "G loss: 11.56252670288086\n",
      "Iteration 351\n",
      "D loss: -42.51397705078125\n",
      "GP: 8.6753568649292\n",
      "Gradient norm: 1.2104235887527466\n",
      "G loss: 12.214346885681152\n",
      "\n",
      "Epoch 154\n",
      "Iteration 1\n",
      "D loss: -43.77743148803711\n",
      "GP: 7.154641151428223\n",
      "Gradient norm: 1.182944893836975\n",
      "G loss: 11.035595893859863\n",
      "Iteration 51\n",
      "D loss: -42.30367660522461\n",
      "GP: 7.73726224899292\n",
      "Gradient norm: 1.19489324092865\n",
      "G loss: 10.817221641540527\n",
      "Iteration 101\n",
      "D loss: -43.370849609375\n",
      "GP: 7.174821853637695\n",
      "Gradient norm: 1.1969646215438843\n",
      "G loss: 10.106781959533691\n",
      "Iteration 151\n",
      "D loss: -42.566158294677734\n",
      "GP: 8.343449592590332\n",
      "Gradient norm: 1.2065590620040894\n",
      "G loss: 11.074624061584473\n",
      "Iteration 201\n",
      "D loss: -43.2932014465332\n",
      "GP: 7.583171844482422\n",
      "Gradient norm: 1.1804018020629883\n",
      "G loss: 10.582282066345215\n",
      "Iteration 251\n",
      "D loss: -43.34994125366211\n",
      "GP: 6.373012065887451\n",
      "Gradient norm: 1.17120361328125\n",
      "G loss: 11.445825576782227\n",
      "Iteration 301\n",
      "D loss: -41.70470428466797\n",
      "GP: 6.859553337097168\n",
      "Gradient norm: 1.1736310720443726\n",
      "G loss: 12.01053237915039\n",
      "Iteration 351\n",
      "D loss: -42.734771728515625\n",
      "GP: 6.514618396759033\n",
      "Gradient norm: 1.1778184175491333\n",
      "G loss: 12.16904354095459\n",
      "\n",
      "Epoch 155\n",
      "Iteration 1\n",
      "D loss: -44.211753845214844\n",
      "GP: 7.220750331878662\n",
      "Gradient norm: 1.1877228021621704\n",
      "G loss: 11.04839038848877\n",
      "Iteration 51\n",
      "D loss: -45.5673942565918\n",
      "GP: 5.822848320007324\n",
      "Gradient norm: 1.1690750122070312\n",
      "G loss: 10.589617729187012\n",
      "Iteration 101\n",
      "D loss: -44.112823486328125\n",
      "GP: 6.643124580383301\n",
      "Gradient norm: 1.178457498550415\n",
      "G loss: 10.681614875793457\n",
      "Iteration 151\n",
      "D loss: -44.23480224609375\n",
      "GP: 7.158227920532227\n",
      "Gradient norm: 1.183950424194336\n",
      "G loss: 10.905953407287598\n",
      "Iteration 201\n",
      "D loss: -44.322532653808594\n",
      "GP: 4.703354835510254\n",
      "Gradient norm: 1.1330318450927734\n",
      "G loss: 12.551651000976562\n",
      "Iteration 251\n",
      "D loss: -45.20582580566406\n",
      "GP: 8.399904251098633\n",
      "Gradient norm: 1.2088404893875122\n",
      "G loss: 11.132081031799316\n",
      "Iteration 301\n",
      "D loss: -42.30732345581055\n",
      "GP: 8.098727226257324\n",
      "Gradient norm: 1.18971848487854\n",
      "G loss: 10.924015045166016\n",
      "Iteration 351\n",
      "D loss: -42.108070373535156\n",
      "GP: 10.080362319946289\n",
      "Gradient norm: 1.229103922843933\n",
      "G loss: 11.159658432006836\n",
      "\n",
      "Epoch 156\n",
      "Iteration 1\n",
      "D loss: -43.037269592285156\n",
      "GP: 7.128915786743164\n",
      "Gradient norm: 1.180029273033142\n",
      "G loss: 10.353697776794434\n",
      "Iteration 51\n",
      "D loss: -43.69893264770508\n",
      "GP: 7.224047660827637\n",
      "Gradient norm: 1.1930906772613525\n",
      "G loss: 11.511279106140137\n",
      "Iteration 101\n",
      "D loss: -43.52414321899414\n",
      "GP: 7.1139092445373535\n",
      "Gradient norm: 1.1855965852737427\n",
      "G loss: 10.985102653503418\n",
      "Iteration 151\n",
      "D loss: -44.776222229003906\n",
      "GP: 6.90995979309082\n",
      "Gradient norm: 1.2026636600494385\n",
      "G loss: 11.646151542663574\n",
      "Iteration 201\n",
      "D loss: -43.77225112915039\n",
      "GP: 7.0825042724609375\n",
      "Gradient norm: 1.195328950881958\n",
      "G loss: 11.202583312988281\n",
      "Iteration 251\n",
      "D loss: -43.238136291503906\n",
      "GP: 5.7392377853393555\n",
      "Gradient norm: 1.1689225435256958\n",
      "G loss: 11.412942886352539\n",
      "Iteration 301\n",
      "D loss: -40.94557189941406\n",
      "GP: 6.302923679351807\n",
      "Gradient norm: 1.1608946323394775\n",
      "G loss: 10.438523292541504\n",
      "Iteration 351\n",
      "D loss: -43.26475524902344\n",
      "GP: 10.814264297485352\n",
      "Gradient norm: 1.2484898567199707\n",
      "G loss: 7.948671817779541\n",
      "\n",
      "Epoch 157\n",
      "Iteration 1\n",
      "D loss: -41.10157012939453\n",
      "GP: 7.2642364501953125\n",
      "Gradient norm: 1.1647088527679443\n",
      "G loss: 9.890572547912598\n",
      "Iteration 51\n",
      "D loss: -43.55381774902344\n",
      "GP: 8.292008399963379\n",
      "Gradient norm: 1.2058980464935303\n",
      "G loss: 11.06495475769043\n",
      "Iteration 101\n",
      "D loss: -44.129852294921875\n",
      "GP: 8.085465431213379\n",
      "Gradient norm: 1.2031996250152588\n",
      "G loss: 11.105234146118164\n",
      "Iteration 151\n",
      "D loss: -45.68901062011719\n",
      "GP: 8.806344032287598\n",
      "Gradient norm: 1.2184516191482544\n",
      "G loss: 12.273366928100586\n",
      "Iteration 201\n",
      "D loss: -42.671451568603516\n",
      "GP: 6.087350368499756\n",
      "Gradient norm: 1.1680203676223755\n",
      "G loss: 11.064257621765137\n",
      "Iteration 251\n",
      "D loss: -44.58486557006836\n",
      "GP: 7.517033100128174\n",
      "Gradient norm: 1.1912940740585327\n",
      "G loss: 11.859254837036133\n",
      "Iteration 301\n",
      "D loss: -45.07435989379883\n",
      "GP: 4.906424522399902\n",
      "Gradient norm: 1.1411006450653076\n",
      "G loss: 11.216043472290039\n",
      "Iteration 351\n",
      "D loss: -45.61147689819336\n",
      "GP: 5.9048662185668945\n",
      "Gradient norm: 1.169593095779419\n",
      "G loss: 10.50536823272705\n",
      "\n",
      "Epoch 158\n",
      "Iteration 1\n",
      "D loss: -41.76141357421875\n",
      "GP: 8.103063583374023\n",
      "Gradient norm: 1.204906702041626\n",
      "G loss: 11.008233070373535\n",
      "Iteration 51\n",
      "D loss: -45.35236740112305\n",
      "GP: 5.978379249572754\n",
      "Gradient norm: 1.173158884048462\n",
      "G loss: 10.827531814575195\n",
      "Iteration 101\n",
      "D loss: -42.71734619140625\n",
      "GP: 7.643261432647705\n",
      "Gradient norm: 1.195664405822754\n",
      "G loss: 11.355018615722656\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 151\n",
      "D loss: -42.63785171508789\n",
      "GP: 8.09878158569336\n",
      "Gradient norm: 1.210257649421692\n",
      "G loss: 11.161617279052734\n",
      "Iteration 201\n",
      "D loss: -42.299991607666016\n",
      "GP: 7.204960823059082\n",
      "Gradient norm: 1.1898033618927002\n",
      "G loss: 9.598123550415039\n",
      "Iteration 251\n",
      "D loss: -45.08039093017578\n",
      "GP: 7.68824577331543\n",
      "Gradient norm: 1.195513129234314\n",
      "G loss: 10.27978229522705\n",
      "Iteration 301\n",
      "D loss: -42.5727424621582\n",
      "GP: 6.289832592010498\n",
      "Gradient norm: 1.165084719657898\n",
      "G loss: 10.123855590820312\n",
      "Iteration 351\n",
      "D loss: -43.38259506225586\n",
      "GP: 6.275649070739746\n",
      "Gradient norm: 1.1689437627792358\n",
      "G loss: 11.030874252319336\n",
      "\n",
      "Epoch 159\n",
      "Iteration 1\n",
      "D loss: -40.35023880004883\n",
      "GP: 6.906464099884033\n",
      "Gradient norm: 1.1712098121643066\n",
      "G loss: 11.260481834411621\n",
      "Iteration 51\n",
      "D loss: -43.959022521972656\n",
      "GP: 7.1953511238098145\n",
      "Gradient norm: 1.1942845582962036\n",
      "G loss: 10.328581809997559\n",
      "Iteration 101\n",
      "D loss: -41.253902435302734\n",
      "GP: 7.397767066955566\n",
      "Gradient norm: 1.1933989524841309\n",
      "G loss: 11.387974739074707\n",
      "Iteration 151\n",
      "D loss: -44.39198303222656\n",
      "GP: 7.250844955444336\n",
      "Gradient norm: 1.1888676881790161\n",
      "G loss: 10.894344329833984\n",
      "Iteration 201\n",
      "D loss: -41.746986389160156\n",
      "GP: 6.922865867614746\n",
      "Gradient norm: 1.1692978143692017\n",
      "G loss: 11.090621948242188\n",
      "Iteration 251\n",
      "D loss: -43.27153396606445\n",
      "GP: 5.0434370040893555\n",
      "Gradient norm: 1.1394418478012085\n",
      "G loss: 10.962485313415527\n",
      "Iteration 301\n",
      "D loss: -43.456966400146484\n",
      "GP: 7.502896785736084\n",
      "Gradient norm: 1.1893991231918335\n",
      "G loss: 10.017767906188965\n",
      "Iteration 351\n",
      "D loss: -42.728858947753906\n",
      "GP: 9.956729888916016\n",
      "Gradient norm: 1.2365927696228027\n",
      "G loss: 11.183795928955078\n",
      "\n",
      "Epoch 160\n",
      "Iteration 1\n",
      "D loss: -43.866729736328125\n",
      "GP: 8.644611358642578\n",
      "Gradient norm: 1.2034894227981567\n",
      "G loss: 10.95089340209961\n",
      "Iteration 51\n",
      "D loss: -46.012474060058594\n",
      "GP: 6.481628894805908\n",
      "Gradient norm: 1.1675690412521362\n",
      "G loss: 10.740850448608398\n",
      "Iteration 101\n",
      "D loss: -44.114891052246094\n",
      "GP: 6.050924777984619\n",
      "Gradient norm: 1.1651250123977661\n",
      "G loss: 10.474677085876465\n",
      "Iteration 151\n",
      "D loss: -43.42457580566406\n",
      "GP: 9.517915725708008\n",
      "Gradient norm: 1.221275806427002\n",
      "G loss: 10.116401672363281\n",
      "Iteration 201\n",
      "D loss: -43.37332534790039\n",
      "GP: 6.536160469055176\n",
      "Gradient norm: 1.1796040534973145\n",
      "G loss: 9.7428560256958\n",
      "Iteration 251\n",
      "D loss: -43.78511047363281\n",
      "GP: 8.562287330627441\n",
      "Gradient norm: 1.2102400064468384\n",
      "G loss: 10.372851371765137\n",
      "Iteration 301\n",
      "D loss: -43.147315979003906\n",
      "GP: 6.601059913635254\n",
      "Gradient norm: 1.1831616163253784\n",
      "G loss: 11.106462478637695\n",
      "Iteration 351\n",
      "D loss: -43.52688980102539\n",
      "GP: 7.988261699676514\n",
      "Gradient norm: 1.196653962135315\n",
      "G loss: 9.523725509643555\n",
      "\n",
      "Epoch 161\n",
      "Iteration 1\n",
      "D loss: -44.39665985107422\n",
      "GP: 7.704195499420166\n",
      "Gradient norm: 1.2005023956298828\n",
      "G loss: 10.397448539733887\n",
      "Iteration 51\n",
      "D loss: -43.86618423461914\n",
      "GP: 6.498463153839111\n",
      "Gradient norm: 1.1904634237289429\n",
      "G loss: 11.438138961791992\n",
      "Iteration 101\n",
      "D loss: -42.89207458496094\n",
      "GP: 8.347879409790039\n",
      "Gradient norm: 1.2059264183044434\n",
      "G loss: 9.793344497680664\n",
      "Iteration 151\n",
      "D loss: -41.65684127807617\n",
      "GP: 6.662002086639404\n",
      "Gradient norm: 1.179535150527954\n",
      "G loss: 9.978588104248047\n",
      "Iteration 201\n",
      "D loss: -42.43205261230469\n",
      "GP: 6.398855209350586\n",
      "Gradient norm: 1.1672136783599854\n",
      "G loss: 10.643411636352539\n",
      "Iteration 251\n",
      "D loss: -41.77092361450195\n",
      "GP: 6.1897687911987305\n",
      "Gradient norm: 1.1458759307861328\n",
      "G loss: 10.445484161376953\n",
      "Iteration 301\n",
      "D loss: -44.46044921875\n",
      "GP: 4.542830944061279\n",
      "Gradient norm: 1.1391282081604004\n",
      "G loss: 10.252253532409668\n",
      "Iteration 351\n",
      "D loss: -43.79897689819336\n",
      "GP: 8.1724853515625\n",
      "Gradient norm: 1.2082364559173584\n",
      "G loss: 10.009685516357422\n",
      "\n",
      "Epoch 162\n",
      "Iteration 1\n",
      "D loss: -45.796749114990234\n",
      "GP: 5.884716987609863\n",
      "Gradient norm: 1.157056212425232\n",
      "G loss: 9.483552932739258\n",
      "Iteration 51\n",
      "D loss: -42.34340286254883\n",
      "GP: 6.778590679168701\n",
      "Gradient norm: 1.1898021697998047\n",
      "G loss: 9.40214729309082\n",
      "Iteration 101\n",
      "D loss: -42.25447463989258\n",
      "GP: 5.569023132324219\n",
      "Gradient norm: 1.1621544361114502\n",
      "G loss: 10.246064186096191\n",
      "Iteration 151\n",
      "D loss: -41.167850494384766\n",
      "GP: 7.965724945068359\n",
      "Gradient norm: 1.1886019706726074\n",
      "G loss: 11.034114837646484\n",
      "Iteration 201\n",
      "D loss: -42.84769058227539\n",
      "GP: 7.921135902404785\n",
      "Gradient norm: 1.197640061378479\n",
      "G loss: 9.59341812133789\n",
      "Iteration 251\n",
      "D loss: -41.480194091796875\n",
      "GP: 6.235424041748047\n",
      "Gradient norm: 1.1608387231826782\n",
      "G loss: 9.79621410369873\n",
      "Iteration 301\n",
      "D loss: -42.405517578125\n",
      "GP: 7.443601608276367\n",
      "Gradient norm: 1.1943565607070923\n",
      "G loss: 10.131653785705566\n",
      "Iteration 351\n",
      "D loss: -45.26529312133789\n",
      "GP: 6.916939735412598\n",
      "Gradient norm: 1.1949777603149414\n",
      "G loss: 9.783795356750488\n",
      "\n",
      "Epoch 163\n",
      "Iteration 1\n",
      "D loss: -42.49716567993164\n",
      "GP: 7.942709445953369\n",
      "Gradient norm: 1.1974250078201294\n",
      "G loss: 11.005916595458984\n",
      "Iteration 51\n",
      "D loss: -43.39165115356445\n",
      "GP: 7.558521270751953\n",
      "Gradient norm: 1.1734676361083984\n",
      "G loss: 10.661725044250488\n",
      "Iteration 101\n",
      "D loss: -44.209434509277344\n",
      "GP: 12.692956924438477\n",
      "Gradient norm: 1.2794106006622314\n",
      "G loss: 8.904107093811035\n",
      "Iteration 151\n",
      "D loss: -42.33395004272461\n",
      "GP: 7.554900646209717\n",
      "Gradient norm: 1.1726466417312622\n",
      "G loss: 11.07618522644043\n",
      "Iteration 201\n",
      "D loss: -42.671024322509766\n",
      "GP: 7.1966352462768555\n",
      "Gradient norm: 1.1899404525756836\n",
      "G loss: 10.40665340423584\n",
      "Iteration 251\n",
      "D loss: -41.89620590209961\n",
      "GP: 4.9091362953186035\n",
      "Gradient norm: 1.1466151475906372\n",
      "G loss: 11.095536231994629\n",
      "Iteration 301\n",
      "D loss: -43.83971405029297\n",
      "GP: 6.382486343383789\n",
      "Gradient norm: 1.170002818107605\n",
      "G loss: 10.242788314819336\n",
      "Iteration 351\n",
      "D loss: -41.810829162597656\n",
      "GP: 7.167181968688965\n",
      "Gradient norm: 1.1611467599868774\n",
      "G loss: 10.459670066833496\n",
      "\n",
      "Epoch 164\n",
      "Iteration 1\n",
      "D loss: -44.32691192626953\n",
      "GP: 10.533279418945312\n",
      "Gradient norm: 1.2609257698059082\n",
      "G loss: 10.23175048828125\n",
      "Iteration 51\n",
      "D loss: -43.25407409667969\n",
      "GP: 7.10892391204834\n",
      "Gradient norm: 1.183998942375183\n",
      "G loss: 10.746554374694824\n",
      "Iteration 101\n",
      "D loss: -42.046409606933594\n",
      "GP: 5.63818883895874\n",
      "Gradient norm: 1.1709721088409424\n",
      "G loss: 9.989110946655273\n",
      "Iteration 151\n",
      "D loss: -43.66847229003906\n",
      "GP: 8.802680015563965\n",
      "Gradient norm: 1.2259106636047363\n",
      "G loss: 10.396100997924805\n",
      "Iteration 201\n",
      "D loss: -45.5648307800293\n",
      "GP: 6.2914228439331055\n",
      "Gradient norm: 1.1611413955688477\n",
      "G loss: 11.4519624710083\n",
      "Iteration 251\n",
      "D loss: -43.86737823486328\n",
      "GP: 6.867288589477539\n",
      "Gradient norm: 1.1905299425125122\n",
      "G loss: 10.7825345993042\n",
      "Iteration 301\n",
      "D loss: -42.16516876220703\n",
      "GP: 7.689589977264404\n",
      "Gradient norm: 1.198669672012329\n",
      "G loss: 11.423177719116211\n",
      "Iteration 351\n",
      "D loss: -45.34154510498047\n",
      "GP: 5.565613746643066\n",
      "Gradient norm: 1.1399697065353394\n",
      "G loss: 11.108074188232422\n",
      "\n",
      "Epoch 165\n",
      "Iteration 1\n",
      "D loss: -43.27809524536133\n",
      "GP: 6.316483497619629\n",
      "Gradient norm: 1.1775137186050415\n",
      "G loss: 11.25539779663086\n",
      "Iteration 51\n",
      "D loss: -44.66289138793945\n",
      "GP: 7.8568549156188965\n",
      "Gradient norm: 1.2039058208465576\n",
      "G loss: 12.068244934082031\n",
      "Iteration 101\n",
      "D loss: -42.26372528076172\n",
      "GP: 9.592639923095703\n",
      "Gradient norm: 1.235906958580017\n",
      "G loss: 9.882158279418945\n",
      "Iteration 151\n",
      "D loss: -41.656864166259766\n",
      "GP: 7.7418951988220215\n",
      "Gradient norm: 1.2024437189102173\n",
      "G loss: 11.393531799316406\n",
      "Iteration 201\n",
      "D loss: -42.180580139160156\n",
      "GP: 6.457797050476074\n",
      "Gradient norm: 1.1731594800949097\n",
      "G loss: 9.830612182617188\n",
      "Iteration 251\n",
      "D loss: -45.37118148803711\n",
      "GP: 9.347289085388184\n",
      "Gradient norm: 1.2289512157440186\n",
      "G loss: 10.129739761352539\n",
      "Iteration 301\n",
      "D loss: -44.99796676635742\n",
      "GP: 7.256784915924072\n",
      "Gradient norm: 1.192731499671936\n",
      "G loss: 10.155182838439941\n",
      "Iteration 351\n",
      "D loss: -40.92601776123047\n",
      "GP: 5.239923477172852\n",
      "Gradient norm: 1.1529009342193604\n",
      "G loss: 10.179981231689453\n",
      "\n",
      "Epoch 166\n",
      "Iteration 1\n",
      "D loss: -44.66264724731445\n",
      "GP: 6.908473491668701\n",
      "Gradient norm: 1.1950498819351196\n",
      "G loss: 10.442336082458496\n",
      "Iteration 51\n",
      "D loss: -42.13261032104492\n",
      "GP: 6.481028079986572\n",
      "Gradient norm: 1.1835737228393555\n",
      "G loss: 11.199220657348633\n",
      "Iteration 101\n",
      "D loss: -44.56593704223633\n",
      "GP: 5.584259510040283\n",
      "Gradient norm: 1.1577562093734741\n",
      "G loss: 10.072233200073242\n",
      "Iteration 151\n",
      "D loss: -44.80183792114258\n",
      "GP: 7.2567315101623535\n",
      "Gradient norm: 1.2116583585739136\n",
      "G loss: 10.405524253845215\n",
      "Iteration 201\n",
      "D loss: -43.20977020263672\n",
      "GP: 6.8114848136901855\n",
      "Gradient norm: 1.1876450777053833\n",
      "G loss: 9.910544395446777\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 251\n",
      "D loss: -44.19392776489258\n",
      "GP: 7.515331268310547\n",
      "Gradient norm: 1.1724504232406616\n",
      "G loss: 9.216492652893066\n",
      "Iteration 301\n",
      "D loss: -43.54234313964844\n",
      "GP: 5.936005115509033\n",
      "Gradient norm: 1.1662517786026\n",
      "G loss: 11.231467247009277\n",
      "Iteration 351\n",
      "D loss: -41.90782928466797\n",
      "GP: 7.6199951171875\n",
      "Gradient norm: 1.2018903493881226\n",
      "G loss: 10.891697883605957\n",
      "\n",
      "Epoch 167\n",
      "Iteration 1\n",
      "D loss: -42.05669403076172\n",
      "GP: 5.523595809936523\n",
      "Gradient norm: 1.1580922603607178\n",
      "G loss: 10.258955001831055\n",
      "Iteration 51\n",
      "D loss: -47.30026626586914\n",
      "GP: 7.151599884033203\n",
      "Gradient norm: 1.1798527240753174\n",
      "G loss: 11.198417663574219\n",
      "Iteration 101\n",
      "D loss: -43.51435852050781\n",
      "GP: 6.869340419769287\n",
      "Gradient norm: 1.1791632175445557\n",
      "G loss: 9.889423370361328\n",
      "Iteration 151\n",
      "D loss: -42.98408889770508\n",
      "GP: 8.028095245361328\n",
      "Gradient norm: 1.2079143524169922\n",
      "G loss: 9.123421669006348\n",
      "Iteration 201\n",
      "D loss: -43.68748092651367\n",
      "GP: 6.799106597900391\n",
      "Gradient norm: 1.1620683670043945\n",
      "G loss: 8.999800682067871\n",
      "Iteration 251\n",
      "D loss: -41.8658561706543\n",
      "GP: 8.387320518493652\n",
      "Gradient norm: 1.1922961473464966\n",
      "G loss: 8.951706886291504\n",
      "Iteration 301\n",
      "D loss: -41.16865921020508\n",
      "GP: 6.293484687805176\n",
      "Gradient norm: 1.1674201488494873\n",
      "G loss: 8.474015235900879\n",
      "Iteration 351\n",
      "D loss: -42.519168853759766\n",
      "GP: 5.696867942810059\n",
      "Gradient norm: 1.163767695426941\n",
      "G loss: 9.405767440795898\n",
      "\n",
      "Epoch 168\n",
      "Iteration 1\n",
      "D loss: -45.92341232299805\n",
      "GP: 5.068462371826172\n",
      "Gradient norm: 1.1395760774612427\n",
      "G loss: 10.262250900268555\n",
      "Iteration 51\n",
      "D loss: -44.6573371887207\n",
      "GP: 8.221061706542969\n",
      "Gradient norm: 1.2115588188171387\n",
      "G loss: 9.627361297607422\n",
      "Iteration 101\n",
      "D loss: -43.949745178222656\n",
      "GP: 7.098798751831055\n",
      "Gradient norm: 1.1871654987335205\n",
      "G loss: 10.542985916137695\n",
      "Iteration 151\n",
      "D loss: -44.23865509033203\n",
      "GP: 6.645854949951172\n",
      "Gradient norm: 1.183088779449463\n",
      "G loss: 10.872933387756348\n",
      "Iteration 201\n",
      "D loss: -43.424686431884766\n",
      "GP: 6.231609344482422\n",
      "Gradient norm: 1.1683506965637207\n",
      "G loss: 9.400328636169434\n",
      "Iteration 251\n",
      "D loss: -42.304195404052734\n",
      "GP: 5.011146545410156\n",
      "Gradient norm: 1.1487491130828857\n",
      "G loss: 9.33065414428711\n",
      "Iteration 301\n",
      "D loss: -40.935203552246094\n",
      "GP: 6.680562496185303\n",
      "Gradient norm: 1.1562800407409668\n",
      "G loss: 8.995753288269043\n",
      "Iteration 351\n",
      "D loss: -46.73432159423828\n",
      "GP: 6.847760200500488\n",
      "Gradient norm: 1.1723169088363647\n",
      "G loss: 8.424898147583008\n",
      "\n",
      "Epoch 169\n",
      "Iteration 1\n",
      "D loss: -40.90351867675781\n",
      "GP: 7.427704334259033\n",
      "Gradient norm: 1.1848541498184204\n",
      "G loss: 10.038846015930176\n",
      "Iteration 51\n",
      "D loss: -43.385040283203125\n",
      "GP: 6.253567695617676\n",
      "Gradient norm: 1.16887629032135\n",
      "G loss: 9.551871299743652\n",
      "Iteration 101\n",
      "D loss: -44.353485107421875\n",
      "GP: 5.173778533935547\n",
      "Gradient norm: 1.136315107345581\n",
      "G loss: 10.831596374511719\n",
      "Iteration 151\n",
      "D loss: -43.4940185546875\n",
      "GP: 7.966771602630615\n",
      "Gradient norm: 1.1933263540267944\n",
      "G loss: 9.151482582092285\n",
      "Iteration 201\n",
      "D loss: -42.72106170654297\n",
      "GP: 5.825370788574219\n",
      "Gradient norm: 1.1471608877182007\n",
      "G loss: 9.734012603759766\n",
      "Iteration 251\n",
      "D loss: -42.398780822753906\n",
      "GP: 6.050312519073486\n",
      "Gradient norm: 1.1450554132461548\n",
      "G loss: 8.927589416503906\n",
      "Iteration 301\n",
      "D loss: -43.23117446899414\n",
      "GP: 6.667449474334717\n",
      "Gradient norm: 1.1885751485824585\n",
      "G loss: 9.682113647460938\n",
      "Iteration 351\n",
      "D loss: -42.942405700683594\n",
      "GP: 7.256606101989746\n",
      "Gradient norm: 1.197213888168335\n",
      "G loss: 8.71675968170166\n",
      "\n",
      "Epoch 170\n",
      "Iteration 1\n",
      "D loss: -43.133113861083984\n",
      "GP: 6.564888954162598\n",
      "Gradient norm: 1.1733932495117188\n",
      "G loss: 9.167088508605957\n",
      "Iteration 51\n",
      "D loss: -42.63254928588867\n",
      "GP: 7.021907806396484\n",
      "Gradient norm: 1.1656172275543213\n",
      "G loss: 9.222189903259277\n",
      "Iteration 101\n",
      "D loss: -43.62950134277344\n",
      "GP: 7.450319290161133\n",
      "Gradient norm: 1.204949140548706\n",
      "G loss: 9.169694900512695\n",
      "Iteration 151\n",
      "D loss: -45.03660202026367\n",
      "GP: 6.146082878112793\n",
      "Gradient norm: 1.175199270248413\n",
      "G loss: 10.347148895263672\n",
      "Iteration 201\n",
      "D loss: -41.821346282958984\n",
      "GP: 5.9178266525268555\n",
      "Gradient norm: 1.1490826606750488\n",
      "G loss: 9.3721923828125\n",
      "Iteration 251\n",
      "D loss: -45.47476577758789\n",
      "GP: 7.424465179443359\n",
      "Gradient norm: 1.200166940689087\n",
      "G loss: 10.085362434387207\n",
      "Iteration 301\n",
      "D loss: -45.1455078125\n",
      "GP: 7.875672340393066\n",
      "Gradient norm: 1.190934658050537\n",
      "G loss: 7.7207560539245605\n",
      "Iteration 351\n",
      "D loss: -41.59904479980469\n",
      "GP: 8.786127090454102\n",
      "Gradient norm: 1.2292020320892334\n",
      "G loss: 8.664066314697266\n",
      "\n",
      "Epoch 171\n",
      "Iteration 1\n",
      "D loss: -44.43722915649414\n",
      "GP: 6.7849016189575195\n",
      "Gradient norm: 1.1879788637161255\n",
      "G loss: 8.85008716583252\n",
      "Iteration 51\n",
      "D loss: -44.62014389038086\n",
      "GP: 6.088282585144043\n",
      "Gradient norm: 1.1670253276824951\n",
      "G loss: 9.471650123596191\n",
      "Iteration 101\n",
      "D loss: -43.83591079711914\n",
      "GP: 5.880471229553223\n",
      "Gradient norm: 1.150525689125061\n",
      "G loss: 8.211553573608398\n",
      "Iteration 151\n",
      "D loss: -42.1922492980957\n",
      "GP: 7.5492262840271\n",
      "Gradient norm: 1.1994503736495972\n",
      "G loss: 9.108928680419922\n",
      "Iteration 201\n",
      "D loss: -43.989322662353516\n",
      "GP: 9.216765403747559\n",
      "Gradient norm: 1.203101396560669\n",
      "G loss: 8.538527488708496\n",
      "Iteration 251\n",
      "D loss: -41.28388214111328\n",
      "GP: 6.8021769523620605\n",
      "Gradient norm: 1.1784958839416504\n",
      "G loss: 8.788228034973145\n",
      "Iteration 301\n",
      "D loss: -43.10274124145508\n",
      "GP: 7.548151969909668\n",
      "Gradient norm: 1.1891605854034424\n",
      "G loss: 8.643830299377441\n",
      "Iteration 351\n",
      "D loss: -44.100746154785156\n",
      "GP: 7.131627082824707\n",
      "Gradient norm: 1.184559941291809\n",
      "G loss: 7.97439432144165\n",
      "\n",
      "Epoch 172\n",
      "Iteration 1\n",
      "D loss: -41.747982025146484\n",
      "GP: 7.090742588043213\n",
      "Gradient norm: 1.1834419965744019\n",
      "G loss: 9.183707237243652\n",
      "Iteration 51\n",
      "D loss: -44.16280746459961\n",
      "GP: 7.382509708404541\n",
      "Gradient norm: 1.205885648727417\n",
      "G loss: 9.750045776367188\n",
      "Iteration 101\n",
      "D loss: -45.65480422973633\n",
      "GP: 5.961224555969238\n",
      "Gradient norm: 1.152335524559021\n",
      "G loss: 8.957633018493652\n",
      "Iteration 151\n",
      "D loss: -41.72648620605469\n",
      "GP: 4.679296970367432\n",
      "Gradient norm: 1.1481486558914185\n",
      "G loss: 9.667465209960938\n",
      "Iteration 201\n",
      "D loss: -43.953651428222656\n",
      "GP: 5.426576137542725\n",
      "Gradient norm: 1.160954236984253\n",
      "G loss: 8.31711483001709\n",
      "Iteration 251\n",
      "D loss: -44.93623733520508\n",
      "GP: 7.641437530517578\n",
      "Gradient norm: 1.197427749633789\n",
      "G loss: 7.784210681915283\n",
      "Iteration 301\n",
      "D loss: -41.11677551269531\n",
      "GP: 5.324621677398682\n",
      "Gradient norm: 1.1572167873382568\n",
      "G loss: 8.208621978759766\n",
      "Iteration 351\n",
      "D loss: -40.241729736328125\n",
      "GP: 5.004873752593994\n",
      "Gradient norm: 1.1526199579238892\n",
      "G loss: 8.65114688873291\n",
      "\n",
      "Epoch 173\n",
      "Iteration 1\n",
      "D loss: -42.104007720947266\n",
      "GP: 6.402739524841309\n",
      "Gradient norm: 1.1803845167160034\n",
      "G loss: 8.520051956176758\n",
      "Iteration 51\n",
      "D loss: -43.12419509887695\n",
      "GP: 7.330692291259766\n",
      "Gradient norm: 1.1773685216903687\n",
      "G loss: 9.359010696411133\n",
      "Iteration 101\n",
      "D loss: -44.31645965576172\n",
      "GP: 7.66076135635376\n",
      "Gradient norm: 1.1930209398269653\n",
      "G loss: 7.657517433166504\n",
      "Iteration 151\n",
      "D loss: -42.902854919433594\n",
      "GP: 7.12162971496582\n",
      "Gradient norm: 1.1863853931427002\n",
      "G loss: 8.500717163085938\n",
      "Iteration 201\n",
      "D loss: -42.48403549194336\n",
      "GP: 6.976833343505859\n",
      "Gradient norm: 1.1909458637237549\n",
      "G loss: 8.185352325439453\n",
      "Iteration 251\n",
      "D loss: -43.84538269042969\n",
      "GP: 7.980319023132324\n",
      "Gradient norm: 1.1854512691497803\n",
      "G loss: 8.952676773071289\n",
      "Iteration 301\n",
      "D loss: -42.157867431640625\n",
      "GP: 6.89807653427124\n",
      "Gradient norm: 1.2021915912628174\n",
      "G loss: 9.290985107421875\n",
      "Iteration 351\n",
      "D loss: -40.536277770996094\n",
      "GP: 7.475763320922852\n",
      "Gradient norm: 1.1832400560379028\n",
      "G loss: 7.578108310699463\n",
      "\n",
      "Epoch 174\n",
      "Iteration 1\n",
      "D loss: -42.00883483886719\n",
      "GP: 6.418718338012695\n",
      "Gradient norm: 1.1782500743865967\n",
      "G loss: 8.315146446228027\n",
      "Iteration 51\n",
      "D loss: -45.0048713684082\n",
      "GP: 5.958909511566162\n",
      "Gradient norm: 1.154767394065857\n",
      "G loss: 9.376154899597168\n",
      "Iteration 101\n",
      "D loss: -41.52690887451172\n",
      "GP: 5.840826511383057\n",
      "Gradient norm: 1.1660805940628052\n",
      "G loss: 10.696035385131836\n",
      "Iteration 151\n",
      "D loss: -43.434967041015625\n",
      "GP: 5.546331405639648\n",
      "Gradient norm: 1.1618146896362305\n",
      "G loss: 9.278813362121582\n",
      "Iteration 201\n",
      "D loss: -45.98409652709961\n",
      "GP: 8.264571189880371\n",
      "Gradient norm: 1.1940547227859497\n",
      "G loss: 9.446690559387207\n",
      "Iteration 251\n",
      "D loss: -43.228694915771484\n",
      "GP: 6.438016414642334\n",
      "Gradient norm: 1.1637312173843384\n",
      "G loss: 8.262267112731934\n",
      "Iteration 301\n",
      "D loss: -42.84199905395508\n",
      "GP: 8.283340454101562\n",
      "Gradient norm: 1.206470012664795\n",
      "G loss: 7.7730255126953125\n",
      "Iteration 351\n",
      "D loss: -42.674076080322266\n",
      "GP: 7.055857181549072\n",
      "Gradient norm: 1.1840583086013794\n",
      "G loss: 8.447555541992188\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 175\n",
      "Iteration 1\n",
      "D loss: -42.52950668334961\n",
      "GP: 7.346904754638672\n",
      "Gradient norm: 1.204232096672058\n",
      "G loss: 8.766777038574219\n",
      "Iteration 51\n",
      "D loss: -44.32416915893555\n",
      "GP: 5.812816143035889\n",
      "Gradient norm: 1.162803053855896\n",
      "G loss: 9.578554153442383\n",
      "Iteration 101\n",
      "D loss: -43.662296295166016\n",
      "GP: 5.802325248718262\n",
      "Gradient norm: 1.1646405458450317\n",
      "G loss: 6.983448028564453\n",
      "Iteration 151\n",
      "D loss: -43.28953552246094\n",
      "GP: 6.195048809051514\n",
      "Gradient norm: 1.152497410774231\n",
      "G loss: 8.461283683776855\n",
      "Iteration 201\n",
      "D loss: -44.07952880859375\n",
      "GP: 7.210288047790527\n",
      "Gradient norm: 1.199709415435791\n",
      "G loss: 7.858500957489014\n",
      "Iteration 251\n",
      "D loss: -43.01838684082031\n",
      "GP: 7.256510257720947\n",
      "Gradient norm: 1.1912617683410645\n",
      "G loss: 9.31982707977295\n",
      "Iteration 301\n",
      "D loss: -46.29134750366211\n",
      "GP: 7.170962333679199\n",
      "Gradient norm: 1.198848843574524\n",
      "G loss: 6.672261714935303\n",
      "Iteration 351\n",
      "D loss: -42.34696960449219\n",
      "GP: 6.548547744750977\n",
      "Gradient norm: 1.1854748725891113\n",
      "G loss: 8.628813743591309\n",
      "\n",
      "Epoch 176\n",
      "Iteration 1\n",
      "D loss: -42.81416320800781\n",
      "GP: 7.366910457611084\n",
      "Gradient norm: 1.1909685134887695\n",
      "G loss: 9.861526489257812\n",
      "Iteration 51\n",
      "D loss: -45.270904541015625\n",
      "GP: 8.068025588989258\n",
      "Gradient norm: 1.210271954536438\n",
      "G loss: 9.55853271484375\n",
      "Iteration 101\n",
      "D loss: -42.845890045166016\n",
      "GP: 6.931114196777344\n",
      "Gradient norm: 1.1730222702026367\n",
      "G loss: 9.135180473327637\n",
      "Iteration 151\n",
      "D loss: -42.1099967956543\n",
      "GP: 6.852325439453125\n",
      "Gradient norm: 1.1830195188522339\n",
      "G loss: 9.274820327758789\n",
      "Iteration 201\n",
      "D loss: -44.52470397949219\n",
      "GP: 5.948240280151367\n",
      "Gradient norm: 1.1524336338043213\n",
      "G loss: 8.64425277709961\n",
      "Iteration 251\n",
      "D loss: -45.576171875\n",
      "GP: 6.034543991088867\n",
      "Gradient norm: 1.1638883352279663\n",
      "G loss: 8.399613380432129\n",
      "Iteration 301\n",
      "D loss: -42.22958755493164\n",
      "GP: 8.190583229064941\n",
      "Gradient norm: 1.1944732666015625\n",
      "G loss: 8.084983825683594\n",
      "Iteration 351\n",
      "D loss: -41.91483688354492\n",
      "GP: 7.48666524887085\n",
      "Gradient norm: 1.1867235898971558\n",
      "G loss: 8.923114776611328\n",
      "\n",
      "Epoch 177\n",
      "Iteration 1\n",
      "D loss: -43.769309997558594\n",
      "GP: 6.7754974365234375\n",
      "Gradient norm: 1.1729966402053833\n",
      "G loss: 10.022442817687988\n",
      "Iteration 51\n",
      "D loss: -41.90666961669922\n",
      "GP: 7.808429718017578\n",
      "Gradient norm: 1.1985455751419067\n",
      "G loss: 8.778482437133789\n",
      "Iteration 101\n",
      "D loss: -44.08293151855469\n",
      "GP: 6.38681697845459\n",
      "Gradient norm: 1.1551082134246826\n",
      "G loss: 8.364076614379883\n",
      "Iteration 151\n",
      "D loss: -45.19756317138672\n",
      "GP: 6.734925270080566\n",
      "Gradient norm: 1.177964210510254\n",
      "G loss: 8.925841331481934\n",
      "Iteration 201\n",
      "D loss: -45.319637298583984\n",
      "GP: 6.705334186553955\n",
      "Gradient norm: 1.1857688426971436\n",
      "G loss: 7.937478065490723\n",
      "Iteration 251\n",
      "D loss: -44.55063247680664\n",
      "GP: 7.617895603179932\n",
      "Gradient norm: 1.202300786972046\n",
      "G loss: 7.862079620361328\n",
      "Iteration 301\n",
      "D loss: -44.802825927734375\n",
      "GP: 6.023726940155029\n",
      "Gradient norm: 1.1761753559112549\n",
      "G loss: 9.30581283569336\n",
      "Iteration 351\n",
      "D loss: -44.16581726074219\n",
      "GP: 5.851598262786865\n",
      "Gradient norm: 1.1699261665344238\n",
      "G loss: 8.53219985961914\n",
      "\n",
      "Epoch 178\n",
      "Iteration 1\n",
      "D loss: -42.24345779418945\n",
      "GP: 9.329298973083496\n",
      "Gradient norm: 1.2226239442825317\n",
      "G loss: 8.174219131469727\n",
      "Iteration 51\n",
      "D loss: -41.774322509765625\n",
      "GP: 5.422548294067383\n",
      "Gradient norm: 1.1540448665618896\n",
      "G loss: 8.7615385055542\n",
      "Iteration 101\n",
      "D loss: -43.501747131347656\n",
      "GP: 7.270402431488037\n",
      "Gradient norm: 1.1700319051742554\n",
      "G loss: 7.776059150695801\n",
      "Iteration 151\n",
      "D loss: -45.09253692626953\n",
      "GP: 6.818090438842773\n",
      "Gradient norm: 1.1907720565795898\n",
      "G loss: 7.9107584953308105\n",
      "Iteration 201\n",
      "D loss: -44.31800079345703\n",
      "GP: 5.589911460876465\n",
      "Gradient norm: 1.1645692586898804\n",
      "G loss: 8.21049690246582\n",
      "Iteration 251\n",
      "D loss: -43.74937438964844\n",
      "GP: 6.303536415100098\n",
      "Gradient norm: 1.1715526580810547\n",
      "G loss: 7.80355167388916\n",
      "Iteration 301\n",
      "D loss: -44.5312385559082\n",
      "GP: 7.972042560577393\n",
      "Gradient norm: 1.2132619619369507\n",
      "G loss: 8.183771133422852\n",
      "Iteration 351\n",
      "D loss: -42.996307373046875\n",
      "GP: 6.3004679679870605\n",
      "Gradient norm: 1.1713404655456543\n",
      "G loss: 7.695343017578125\n",
      "\n",
      "Epoch 179\n",
      "Iteration 1\n",
      "D loss: -42.44747543334961\n",
      "GP: 8.28234577178955\n",
      "Gradient norm: 1.2063418626785278\n",
      "G loss: 6.545610427856445\n",
      "Iteration 51\n",
      "D loss: -45.16902542114258\n",
      "GP: 7.7207417488098145\n",
      "Gradient norm: 1.1959643363952637\n",
      "G loss: 7.7673234939575195\n",
      "Iteration 101\n",
      "D loss: -44.11528015136719\n",
      "GP: 5.966179370880127\n",
      "Gradient norm: 1.1592426300048828\n",
      "G loss: 8.378483772277832\n",
      "Iteration 151\n",
      "D loss: -41.41169357299805\n",
      "GP: 7.272854328155518\n",
      "Gradient norm: 1.1878525018692017\n",
      "G loss: 7.178709983825684\n",
      "Iteration 201\n",
      "D loss: -44.65904998779297\n",
      "GP: 5.378464698791504\n",
      "Gradient norm: 1.1526248455047607\n",
      "G loss: 8.068093299865723\n",
      "Iteration 251\n",
      "D loss: -43.379112243652344\n",
      "GP: 7.984766006469727\n",
      "Gradient norm: 1.2127175331115723\n",
      "G loss: 8.00296688079834\n",
      "Iteration 301\n",
      "D loss: -45.49927520751953\n",
      "GP: 7.049100875854492\n",
      "Gradient norm: 1.1920557022094727\n",
      "G loss: 7.431476593017578\n",
      "Iteration 351\n",
      "D loss: -44.47047424316406\n",
      "GP: 6.713408946990967\n",
      "Gradient norm: 1.1839022636413574\n",
      "G loss: 7.742464065551758\n",
      "\n",
      "Epoch 180\n",
      "Iteration 1\n",
      "D loss: -42.827518463134766\n",
      "GP: 6.604260444641113\n",
      "Gradient norm: 1.1815412044525146\n",
      "G loss: 6.491584777832031\n",
      "Iteration 51\n",
      "D loss: -43.5256462097168\n",
      "GP: 5.420050621032715\n",
      "Gradient norm: 1.13847815990448\n",
      "G loss: 8.193270683288574\n",
      "Iteration 101\n",
      "D loss: -41.43681335449219\n",
      "GP: 7.984416961669922\n",
      "Gradient norm: 1.2114710807800293\n",
      "G loss: 8.079614639282227\n",
      "Iteration 151\n",
      "D loss: -42.469627380371094\n",
      "GP: 6.6328911781311035\n",
      "Gradient norm: 1.1922084093093872\n",
      "G loss: 8.138614654541016\n",
      "Iteration 201\n",
      "D loss: -46.37056350708008\n",
      "GP: 6.2967658042907715\n",
      "Gradient norm: 1.1654750108718872\n",
      "G loss: 8.913551330566406\n",
      "Iteration 251\n",
      "D loss: -43.833900451660156\n",
      "GP: 8.146163940429688\n",
      "Gradient norm: 1.2107402086257935\n",
      "G loss: 7.5046491622924805\n",
      "Iteration 301\n",
      "D loss: -46.75078582763672\n",
      "GP: 6.313240051269531\n",
      "Gradient norm: 1.1716564893722534\n",
      "G loss: 7.2580718994140625\n",
      "Iteration 351\n",
      "D loss: -43.48786926269531\n",
      "GP: 6.626641750335693\n",
      "Gradient norm: 1.1925458908081055\n",
      "G loss: 7.005659580230713\n",
      "\n",
      "Epoch 181\n",
      "Iteration 1\n",
      "D loss: -44.90766906738281\n",
      "GP: 6.541975975036621\n",
      "Gradient norm: 1.1862766742706299\n",
      "G loss: 7.540046215057373\n",
      "Iteration 51\n",
      "D loss: -42.53396987915039\n",
      "GP: 5.8034348487854\n",
      "Gradient norm: 1.1600642204284668\n",
      "G loss: 7.166705131530762\n",
      "Iteration 101\n",
      "D loss: -42.367408752441406\n",
      "GP: 5.925326347351074\n",
      "Gradient norm: 1.1658450365066528\n",
      "G loss: 7.182578086853027\n",
      "Iteration 151\n",
      "D loss: -43.322731018066406\n",
      "GP: 8.015449523925781\n",
      "Gradient norm: 1.199903130531311\n",
      "G loss: 7.723508834838867\n",
      "Iteration 201\n",
      "D loss: -45.981876373291016\n",
      "GP: 8.216899871826172\n",
      "Gradient norm: 1.1879438161849976\n",
      "G loss: 7.773841381072998\n",
      "Iteration 251\n",
      "D loss: -43.1077880859375\n",
      "GP: 5.884210586547852\n",
      "Gradient norm: 1.1584831476211548\n",
      "G loss: 7.68172550201416\n",
      "Iteration 301\n",
      "D loss: -39.32631301879883\n",
      "GP: 5.090396404266357\n",
      "Gradient norm: 1.116716980934143\n",
      "G loss: 7.842245578765869\n",
      "Iteration 351\n",
      "D loss: -43.66683578491211\n",
      "GP: 6.5031633377075195\n",
      "Gradient norm: 1.1706897020339966\n",
      "G loss: 8.875310897827148\n",
      "\n",
      "Epoch 182\n",
      "Iteration 1\n",
      "D loss: -45.642696380615234\n",
      "GP: 7.565109729766846\n",
      "Gradient norm: 1.2004011869430542\n",
      "G loss: 8.2200345993042\n",
      "Iteration 51\n",
      "D loss: -44.07009506225586\n",
      "GP: 6.486191749572754\n",
      "Gradient norm: 1.182883620262146\n",
      "G loss: 8.322074890136719\n",
      "Iteration 101\n",
      "D loss: -43.06877899169922\n",
      "GP: 6.107720375061035\n",
      "Gradient norm: 1.1750388145446777\n",
      "G loss: 9.257607460021973\n",
      "Iteration 151\n",
      "D loss: -45.39272689819336\n",
      "GP: 6.208557605743408\n",
      "Gradient norm: 1.1649426221847534\n",
      "G loss: 7.306699275970459\n",
      "Iteration 201\n",
      "D loss: -41.76763153076172\n",
      "GP: 6.480592727661133\n",
      "Gradient norm: 1.1727805137634277\n",
      "G loss: 6.611171722412109\n",
      "Iteration 251\n",
      "D loss: -41.974884033203125\n",
      "GP: 6.552369594573975\n",
      "Gradient norm: 1.1809232234954834\n",
      "G loss: 7.625133991241455\n",
      "Iteration 301\n",
      "D loss: -43.188011169433594\n",
      "GP: 7.2041473388671875\n",
      "Gradient norm: 1.1930240392684937\n",
      "G loss: 7.017420768737793\n",
      "Iteration 351\n",
      "D loss: -43.401058197021484\n",
      "GP: 7.690406799316406\n",
      "Gradient norm: 1.214024543762207\n",
      "G loss: 8.281963348388672\n",
      "\n",
      "Epoch 183\n",
      "Iteration 1\n",
      "D loss: -43.23332977294922\n",
      "GP: 6.993789196014404\n",
      "Gradient norm: 1.1893434524536133\n",
      "G loss: 8.897625923156738\n",
      "Iteration 51\n",
      "D loss: -43.61620330810547\n",
      "GP: 6.694830894470215\n",
      "Gradient norm: 1.186919927597046\n",
      "G loss: 8.930667877197266\n",
      "Iteration 101\n",
      "D loss: -46.2161750793457\n",
      "GP: 5.93566370010376\n",
      "Gradient norm: 1.1616346836090088\n",
      "G loss: 8.178622245788574\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 151\n",
      "D loss: -41.820377349853516\n",
      "GP: 6.544631004333496\n",
      "Gradient norm: 1.1636431217193604\n",
      "G loss: 7.585343837738037\n",
      "Iteration 201\n",
      "D loss: -42.203102111816406\n",
      "GP: 5.880734920501709\n",
      "Gradient norm: 1.1733942031860352\n",
      "G loss: 8.663506507873535\n",
      "Iteration 251\n",
      "D loss: -43.64924621582031\n",
      "GP: 8.055233001708984\n",
      "Gradient norm: 1.1841551065444946\n",
      "G loss: 6.927913665771484\n",
      "Iteration 301\n",
      "D loss: -40.18443298339844\n",
      "GP: 8.152483940124512\n",
      "Gradient norm: 1.199450969696045\n",
      "G loss: 7.07976770401001\n",
      "Iteration 351\n",
      "D loss: -42.888187408447266\n",
      "GP: 7.546398639678955\n",
      "Gradient norm: 1.195517897605896\n",
      "G loss: 8.345917701721191\n",
      "\n",
      "Epoch 184\n",
      "Iteration 1\n",
      "D loss: -43.198177337646484\n",
      "GP: 9.079607963562012\n",
      "Gradient norm: 1.2355806827545166\n",
      "G loss: 7.826138496398926\n",
      "Iteration 51\n",
      "D loss: -43.815189361572266\n",
      "GP: 6.912151336669922\n",
      "Gradient norm: 1.1730526685714722\n",
      "G loss: 7.1402058601379395\n",
      "Iteration 101\n",
      "D loss: -42.09947967529297\n",
      "GP: 6.154240608215332\n",
      "Gradient norm: 1.1780864000320435\n",
      "G loss: 7.848694324493408\n",
      "Iteration 151\n",
      "D loss: -43.277584075927734\n",
      "GP: 6.604428291320801\n",
      "Gradient norm: 1.1605157852172852\n",
      "G loss: 7.913784027099609\n",
      "Iteration 201\n",
      "D loss: -43.35115432739258\n",
      "GP: 8.510021209716797\n",
      "Gradient norm: 1.2096349000930786\n",
      "G loss: 7.450944423675537\n",
      "Iteration 251\n",
      "D loss: -46.23207473754883\n",
      "GP: 8.107364654541016\n",
      "Gradient norm: 1.2102233171463013\n",
      "G loss: 6.989376544952393\n",
      "Iteration 301\n",
      "D loss: -46.56439208984375\n",
      "GP: 7.312321186065674\n",
      "Gradient norm: 1.1971858739852905\n",
      "G loss: 6.914446830749512\n",
      "Iteration 351\n",
      "D loss: -43.998592376708984\n",
      "GP: 7.524121284484863\n",
      "Gradient norm: 1.1986738443374634\n",
      "G loss: 8.735360145568848\n",
      "\n",
      "Epoch 185\n",
      "Iteration 1\n",
      "D loss: -44.063636779785156\n",
      "GP: 5.90056037902832\n",
      "Gradient norm: 1.1591728925704956\n",
      "G loss: 7.577933311462402\n",
      "Iteration 51\n",
      "D loss: -44.177146911621094\n",
      "GP: 6.360307693481445\n",
      "Gradient norm: 1.1729100942611694\n",
      "G loss: 8.34184741973877\n",
      "Iteration 101\n",
      "D loss: -42.65705871582031\n",
      "GP: 7.789559364318848\n",
      "Gradient norm: 1.200797438621521\n",
      "G loss: 9.16866397857666\n",
      "Iteration 151\n",
      "D loss: -41.41171646118164\n",
      "GP: 6.143503665924072\n",
      "Gradient norm: 1.1519036293029785\n",
      "G loss: 8.687862396240234\n",
      "Iteration 201\n",
      "D loss: -43.50526809692383\n",
      "GP: 8.923768043518066\n",
      "Gradient norm: 1.2083193063735962\n",
      "G loss: 8.751535415649414\n",
      "Iteration 251\n",
      "D loss: -41.8089485168457\n",
      "GP: 6.173762321472168\n",
      "Gradient norm: 1.170430302619934\n",
      "G loss: 7.635622978210449\n",
      "Iteration 301\n",
      "D loss: -44.66279602050781\n",
      "GP: 6.804101467132568\n",
      "Gradient norm: 1.1892971992492676\n",
      "G loss: 8.282368659973145\n",
      "Iteration 351\n",
      "D loss: -41.24772262573242\n",
      "GP: 7.345265865325928\n",
      "Gradient norm: 1.1936012506484985\n",
      "G loss: 7.870697021484375\n",
      "\n",
      "Epoch 186\n",
      "Iteration 1\n",
      "D loss: -43.480384826660156\n",
      "GP: 5.795821189880371\n",
      "Gradient norm: 1.1597105264663696\n",
      "G loss: 8.792715072631836\n",
      "Iteration 51\n",
      "D loss: -44.304290771484375\n",
      "GP: 7.094757556915283\n",
      "Gradient norm: 1.1830625534057617\n",
      "G loss: 8.418444633483887\n",
      "Iteration 101\n",
      "D loss: -43.630699157714844\n",
      "GP: 5.538544178009033\n",
      "Gradient norm: 1.1397349834442139\n",
      "G loss: 8.801610946655273\n",
      "Iteration 151\n",
      "D loss: -42.832706451416016\n",
      "GP: 7.036035060882568\n",
      "Gradient norm: 1.1581950187683105\n",
      "G loss: 7.798497200012207\n",
      "Iteration 201\n",
      "D loss: -44.53174591064453\n",
      "GP: 7.519399642944336\n",
      "Gradient norm: 1.1860146522521973\n",
      "G loss: 9.475502014160156\n",
      "Iteration 251\n",
      "D loss: -44.902008056640625\n",
      "GP: 7.249070167541504\n",
      "Gradient norm: 1.191851019859314\n",
      "G loss: 8.776402473449707\n",
      "Iteration 301\n",
      "D loss: -41.2835693359375\n",
      "GP: 6.57490348815918\n",
      "Gradient norm: 1.1752984523773193\n",
      "G loss: 7.906065940856934\n",
      "Iteration 351\n",
      "D loss: -42.42974090576172\n",
      "GP: 5.755141258239746\n",
      "Gradient norm: 1.166124701499939\n",
      "G loss: 8.238994598388672\n",
      "\n",
      "Epoch 187\n",
      "Iteration 1\n",
      "D loss: -42.42979049682617\n",
      "GP: 5.497135162353516\n",
      "Gradient norm: 1.154219150543213\n",
      "G loss: 8.360749244689941\n",
      "Iteration 51\n",
      "D loss: -42.76521301269531\n",
      "GP: 6.66596794128418\n",
      "Gradient norm: 1.1723947525024414\n",
      "G loss: 8.504276275634766\n",
      "Iteration 101\n",
      "D loss: -42.75652313232422\n",
      "GP: 7.204275131225586\n",
      "Gradient norm: 1.1905603408813477\n",
      "G loss: 9.017411231994629\n",
      "Iteration 151\n",
      "D loss: -42.76406478881836\n",
      "GP: 6.912550926208496\n",
      "Gradient norm: 1.1992024183273315\n",
      "G loss: 8.830921173095703\n",
      "Iteration 201\n",
      "D loss: -43.080631256103516\n",
      "GP: 6.485989570617676\n",
      "Gradient norm: 1.1670202016830444\n",
      "G loss: 9.06344985961914\n",
      "Iteration 251\n",
      "D loss: -45.80678176879883\n",
      "GP: 5.450994968414307\n",
      "Gradient norm: 1.150780439376831\n",
      "G loss: 9.36077880859375\n",
      "Iteration 301\n",
      "D loss: -44.579505920410156\n",
      "GP: 8.869486808776855\n",
      "Gradient norm: 1.206898808479309\n",
      "G loss: 8.72765827178955\n",
      "Iteration 351\n",
      "D loss: -43.65890121459961\n",
      "GP: 6.682314872741699\n",
      "Gradient norm: 1.1862378120422363\n",
      "G loss: 9.538826942443848\n",
      "\n",
      "Epoch 188\n",
      "Iteration 1\n",
      "D loss: -44.141666412353516\n",
      "GP: 7.301181793212891\n",
      "Gradient norm: 1.1889607906341553\n",
      "G loss: 10.692811965942383\n",
      "Iteration 51\n",
      "D loss: -43.2603874206543\n",
      "GP: 5.549471855163574\n",
      "Gradient norm: 1.1360273361206055\n",
      "G loss: 10.81427001953125\n",
      "Iteration 101\n",
      "D loss: -41.4799919128418\n",
      "GP: 6.826868534088135\n",
      "Gradient norm: 1.1882890462875366\n",
      "G loss: 9.798418998718262\n",
      "Iteration 151\n",
      "D loss: -44.75867462158203\n",
      "GP: 7.025816440582275\n",
      "Gradient norm: 1.1672061681747437\n",
      "G loss: 7.7414116859436035\n",
      "Iteration 201\n",
      "D loss: -42.777774810791016\n",
      "GP: 5.202197074890137\n",
      "Gradient norm: 1.1491321325302124\n",
      "G loss: 9.262594223022461\n",
      "Iteration 251\n",
      "D loss: -41.08348846435547\n",
      "GP: 6.344552993774414\n",
      "Gradient norm: 1.174017310142517\n",
      "G loss: 9.138574600219727\n",
      "Iteration 301\n",
      "D loss: -41.28114318847656\n",
      "GP: 6.284921646118164\n",
      "Gradient norm: 1.1652276515960693\n",
      "G loss: 8.836989402770996\n",
      "Iteration 351\n",
      "D loss: -42.99508285522461\n",
      "GP: 6.581086158752441\n",
      "Gradient norm: 1.1756095886230469\n",
      "G loss: 9.329449653625488\n",
      "\n",
      "Epoch 189\n",
      "Iteration 1\n",
      "D loss: -41.85957717895508\n",
      "GP: 7.219546318054199\n",
      "Gradient norm: 1.194124698638916\n",
      "G loss: 7.879046440124512\n",
      "Iteration 51\n",
      "D loss: -44.884002685546875\n",
      "GP: 8.160125732421875\n",
      "Gradient norm: 1.1926829814910889\n",
      "G loss: 8.823166847229004\n",
      "Iteration 101\n",
      "D loss: -41.23550796508789\n",
      "GP: 7.207828044891357\n",
      "Gradient norm: 1.1875280141830444\n",
      "G loss: 8.021878242492676\n",
      "Iteration 151\n",
      "D loss: -43.27141571044922\n",
      "GP: 8.661040306091309\n",
      "Gradient norm: 1.2130993604660034\n",
      "G loss: 7.52675724029541\n",
      "Iteration 201\n",
      "D loss: -43.481868743896484\n",
      "GP: 6.649899482727051\n",
      "Gradient norm: 1.1796369552612305\n",
      "G loss: 8.58769416809082\n",
      "Iteration 251\n",
      "D loss: -41.23052978515625\n",
      "GP: 8.540677070617676\n",
      "Gradient norm: 1.206673264503479\n",
      "G loss: 8.824832916259766\n",
      "Iteration 301\n",
      "D loss: -43.29365539550781\n",
      "GP: 6.110563278198242\n",
      "Gradient norm: 1.163987636566162\n",
      "G loss: 9.70872974395752\n",
      "Iteration 351\n",
      "D loss: -43.7031135559082\n",
      "GP: 7.350882530212402\n",
      "Gradient norm: 1.1962847709655762\n",
      "G loss: 9.462696075439453\n",
      "\n",
      "Epoch 190\n",
      "Iteration 1\n",
      "D loss: -43.120140075683594\n",
      "GP: 7.003135681152344\n",
      "Gradient norm: 1.1763328313827515\n",
      "G loss: 9.07689094543457\n",
      "Iteration 51\n",
      "D loss: -43.546966552734375\n",
      "GP: 6.888704299926758\n",
      "Gradient norm: 1.1838918924331665\n",
      "G loss: 10.143219947814941\n",
      "Iteration 101\n",
      "D loss: -43.242252349853516\n",
      "GP: 6.185271739959717\n",
      "Gradient norm: 1.1732912063598633\n",
      "G loss: 9.514985084533691\n",
      "Iteration 151\n",
      "D loss: -43.10543441772461\n",
      "GP: 5.566661834716797\n",
      "Gradient norm: 1.1539909839630127\n",
      "G loss: 7.906750679016113\n",
      "Iteration 201\n",
      "D loss: -43.928245544433594\n",
      "GP: 5.400251388549805\n",
      "Gradient norm: 1.1559048891067505\n",
      "G loss: 10.155686378479004\n",
      "Iteration 251\n",
      "D loss: -41.02900314331055\n",
      "GP: 6.484544277191162\n",
      "Gradient norm: 1.1654597520828247\n",
      "G loss: 9.40278434753418\n",
      "Iteration 301\n",
      "D loss: -41.06470489501953\n",
      "GP: 7.402205467224121\n",
      "Gradient norm: 1.1756411790847778\n",
      "G loss: 9.24535083770752\n",
      "Iteration 351\n",
      "D loss: -40.90223693847656\n",
      "GP: 6.001059532165527\n",
      "Gradient norm: 1.1631433963775635\n",
      "G loss: 9.315954208374023\n",
      "\n",
      "Epoch 191\n",
      "Iteration 1\n",
      "D loss: -44.652931213378906\n",
      "GP: 9.601778030395508\n",
      "Gradient norm: 1.2276325225830078\n",
      "G loss: 8.823050498962402\n",
      "Iteration 51\n",
      "D loss: -43.59352111816406\n",
      "GP: 6.920567035675049\n",
      "Gradient norm: 1.1926008462905884\n",
      "G loss: 8.646446228027344\n",
      "Iteration 101\n",
      "D loss: -42.991302490234375\n",
      "GP: 6.6524457931518555\n",
      "Gradient norm: 1.181883454322815\n",
      "G loss: 9.389206886291504\n",
      "Iteration 151\n",
      "D loss: -42.65577697753906\n",
      "GP: 5.663228988647461\n",
      "Gradient norm: 1.173615574836731\n",
      "G loss: 9.55544376373291\n",
      "Iteration 201\n",
      "D loss: -42.46681213378906\n",
      "GP: 6.990635395050049\n",
      "Gradient norm: 1.1932905912399292\n",
      "G loss: 9.37864875793457\n",
      "Iteration 251\n",
      "D loss: -43.81072998046875\n",
      "GP: 5.207563400268555\n",
      "Gradient norm: 1.1382420063018799\n",
      "G loss: 9.63195514678955\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 301\n",
      "D loss: -44.788116455078125\n",
      "GP: 6.108865737915039\n",
      "Gradient norm: 1.1669994592666626\n",
      "G loss: 8.552721977233887\n",
      "Iteration 351\n",
      "D loss: -41.09691619873047\n",
      "GP: 8.175530433654785\n",
      "Gradient norm: 1.203412652015686\n",
      "G loss: 9.76253604888916\n",
      "\n",
      "Epoch 192\n",
      "Iteration 1\n",
      "D loss: -43.577491760253906\n",
      "GP: 9.391457557678223\n",
      "Gradient norm: 1.2188140153884888\n",
      "G loss: 10.196751594543457\n",
      "Iteration 51\n",
      "D loss: -41.57230758666992\n",
      "GP: 6.821496486663818\n",
      "Gradient norm: 1.182371973991394\n",
      "G loss: 9.715232849121094\n",
      "Iteration 101\n",
      "D loss: -43.896392822265625\n",
      "GP: 7.359326362609863\n",
      "Gradient norm: 1.169978141784668\n",
      "G loss: 8.87881851196289\n",
      "Iteration 151\n",
      "D loss: -41.3979377746582\n",
      "GP: 5.810047149658203\n",
      "Gradient norm: 1.1655176877975464\n",
      "G loss: 9.944228172302246\n",
      "Iteration 201\n",
      "D loss: -42.21847915649414\n",
      "GP: 8.580432891845703\n",
      "Gradient norm: 1.2187044620513916\n",
      "G loss: 9.50688362121582\n",
      "Iteration 251\n",
      "D loss: -41.95868682861328\n",
      "GP: 6.024553298950195\n",
      "Gradient norm: 1.168769121170044\n",
      "G loss: 8.150641441345215\n",
      "Iteration 301\n",
      "D loss: -41.53273391723633\n",
      "GP: 4.849330902099609\n",
      "Gradient norm: 1.1368926763534546\n",
      "G loss: 7.822940349578857\n",
      "Iteration 351\n",
      "D loss: -43.68389129638672\n",
      "GP: 5.8907084465026855\n",
      "Gradient norm: 1.1595948934555054\n",
      "G loss: 8.797995567321777\n",
      "\n",
      "Epoch 193\n",
      "Iteration 1\n",
      "D loss: -43.15126037597656\n",
      "GP: 7.256502151489258\n",
      "Gradient norm: 1.188553810119629\n",
      "G loss: 9.028904914855957\n",
      "Iteration 51\n",
      "D loss: -43.50149154663086\n",
      "GP: 7.410935401916504\n",
      "Gradient norm: 1.1944952011108398\n",
      "G loss: 9.426386833190918\n",
      "Iteration 101\n",
      "D loss: -43.75135040283203\n",
      "GP: 7.452729225158691\n",
      "Gradient norm: 1.1922070980072021\n",
      "G loss: 9.385208129882812\n",
      "Iteration 151\n",
      "D loss: -43.94652557373047\n",
      "GP: 5.466189861297607\n",
      "Gradient norm: 1.1643524169921875\n",
      "G loss: 8.844746589660645\n",
      "Iteration 201\n",
      "D loss: -42.097469329833984\n",
      "GP: 6.586681365966797\n",
      "Gradient norm: 1.1854819059371948\n",
      "G loss: 9.694476127624512\n",
      "Iteration 251\n",
      "D loss: -44.89188003540039\n",
      "GP: 6.8130011558532715\n",
      "Gradient norm: 1.1704716682434082\n",
      "G loss: 10.43349838256836\n",
      "Iteration 301\n",
      "D loss: -42.827659606933594\n",
      "GP: 6.583540916442871\n",
      "Gradient norm: 1.176365613937378\n",
      "G loss: 7.7492451667785645\n",
      "Iteration 351\n",
      "D loss: -44.031288146972656\n",
      "GP: 6.863608360290527\n",
      "Gradient norm: 1.190402865409851\n",
      "G loss: 9.51481819152832\n",
      "\n",
      "Epoch 194\n",
      "Iteration 1\n",
      "D loss: -42.84893035888672\n",
      "GP: 7.918516159057617\n",
      "Gradient norm: 1.1849271059036255\n",
      "G loss: 9.524282455444336\n",
      "Iteration 51\n",
      "D loss: -41.917728424072266\n",
      "GP: 7.560369968414307\n",
      "Gradient norm: 1.1979213953018188\n",
      "G loss: 8.967881202697754\n",
      "Iteration 101\n",
      "D loss: -43.443138122558594\n",
      "GP: 8.53381061553955\n",
      "Gradient norm: 1.2046326398849487\n",
      "G loss: 8.875910758972168\n",
      "Iteration 151\n",
      "D loss: -45.75271987915039\n",
      "GP: 7.62300443649292\n",
      "Gradient norm: 1.1943598985671997\n",
      "G loss: 7.585576057434082\n",
      "Iteration 201\n",
      "D loss: -43.41740417480469\n",
      "GP: 7.484734058380127\n",
      "Gradient norm: 1.1979825496673584\n",
      "G loss: 8.383700370788574\n",
      "Iteration 251\n",
      "D loss: -44.56224060058594\n",
      "GP: 7.338947772979736\n",
      "Gradient norm: 1.1902042627334595\n",
      "G loss: 8.223470687866211\n",
      "Iteration 301\n",
      "D loss: -42.163917541503906\n",
      "GP: 6.171178340911865\n",
      "Gradient norm: 1.1578558683395386\n",
      "G loss: 8.403913497924805\n",
      "Iteration 351\n",
      "D loss: -40.95304870605469\n",
      "GP: 7.559894561767578\n",
      "Gradient norm: 1.1872755289077759\n",
      "G loss: 8.162373542785645\n",
      "\n",
      "Epoch 195\n",
      "Iteration 1\n",
      "D loss: -43.14620590209961\n",
      "GP: 7.841590881347656\n",
      "Gradient norm: 1.1938190460205078\n",
      "G loss: 9.017304420471191\n",
      "Iteration 51\n",
      "D loss: -41.72331237792969\n",
      "GP: 5.353364944458008\n",
      "Gradient norm: 1.1368603706359863\n",
      "G loss: 8.99985122680664\n",
      "Iteration 101\n",
      "D loss: -42.98664855957031\n",
      "GP: 7.40084171295166\n",
      "Gradient norm: 1.213661789894104\n",
      "G loss: 8.651965141296387\n",
      "Iteration 151\n",
      "D loss: -40.71881866455078\n",
      "GP: 8.758498191833496\n",
      "Gradient norm: 1.2104191780090332\n",
      "G loss: 9.827770233154297\n",
      "Iteration 201\n",
      "D loss: -41.96676254272461\n",
      "GP: 6.326504707336426\n",
      "Gradient norm: 1.1758348941802979\n",
      "G loss: 8.539630889892578\n",
      "Iteration 251\n",
      "D loss: -44.25929260253906\n",
      "GP: 7.640876770019531\n",
      "Gradient norm: 1.1962441205978394\n",
      "G loss: 9.403108596801758\n",
      "Iteration 301\n",
      "D loss: -43.35224533081055\n",
      "GP: 7.425727844238281\n",
      "Gradient norm: 1.1957443952560425\n",
      "G loss: 9.524580955505371\n",
      "Iteration 351\n",
      "D loss: -41.3897819519043\n",
      "GP: 6.8941121101379395\n",
      "Gradient norm: 1.167493462562561\n",
      "G loss: 9.148277282714844\n",
      "\n",
      "Epoch 196\n",
      "Iteration 1\n",
      "D loss: -43.56470489501953\n",
      "GP: 6.407346725463867\n",
      "Gradient norm: 1.1688876152038574\n",
      "G loss: 9.258116722106934\n",
      "Iteration 51\n",
      "D loss: -42.12793731689453\n",
      "GP: 8.01520824432373\n",
      "Gradient norm: 1.2110754251480103\n",
      "G loss: 8.800494194030762\n",
      "Iteration 101\n",
      "D loss: -41.0638427734375\n",
      "GP: 9.006549835205078\n",
      "Gradient norm: 1.230057716369629\n",
      "G loss: 7.611472129821777\n",
      "Iteration 151\n",
      "D loss: -45.577354431152344\n",
      "GP: 6.853364944458008\n",
      "Gradient norm: 1.1844234466552734\n",
      "G loss: 8.233477592468262\n",
      "Iteration 201\n",
      "D loss: -44.16339874267578\n",
      "GP: 6.10553503036499\n",
      "Gradient norm: 1.1712886095046997\n",
      "G loss: 8.90079402923584\n",
      "Iteration 251\n",
      "D loss: -44.72648239135742\n",
      "GP: 6.7733001708984375\n",
      "Gradient norm: 1.1812494993209839\n",
      "G loss: 9.080164909362793\n",
      "Iteration 301\n",
      "D loss: -44.535037994384766\n",
      "GP: 6.3259148597717285\n",
      "Gradient norm: 1.1839267015457153\n",
      "G loss: 9.116026878356934\n",
      "Iteration 351\n",
      "D loss: -44.31391525268555\n",
      "GP: 6.316051483154297\n",
      "Gradient norm: 1.1724501848220825\n",
      "G loss: 9.054933547973633\n",
      "\n",
      "Epoch 197\n",
      "Iteration 1\n",
      "D loss: -43.28543472290039\n",
      "GP: 5.954428672790527\n",
      "Gradient norm: 1.1656215190887451\n",
      "G loss: 8.10067081451416\n",
      "Iteration 51\n",
      "D loss: -43.66897964477539\n",
      "GP: 5.996647834777832\n",
      "Gradient norm: 1.1667741537094116\n",
      "G loss: 8.487911224365234\n",
      "Iteration 101\n",
      "D loss: -44.64642333984375\n",
      "GP: 7.076024055480957\n",
      "Gradient norm: 1.1932042837142944\n",
      "G loss: 7.795382499694824\n",
      "Iteration 151\n",
      "D loss: -42.06477355957031\n",
      "GP: 4.982294082641602\n",
      "Gradient norm: 1.1344661712646484\n",
      "G loss: 7.699120044708252\n",
      "Iteration 201\n",
      "D loss: -42.79438018798828\n",
      "GP: 5.54180383682251\n",
      "Gradient norm: 1.1431586742401123\n",
      "G loss: 8.654925346374512\n",
      "Iteration 251\n",
      "D loss: -41.63217544555664\n",
      "GP: 7.250190734863281\n",
      "Gradient norm: 1.1849907636642456\n",
      "G loss: 7.801436901092529\n",
      "Iteration 301\n",
      "D loss: -45.5833740234375\n",
      "GP: 6.516850471496582\n",
      "Gradient norm: 1.1795783042907715\n",
      "G loss: 8.649933815002441\n",
      "Iteration 351\n",
      "D loss: -43.35370635986328\n",
      "GP: 6.623085975646973\n",
      "Gradient norm: 1.1562215089797974\n",
      "G loss: 8.049978256225586\n",
      "\n",
      "Epoch 198\n",
      "Iteration 1\n",
      "D loss: -41.36333465576172\n",
      "GP: 7.244420528411865\n",
      "Gradient norm: 1.185979962348938\n",
      "G loss: 8.508999824523926\n",
      "Iteration 51\n",
      "D loss: -43.760986328125\n",
      "GP: 8.77998161315918\n",
      "Gradient norm: 1.2028591632843018\n",
      "G loss: 7.238682270050049\n",
      "Iteration 101\n",
      "D loss: -43.07624816894531\n",
      "GP: 7.954131126403809\n",
      "Gradient norm: 1.2111009359359741\n",
      "G loss: 7.622976303100586\n",
      "Iteration 151\n",
      "D loss: -43.6915283203125\n",
      "GP: 9.706259727478027\n",
      "Gradient norm: 1.218423843383789\n",
      "G loss: 5.88938045501709\n",
      "Iteration 201\n",
      "D loss: -44.45494842529297\n",
      "GP: 7.118249416351318\n",
      "Gradient norm: 1.1837084293365479\n",
      "G loss: 7.161320686340332\n",
      "Iteration 251\n",
      "D loss: -41.336456298828125\n",
      "GP: 6.290414333343506\n",
      "Gradient norm: 1.1786617040634155\n",
      "G loss: 7.693205833435059\n",
      "Iteration 301\n",
      "D loss: -42.20594024658203\n",
      "GP: 6.553277492523193\n",
      "Gradient norm: 1.1782116889953613\n",
      "G loss: 7.486665725708008\n",
      "Iteration 351\n",
      "D loss: -42.09401321411133\n",
      "GP: 8.942728996276855\n",
      "Gradient norm: 1.2088642120361328\n",
      "G loss: 7.5225348472595215\n",
      "\n",
      "Epoch 199\n",
      "Iteration 1\n",
      "D loss: -44.49095153808594\n",
      "GP: 6.574723243713379\n",
      "Gradient norm: 1.1653027534484863\n",
      "G loss: 7.470839023590088\n",
      "Iteration 51\n",
      "D loss: -43.463172912597656\n",
      "GP: 9.185018539428711\n",
      "Gradient norm: 1.2263232469558716\n",
      "G loss: 6.1105146408081055\n",
      "Iteration 101\n",
      "D loss: -43.34086990356445\n",
      "GP: 6.511584281921387\n",
      "Gradient norm: 1.1616387367248535\n",
      "G loss: 8.430428504943848\n",
      "Iteration 151\n",
      "D loss: -41.00419998168945\n",
      "GP: 7.6748199462890625\n",
      "Gradient norm: 1.1910033226013184\n",
      "G loss: 8.995086669921875\n",
      "Iteration 201\n",
      "D loss: -42.72496032714844\n",
      "GP: 7.938957691192627\n",
      "Gradient norm: 1.2092459201812744\n",
      "G loss: 6.354642868041992\n",
      "Iteration 251\n",
      "D loss: -43.13121795654297\n",
      "GP: 6.955105781555176\n",
      "Gradient norm: 1.181745171546936\n",
      "G loss: 7.948094367980957\n",
      "Iteration 301\n",
      "D loss: -44.161224365234375\n",
      "GP: 5.230016708374023\n",
      "Gradient norm: 1.1581929922103882\n",
      "G loss: 6.389375686645508\n",
      "Iteration 351\n",
      "D loss: -43.63623809814453\n",
      "GP: 7.52701473236084\n",
      "Gradient norm: 1.1849583387374878\n",
      "G loss: 7.212804794311523\n",
      "\n",
      "Epoch 200\n",
      "Iteration 1\n",
      "D loss: -42.999107360839844\n",
      "GP: 5.361405372619629\n",
      "Gradient norm: 1.1438575983047485\n",
      "G loss: 7.3395586013793945\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 51\n",
      "D loss: -41.405338287353516\n",
      "GP: 5.475895881652832\n",
      "Gradient norm: 1.1509943008422852\n",
      "G loss: 6.005695819854736\n",
      "Iteration 101\n",
      "D loss: -45.147621154785156\n",
      "GP: 6.041831970214844\n",
      "Gradient norm: 1.1706360578536987\n",
      "G loss: 5.422173500061035\n",
      "Iteration 151\n",
      "D loss: -44.472740173339844\n",
      "GP: 6.397027015686035\n",
      "Gradient norm: 1.1636440753936768\n",
      "G loss: 7.280004024505615\n",
      "Iteration 201\n",
      "D loss: -43.698570251464844\n",
      "GP: 7.429328441619873\n",
      "Gradient norm: 1.1973910331726074\n",
      "G loss: 6.554208278656006\n",
      "Iteration 251\n",
      "D loss: -42.116947174072266\n",
      "GP: 6.426486968994141\n",
      "Gradient norm: 1.1756590604782104\n",
      "G loss: 8.071210861206055\n",
      "Iteration 301\n",
      "D loss: -42.62211608886719\n",
      "GP: 6.049298286437988\n",
      "Gradient norm: 1.1547092199325562\n",
      "G loss: 6.577395915985107\n",
      "Iteration 351\n",
      "D loss: -44.50833511352539\n",
      "GP: 7.447003364562988\n",
      "Gradient norm: 1.1877702474594116\n",
      "G loss: 7.422663688659668\n",
      "\n",
      "Epoch 201\n",
      "Iteration 1\n",
      "D loss: -42.456268310546875\n",
      "GP: 6.98470401763916\n",
      "Gradient norm: 1.201156735420227\n",
      "G loss: 6.8801493644714355\n",
      "Iteration 51\n",
      "D loss: -42.898563385009766\n",
      "GP: 7.501118183135986\n",
      "Gradient norm: 1.1883795261383057\n",
      "G loss: 5.832491397857666\n",
      "Iteration 101\n",
      "D loss: -42.40728759765625\n",
      "GP: 7.032349109649658\n",
      "Gradient norm: 1.199387550354004\n",
      "G loss: 7.5376482009887695\n",
      "Iteration 151\n",
      "D loss: -40.49427032470703\n",
      "GP: 6.50456428527832\n",
      "Gradient norm: 1.180176854133606\n",
      "G loss: 7.190343379974365\n",
      "Iteration 201\n",
      "D loss: -43.054229736328125\n",
      "GP: 6.712274551391602\n",
      "Gradient norm: 1.1679548025131226\n",
      "G loss: 6.195566177368164\n",
      "Iteration 251\n",
      "D loss: -42.99336242675781\n",
      "GP: 7.0815205574035645\n",
      "Gradient norm: 1.1956923007965088\n",
      "G loss: 7.345252990722656\n",
      "Iteration 301\n",
      "D loss: -44.52328872680664\n",
      "GP: 7.366114139556885\n",
      "Gradient norm: 1.194590449333191\n",
      "G loss: 5.68131685256958\n",
      "Iteration 351\n",
      "D loss: -41.58473205566406\n",
      "GP: 4.699158191680908\n",
      "Gradient norm: 1.1446812152862549\n",
      "G loss: 7.680517673492432\n",
      "\n",
      "Epoch 202\n",
      "Iteration 1\n",
      "D loss: -39.76410675048828\n",
      "GP: 7.166971206665039\n",
      "Gradient norm: 1.1778862476348877\n",
      "G loss: 5.958683490753174\n",
      "Iteration 51\n",
      "D loss: -43.57148742675781\n",
      "GP: 6.2999396324157715\n",
      "Gradient norm: 1.1535820960998535\n",
      "G loss: 6.21530294418335\n",
      "Iteration 101\n",
      "D loss: -42.94889831542969\n",
      "GP: 4.845243453979492\n",
      "Gradient norm: 1.1450741291046143\n",
      "G loss: 6.969327926635742\n",
      "Iteration 151\n",
      "D loss: -42.924312591552734\n",
      "GP: 7.360500812530518\n",
      "Gradient norm: 1.1901767253875732\n",
      "G loss: 6.909073352813721\n",
      "Iteration 201\n",
      "D loss: -42.69786071777344\n",
      "GP: 6.322919845581055\n",
      "Gradient norm: 1.158500075340271\n",
      "G loss: 6.827584266662598\n",
      "Iteration 251\n",
      "D loss: -44.684043884277344\n",
      "GP: 9.989425659179688\n",
      "Gradient norm: 1.2484122514724731\n",
      "G loss: 8.369646072387695\n",
      "Iteration 301\n",
      "D loss: -46.41728973388672\n",
      "GP: 7.057894706726074\n",
      "Gradient norm: 1.2052818536758423\n",
      "G loss: 7.011840343475342\n",
      "Iteration 351\n",
      "D loss: -42.99431228637695\n",
      "GP: 5.765434265136719\n",
      "Gradient norm: 1.1502712965011597\n",
      "G loss: 6.0787434577941895\n",
      "\n",
      "Epoch 203\n",
      "Iteration 1\n",
      "D loss: -44.0045051574707\n",
      "GP: 7.554583549499512\n",
      "Gradient norm: 1.2016057968139648\n",
      "G loss: 6.157732963562012\n",
      "Iteration 51\n",
      "D loss: -43.331520080566406\n",
      "GP: 7.14626407623291\n",
      "Gradient norm: 1.1892324686050415\n",
      "G loss: 6.828657627105713\n",
      "Iteration 101\n",
      "D loss: -43.324615478515625\n",
      "GP: 5.780429840087891\n",
      "Gradient norm: 1.1699515581130981\n",
      "G loss: 8.349739074707031\n",
      "Iteration 151\n",
      "D loss: -41.605960845947266\n",
      "GP: 7.292881488800049\n",
      "Gradient norm: 1.179927110671997\n",
      "G loss: 6.973559856414795\n",
      "Iteration 201\n",
      "D loss: -44.809326171875\n",
      "GP: 7.49506950378418\n",
      "Gradient norm: 1.195298194885254\n",
      "G loss: 6.451594352722168\n",
      "Iteration 251\n",
      "D loss: -43.32625198364258\n",
      "GP: 7.293988227844238\n",
      "Gradient norm: 1.1989961862564087\n",
      "G loss: 6.720852375030518\n",
      "Iteration 301\n",
      "D loss: -43.09452438354492\n",
      "GP: 7.773687839508057\n",
      "Gradient norm: 1.201911449432373\n",
      "G loss: 6.900926113128662\n",
      "Iteration 351\n",
      "D loss: -42.59934997558594\n",
      "GP: 7.322521209716797\n",
      "Gradient norm: 1.1961208581924438\n",
      "G loss: 7.492654323577881\n",
      "\n",
      "Epoch 204\n",
      "Iteration 1\n",
      "D loss: -44.02034378051758\n",
      "GP: 5.936532020568848\n",
      "Gradient norm: 1.165701150894165\n",
      "G loss: 7.38005256652832\n",
      "Iteration 51\n",
      "D loss: -41.01071548461914\n",
      "GP: 8.12956714630127\n",
      "Gradient norm: 1.2110779285430908\n",
      "G loss: 5.918052673339844\n",
      "Iteration 101\n",
      "D loss: -42.56428146362305\n",
      "GP: 7.576202869415283\n",
      "Gradient norm: 1.1934044361114502\n",
      "G loss: 6.491194248199463\n",
      "Iteration 151\n",
      "D loss: -44.447174072265625\n",
      "GP: 5.838765621185303\n",
      "Gradient norm: 1.1690993309020996\n",
      "G loss: 6.813676834106445\n",
      "Iteration 201\n",
      "D loss: -44.80034637451172\n",
      "GP: 6.897648811340332\n",
      "Gradient norm: 1.1730338335037231\n",
      "G loss: 6.804972171783447\n",
      "Iteration 251\n",
      "D loss: -43.014530181884766\n",
      "GP: 4.877209663391113\n",
      "Gradient norm: 1.1418431997299194\n",
      "G loss: 6.938570976257324\n",
      "Iteration 301\n",
      "D loss: -41.82966232299805\n",
      "GP: 7.780894756317139\n",
      "Gradient norm: 1.198638677597046\n",
      "G loss: 7.459070205688477\n",
      "Iteration 351\n",
      "D loss: -43.87709426879883\n",
      "GP: 6.46853494644165\n",
      "Gradient norm: 1.167966604232788\n",
      "G loss: 7.604765892028809\n",
      "\n",
      "Epoch 205\n",
      "Iteration 1\n",
      "D loss: -46.08360290527344\n",
      "GP: 6.394526481628418\n",
      "Gradient norm: 1.164259910583496\n",
      "G loss: 7.6190900802612305\n",
      "Iteration 51\n",
      "D loss: -43.2819709777832\n",
      "GP: 6.4183526039123535\n",
      "Gradient norm: 1.1727129220962524\n",
      "G loss: 7.103909969329834\n",
      "Iteration 101\n",
      "D loss: -42.62260818481445\n",
      "GP: 6.362129211425781\n",
      "Gradient norm: 1.1824299097061157\n",
      "G loss: 6.861915111541748\n",
      "Iteration 151\n",
      "D loss: -42.41537857055664\n",
      "GP: 7.434389591217041\n",
      "Gradient norm: 1.1995372772216797\n",
      "G loss: 7.137859344482422\n",
      "Iteration 201\n",
      "D loss: -43.72142791748047\n",
      "GP: 6.167924880981445\n",
      "Gradient norm: 1.1657118797302246\n",
      "G loss: 6.419661521911621\n",
      "Iteration 251\n",
      "D loss: -41.51384735107422\n",
      "GP: 5.2901763916015625\n",
      "Gradient norm: 1.1453663110733032\n",
      "G loss: 6.709249973297119\n",
      "Iteration 301\n",
      "D loss: -42.02842330932617\n",
      "GP: 6.558670997619629\n",
      "Gradient norm: 1.174492359161377\n",
      "G loss: 7.258026123046875\n",
      "Iteration 351\n",
      "D loss: -44.15574264526367\n",
      "GP: 8.696632385253906\n",
      "Gradient norm: 1.2091535329818726\n",
      "G loss: 7.272664546966553\n",
      "\n",
      "Epoch 206\n",
      "Iteration 1\n",
      "D loss: -41.99033737182617\n",
      "GP: 6.028964519500732\n",
      "Gradient norm: 1.1710418462753296\n",
      "G loss: 7.157748699188232\n",
      "Iteration 51\n",
      "D loss: -45.156856536865234\n",
      "GP: 5.042163848876953\n",
      "Gradient norm: 1.1462416648864746\n",
      "G loss: 8.03609561920166\n",
      "Iteration 101\n",
      "D loss: -43.56383514404297\n",
      "GP: 5.326595306396484\n",
      "Gradient norm: 1.1468746662139893\n",
      "G loss: 8.330032348632812\n",
      "Iteration 151\n",
      "D loss: -43.91728973388672\n",
      "GP: 6.2631635665893555\n",
      "Gradient norm: 1.1636722087860107\n",
      "G loss: 8.018510818481445\n",
      "Iteration 201\n",
      "D loss: -42.5841178894043\n",
      "GP: 7.751553535461426\n",
      "Gradient norm: 1.1926535367965698\n",
      "G loss: 7.241312503814697\n",
      "Iteration 251\n",
      "D loss: -43.48017883300781\n",
      "GP: 6.138334274291992\n",
      "Gradient norm: 1.1652864217758179\n",
      "G loss: 7.561425685882568\n",
      "Iteration 301\n",
      "D loss: -45.16469192504883\n",
      "GP: 6.562243461608887\n",
      "Gradient norm: 1.1726828813552856\n",
      "G loss: 8.402024269104004\n",
      "Iteration 351\n",
      "D loss: -42.573097229003906\n",
      "GP: 7.117546081542969\n",
      "Gradient norm: 1.2157890796661377\n",
      "G loss: 8.888809204101562\n",
      "\n",
      "Epoch 207\n",
      "Iteration 1\n",
      "D loss: -45.26308822631836\n",
      "GP: 6.040783882141113\n",
      "Gradient norm: 1.143600583076477\n",
      "G loss: 7.933850288391113\n",
      "Iteration 51\n",
      "D loss: -44.195369720458984\n",
      "GP: 6.386270999908447\n",
      "Gradient norm: 1.1734050512313843\n",
      "G loss: 8.600992202758789\n",
      "Iteration 101\n",
      "D loss: -39.112770080566406\n",
      "GP: 8.252235412597656\n",
      "Gradient norm: 1.1915124654769897\n",
      "G loss: 8.711756706237793\n",
      "Iteration 151\n",
      "D loss: -40.581241607666016\n",
      "GP: 8.145861625671387\n",
      "Gradient norm: 1.1961873769760132\n",
      "G loss: 8.257305145263672\n",
      "Iteration 201\n",
      "D loss: -45.345733642578125\n",
      "GP: 6.961241722106934\n",
      "Gradient norm: 1.184653401374817\n",
      "G loss: 9.827282905578613\n",
      "Iteration 251\n",
      "D loss: -42.15446853637695\n",
      "GP: 6.730500221252441\n",
      "Gradient norm: 1.1716936826705933\n",
      "G loss: 9.511054992675781\n",
      "Iteration 301\n",
      "D loss: -44.52888870239258\n",
      "GP: 5.9921417236328125\n",
      "Gradient norm: 1.1734097003936768\n",
      "G loss: 9.408514022827148\n",
      "Iteration 351\n",
      "D loss: -44.156044006347656\n",
      "GP: 10.16716480255127\n",
      "Gradient norm: 1.2201197147369385\n",
      "G loss: 9.362799644470215\n",
      "\n",
      "Epoch 208\n",
      "Iteration 1\n",
      "D loss: -44.145599365234375\n",
      "GP: 6.697225570678711\n",
      "Gradient norm: 1.1904385089874268\n",
      "G loss: 8.883992195129395\n",
      "Iteration 51\n",
      "D loss: -45.97749710083008\n",
      "GP: 6.489743232727051\n",
      "Gradient norm: 1.1743810176849365\n",
      "G loss: 8.80513858795166\n",
      "Iteration 101\n",
      "D loss: -43.94050598144531\n",
      "GP: 6.418829917907715\n",
      "Gradient norm: 1.1684067249298096\n",
      "G loss: 8.997349739074707\n",
      "Iteration 151\n",
      "D loss: -43.139122009277344\n",
      "GP: 7.012558937072754\n",
      "Gradient norm: 1.1767019033432007\n",
      "G loss: 7.936534404754639\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 201\n",
      "D loss: -41.96782302856445\n",
      "GP: 6.792423725128174\n",
      "Gradient norm: 1.1671630144119263\n",
      "G loss: 8.303511619567871\n",
      "Iteration 251\n",
      "D loss: -44.2451286315918\n",
      "GP: 8.761124610900879\n",
      "Gradient norm: 1.2110406160354614\n",
      "G loss: 7.662006378173828\n",
      "Iteration 301\n",
      "D loss: -45.87705993652344\n",
      "GP: 7.502382278442383\n",
      "Gradient norm: 1.2067968845367432\n",
      "G loss: 9.973821640014648\n",
      "Iteration 351\n",
      "D loss: -43.243690490722656\n",
      "GP: 8.782709121704102\n",
      "Gradient norm: 1.2066984176635742\n",
      "G loss: 8.186301231384277\n",
      "\n",
      "Epoch 209\n",
      "Iteration 1\n",
      "D loss: -44.4549560546875\n",
      "GP: 8.007179260253906\n",
      "Gradient norm: 1.1999348402023315\n",
      "G loss: 9.531476974487305\n",
      "Iteration 51\n",
      "D loss: -41.402587890625\n",
      "GP: 7.606149196624756\n",
      "Gradient norm: 1.192205786705017\n",
      "G loss: 9.213457107543945\n",
      "Iteration 101\n",
      "D loss: -43.5142822265625\n",
      "GP: 8.0061674118042\n",
      "Gradient norm: 1.2124791145324707\n",
      "G loss: 9.282898902893066\n",
      "Iteration 151\n",
      "D loss: -45.9129753112793\n",
      "GP: 8.304463386535645\n",
      "Gradient norm: 1.2083046436309814\n",
      "G loss: 9.48948860168457\n",
      "Iteration 201\n",
      "D loss: -42.455650329589844\n",
      "GP: 5.205855846405029\n",
      "Gradient norm: 1.1623916625976562\n",
      "G loss: 9.139615058898926\n",
      "Iteration 251\n",
      "D loss: -43.52921676635742\n",
      "GP: 7.435715198516846\n",
      "Gradient norm: 1.1955580711364746\n",
      "G loss: 9.890115737915039\n",
      "Iteration 301\n",
      "D loss: -43.11674880981445\n",
      "GP: 6.599713325500488\n",
      "Gradient norm: 1.1796469688415527\n",
      "G loss: 8.302440643310547\n",
      "Iteration 351\n",
      "D loss: -43.04293441772461\n",
      "GP: 7.0099077224731445\n",
      "Gradient norm: 1.1937063932418823\n",
      "G loss: 8.76027774810791\n",
      "\n",
      "Epoch 210\n",
      "Iteration 1\n",
      "D loss: -44.842254638671875\n",
      "GP: 6.068558216094971\n",
      "Gradient norm: 1.1532834768295288\n",
      "G loss: 7.326765060424805\n",
      "Iteration 51\n",
      "D loss: -42.74162673950195\n",
      "GP: 6.3863844871521\n",
      "Gradient norm: 1.1607329845428467\n",
      "G loss: 8.317707061767578\n",
      "Iteration 101\n",
      "D loss: -42.266029357910156\n",
      "GP: 4.464831829071045\n",
      "Gradient norm: 1.1263936758041382\n",
      "G loss: 9.104958534240723\n",
      "Iteration 151\n",
      "D loss: -41.23735809326172\n",
      "GP: 6.436528205871582\n",
      "Gradient norm: 1.1656181812286377\n",
      "G loss: 8.321107864379883\n",
      "Iteration 201\n",
      "D loss: -45.16685485839844\n",
      "GP: 5.589970588684082\n",
      "Gradient norm: 1.1681584119796753\n",
      "G loss: 8.784181594848633\n",
      "Iteration 251\n",
      "D loss: -41.09749221801758\n",
      "GP: 6.613361835479736\n",
      "Gradient norm: 1.1718655824661255\n",
      "G loss: 10.409103393554688\n",
      "Iteration 301\n",
      "D loss: -44.082515716552734\n",
      "GP: 6.767974853515625\n",
      "Gradient norm: 1.1807260513305664\n",
      "G loss: 10.14854621887207\n",
      "Iteration 351\n",
      "D loss: -44.71304702758789\n",
      "GP: 7.445172309875488\n",
      "Gradient norm: 1.202047348022461\n",
      "G loss: 9.0094633102417\n",
      "\n",
      "Epoch 211\n",
      "Iteration 1\n",
      "D loss: -42.38227844238281\n",
      "GP: 8.781523704528809\n",
      "Gradient norm: 1.2209477424621582\n",
      "G loss: 7.45196008682251\n",
      "Iteration 51\n",
      "D loss: -44.1566276550293\n",
      "GP: 7.038214206695557\n",
      "Gradient norm: 1.1912693977355957\n",
      "G loss: 7.71973180770874\n",
      "Iteration 101\n",
      "D loss: -43.559669494628906\n",
      "GP: 5.927789688110352\n",
      "Gradient norm: 1.16507887840271\n",
      "G loss: 7.840574741363525\n",
      "Iteration 151\n",
      "D loss: -43.169227600097656\n",
      "GP: 6.695603847503662\n",
      "Gradient norm: 1.1654309034347534\n",
      "G loss: 8.53555679321289\n",
      "Iteration 201\n",
      "D loss: -43.19013214111328\n",
      "GP: 7.085856914520264\n",
      "Gradient norm: 1.1758992671966553\n",
      "G loss: 8.860269546508789\n",
      "Iteration 251\n",
      "D loss: -44.364803314208984\n",
      "GP: 6.035823822021484\n",
      "Gradient norm: 1.1663649082183838\n",
      "G loss: 8.620387077331543\n",
      "Iteration 301\n",
      "D loss: -41.767433166503906\n",
      "GP: 8.218350410461426\n",
      "Gradient norm: 1.2045879364013672\n",
      "G loss: 10.091362953186035\n",
      "Iteration 351\n",
      "D loss: -42.66878128051758\n",
      "GP: 4.779366970062256\n",
      "Gradient norm: 1.1201127767562866\n",
      "G loss: 10.272065162658691\n",
      "\n",
      "Epoch 212\n",
      "Iteration 1\n",
      "D loss: -45.1123161315918\n",
      "GP: 5.852086544036865\n",
      "Gradient norm: 1.163329839706421\n",
      "G loss: 9.64826774597168\n",
      "Iteration 51\n",
      "D loss: -44.957950592041016\n",
      "GP: 6.7833428382873535\n",
      "Gradient norm: 1.1815602779388428\n",
      "G loss: 8.871288299560547\n",
      "Iteration 101\n",
      "D loss: -45.713584899902344\n",
      "GP: 5.331558704376221\n",
      "Gradient norm: 1.1436258554458618\n",
      "G loss: 8.059122085571289\n",
      "Iteration 151\n",
      "D loss: -43.693084716796875\n",
      "GP: 6.622474193572998\n",
      "Gradient norm: 1.1795374155044556\n",
      "G loss: 8.455702781677246\n",
      "Iteration 201\n",
      "D loss: -43.152320861816406\n",
      "GP: 6.357207298278809\n",
      "Gradient norm: 1.158762812614441\n",
      "G loss: 7.240287780761719\n",
      "Iteration 251\n",
      "D loss: -44.869632720947266\n",
      "GP: 5.7918500900268555\n",
      "Gradient norm: 1.1630650758743286\n",
      "G loss: 8.61910629272461\n",
      "Iteration 301\n",
      "D loss: -45.43260955810547\n",
      "GP: 6.16628885269165\n",
      "Gradient norm: 1.172532320022583\n",
      "G loss: 8.103691101074219\n",
      "Iteration 351\n",
      "D loss: -42.412418365478516\n",
      "GP: 7.827297210693359\n",
      "Gradient norm: 1.214223861694336\n",
      "G loss: 8.170365333557129\n",
      "\n",
      "Epoch 213\n",
      "Iteration 1\n",
      "D loss: -43.735984802246094\n",
      "GP: 7.67279577255249\n",
      "Gradient norm: 1.1735849380493164\n",
      "G loss: 9.027305603027344\n",
      "Iteration 51\n",
      "D loss: -42.40227127075195\n",
      "GP: 7.103795051574707\n",
      "Gradient norm: 1.168033480644226\n",
      "G loss: 8.819812774658203\n",
      "Iteration 101\n",
      "D loss: -45.24208450317383\n",
      "GP: 7.606521129608154\n",
      "Gradient norm: 1.192592740058899\n",
      "G loss: 9.913232803344727\n",
      "Iteration 151\n",
      "D loss: -39.95652389526367\n",
      "GP: 9.231487274169922\n",
      "Gradient norm: 1.224045991897583\n",
      "G loss: 8.915166854858398\n",
      "Iteration 201\n",
      "D loss: -42.87712478637695\n",
      "GP: 7.910175323486328\n",
      "Gradient norm: 1.1928962469100952\n",
      "G loss: 8.812639236450195\n",
      "Iteration 251\n",
      "D loss: -44.47757339477539\n",
      "GP: 7.372386455535889\n",
      "Gradient norm: 1.1872522830963135\n",
      "G loss: 8.648820877075195\n",
      "Iteration 301\n",
      "D loss: -41.672760009765625\n",
      "GP: 6.811524391174316\n",
      "Gradient norm: 1.1849135160446167\n",
      "G loss: 8.508792877197266\n",
      "Iteration 351\n",
      "D loss: -43.56501770019531\n",
      "GP: 4.990015029907227\n",
      "Gradient norm: 1.1458353996276855\n",
      "G loss: 9.226105690002441\n",
      "\n",
      "Epoch 214\n",
      "Iteration 1\n",
      "D loss: -44.069393157958984\n",
      "GP: 6.0796613693237305\n",
      "Gradient norm: 1.1629477739334106\n",
      "G loss: 9.14217472076416\n",
      "Iteration 51\n",
      "D loss: -44.40433883666992\n",
      "GP: 8.107109069824219\n",
      "Gradient norm: 1.1849339008331299\n",
      "G loss: 10.460904121398926\n",
      "Iteration 101\n",
      "D loss: -39.94633483886719\n",
      "GP: 6.439872741699219\n",
      "Gradient norm: 1.1408170461654663\n",
      "G loss: 9.808905601501465\n",
      "Iteration 151\n",
      "D loss: -41.16484451293945\n",
      "GP: 7.5428571701049805\n",
      "Gradient norm: 1.192613124847412\n",
      "G loss: 10.146516799926758\n",
      "Iteration 201\n",
      "D loss: -45.45682144165039\n",
      "GP: 7.532997131347656\n",
      "Gradient norm: 1.1917686462402344\n",
      "G loss: 7.968618869781494\n",
      "Iteration 251\n",
      "D loss: -42.0317497253418\n",
      "GP: 6.341320991516113\n",
      "Gradient norm: 1.1650506258010864\n",
      "G loss: 8.018299102783203\n",
      "Iteration 301\n",
      "D loss: -43.04762268066406\n",
      "GP: 5.807542324066162\n",
      "Gradient norm: 1.158804178237915\n",
      "G loss: 8.454370498657227\n",
      "Iteration 351\n",
      "D loss: -42.9340934753418\n",
      "GP: 6.182556629180908\n",
      "Gradient norm: 1.157210111618042\n",
      "G loss: 9.05940055847168\n",
      "\n",
      "Epoch 215\n",
      "Iteration 1\n",
      "D loss: -41.28178405761719\n",
      "GP: 8.24170207977295\n",
      "Gradient norm: 1.1957145929336548\n",
      "G loss: 9.842092514038086\n",
      "Iteration 51\n",
      "D loss: -42.490997314453125\n",
      "GP: 7.157183647155762\n",
      "Gradient norm: 1.1818276643753052\n",
      "G loss: 10.347034454345703\n",
      "Iteration 101\n",
      "D loss: -44.07782745361328\n",
      "GP: 8.233085632324219\n",
      "Gradient norm: 1.200222373008728\n",
      "G loss: 10.440083503723145\n",
      "Iteration 151\n",
      "D loss: -41.415000915527344\n",
      "GP: 6.332981109619141\n",
      "Gradient norm: 1.1679596900939941\n",
      "G loss: 9.882523536682129\n",
      "Iteration 201\n",
      "D loss: -42.12041473388672\n",
      "GP: 6.736621379852295\n",
      "Gradient norm: 1.179566502571106\n",
      "G loss: 9.41950511932373\n",
      "Iteration 251\n",
      "D loss: -41.84560775756836\n",
      "GP: 7.615146160125732\n",
      "Gradient norm: 1.199673056602478\n",
      "G loss: 9.0591402053833\n",
      "Iteration 301\n",
      "D loss: -44.219871520996094\n",
      "GP: 7.879202365875244\n",
      "Gradient norm: 1.1967192888259888\n",
      "G loss: 8.2684326171875\n",
      "Iteration 351\n",
      "D loss: -44.30837631225586\n",
      "GP: 6.163735389709473\n",
      "Gradient norm: 1.161426067352295\n",
      "G loss: 8.107586860656738\n",
      "\n",
      "Epoch 216\n",
      "Iteration 1\n",
      "D loss: -43.42890548706055\n",
      "GP: 6.8993306159973145\n",
      "Gradient norm: 1.167754888534546\n",
      "G loss: 9.067922592163086\n",
      "Iteration 51\n",
      "D loss: -42.372005462646484\n",
      "GP: 5.842070579528809\n",
      "Gradient norm: 1.166710376739502\n",
      "G loss: 9.772971153259277\n",
      "Iteration 101\n",
      "D loss: -43.90245056152344\n",
      "GP: 5.690439224243164\n",
      "Gradient norm: 1.1583902835845947\n",
      "G loss: 9.63805103302002\n",
      "Iteration 151\n",
      "D loss: -43.09413528442383\n",
      "GP: 7.8734025955200195\n",
      "Gradient norm: 1.2062338590621948\n",
      "G loss: 10.006861686706543\n",
      "Iteration 201\n",
      "D loss: -45.04888153076172\n",
      "GP: 6.846436977386475\n",
      "Gradient norm: 1.1729893684387207\n",
      "G loss: 9.186419486999512\n",
      "Iteration 251\n",
      "D loss: -45.26040267944336\n",
      "GP: 6.700847148895264\n",
      "Gradient norm: 1.1831333637237549\n",
      "G loss: 9.903322219848633\n",
      "Iteration 301\n",
      "D loss: -43.403499603271484\n",
      "GP: 7.9751482009887695\n",
      "Gradient norm: 1.2008004188537598\n",
      "G loss: 10.93860912322998\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 351\n",
      "D loss: -42.95974349975586\n",
      "GP: 7.70949125289917\n",
      "Gradient norm: 1.198928713798523\n",
      "G loss: 10.157026290893555\n",
      "\n",
      "Epoch 217\n",
      "Iteration 1\n",
      "D loss: -44.29401779174805\n",
      "GP: 7.634294033050537\n",
      "Gradient norm: 1.1905995607376099\n",
      "G loss: 10.51445198059082\n",
      "Iteration 51\n",
      "D loss: -43.25041580200195\n",
      "GP: 7.093600273132324\n",
      "Gradient norm: 1.1756659746170044\n",
      "G loss: 9.628973960876465\n",
      "Iteration 101\n",
      "D loss: -43.178192138671875\n",
      "GP: 5.654748916625977\n",
      "Gradient norm: 1.1613543033599854\n",
      "G loss: 9.38713264465332\n",
      "Iteration 151\n",
      "D loss: -40.32961654663086\n",
      "GP: 5.483826637268066\n",
      "Gradient norm: 1.1577849388122559\n",
      "G loss: 9.182796478271484\n",
      "Iteration 201\n",
      "D loss: -43.123653411865234\n",
      "GP: 6.576610088348389\n",
      "Gradient norm: 1.1692641973495483\n",
      "G loss: 8.229747772216797\n",
      "Iteration 251\n",
      "D loss: -42.335689544677734\n",
      "GP: 7.959380149841309\n",
      "Gradient norm: 1.195358395576477\n",
      "G loss: 9.643720626831055\n",
      "Iteration 301\n",
      "D loss: -41.66983413696289\n",
      "GP: 7.127560615539551\n",
      "Gradient norm: 1.166658878326416\n",
      "G loss: 9.658371925354004\n",
      "Iteration 351\n",
      "D loss: -43.48354721069336\n",
      "GP: 7.619228363037109\n",
      "Gradient norm: 1.1823809146881104\n",
      "G loss: 9.808947563171387\n",
      "\n",
      "Epoch 218\n",
      "Iteration 1\n",
      "D loss: -42.80000305175781\n",
      "GP: 8.127809524536133\n",
      "Gradient norm: 1.2103773355484009\n",
      "G loss: 10.012974739074707\n",
      "Iteration 51\n",
      "D loss: -44.58172607421875\n",
      "GP: 7.402104377746582\n",
      "Gradient norm: 1.201623558998108\n",
      "G loss: 9.249739646911621\n",
      "Iteration 101\n",
      "D loss: -43.56376647949219\n",
      "GP: 7.1480937004089355\n",
      "Gradient norm: 1.1870087385177612\n",
      "G loss: 9.9787015914917\n",
      "Iteration 151\n",
      "D loss: -42.566490173339844\n",
      "GP: 7.413469314575195\n",
      "Gradient norm: 1.191637396812439\n",
      "G loss: 10.509632110595703\n",
      "Iteration 201\n",
      "D loss: -43.00754165649414\n",
      "GP: 7.904252052307129\n",
      "Gradient norm: 1.2062387466430664\n",
      "G loss: 10.979174613952637\n",
      "Iteration 251\n",
      "D loss: -42.32042694091797\n",
      "GP: 8.679245948791504\n",
      "Gradient norm: 1.2252353429794312\n",
      "G loss: 10.778799057006836\n",
      "Iteration 301\n",
      "D loss: -42.17802429199219\n",
      "GP: 6.602590560913086\n",
      "Gradient norm: 1.1781214475631714\n",
      "G loss: 9.959203720092773\n",
      "Iteration 351\n",
      "D loss: -45.05305862426758\n",
      "GP: 7.111179828643799\n",
      "Gradient norm: 1.189672589302063\n",
      "G loss: 11.466971397399902\n",
      "\n",
      "Epoch 219\n",
      "Iteration 1\n",
      "D loss: -44.2945556640625\n",
      "GP: 8.281728744506836\n",
      "Gradient norm: 1.2075586318969727\n",
      "G loss: 10.968358039855957\n",
      "Iteration 51\n",
      "D loss: -45.27180862426758\n",
      "GP: 7.606523036956787\n",
      "Gradient norm: 1.210260033607483\n",
      "G loss: 11.322895050048828\n",
      "Iteration 101\n",
      "D loss: -42.788211822509766\n",
      "GP: 7.099891662597656\n",
      "Gradient norm: 1.1851061582565308\n",
      "G loss: 10.665444374084473\n",
      "Iteration 151\n",
      "D loss: -43.501708984375\n",
      "GP: 6.361621856689453\n",
      "Gradient norm: 1.1709742546081543\n",
      "G loss: 10.016605377197266\n",
      "Iteration 201\n",
      "D loss: -40.673301696777344\n",
      "GP: 6.228342056274414\n",
      "Gradient norm: 1.1627522706985474\n",
      "G loss: 11.03132152557373\n",
      "Iteration 251\n",
      "D loss: -43.97411346435547\n",
      "GP: 6.311142921447754\n",
      "Gradient norm: 1.1645667552947998\n",
      "G loss: 10.16330623626709\n",
      "Iteration 301\n",
      "D loss: -43.37398147583008\n",
      "GP: 5.8306050300598145\n",
      "Gradient norm: 1.1457645893096924\n",
      "G loss: 11.640812873840332\n",
      "Iteration 351\n",
      "D loss: -47.02583694458008\n",
      "GP: 6.406936168670654\n",
      "Gradient norm: 1.191124677658081\n",
      "G loss: 9.79379940032959\n",
      "\n",
      "Epoch 220\n",
      "Iteration 1\n",
      "D loss: -42.27399444580078\n",
      "GP: 7.56771183013916\n",
      "Gradient norm: 1.1716351509094238\n",
      "G loss: 10.402326583862305\n",
      "Iteration 51\n",
      "D loss: -40.37736892700195\n",
      "GP: 7.1167402267456055\n",
      "Gradient norm: 1.1732832193374634\n",
      "G loss: 10.72118854522705\n",
      "Iteration 101\n",
      "D loss: -43.54751205444336\n",
      "GP: 5.8710761070251465\n",
      "Gradient norm: 1.1612244844436646\n",
      "G loss: 10.39262866973877\n",
      "Iteration 151\n",
      "D loss: -42.35056686401367\n",
      "GP: 6.0329389572143555\n",
      "Gradient norm: 1.1587189435958862\n",
      "G loss: 9.91286849975586\n",
      "Iteration 201\n",
      "D loss: -45.03157043457031\n",
      "GP: 5.929309844970703\n",
      "Gradient norm: 1.1549783945083618\n",
      "G loss: 11.490694046020508\n",
      "Iteration 251\n",
      "D loss: -43.22821044921875\n",
      "GP: 7.3926472663879395\n",
      "Gradient norm: 1.1890065670013428\n",
      "G loss: 11.558013916015625\n",
      "Iteration 301\n",
      "D loss: -43.006832122802734\n",
      "GP: 6.060873985290527\n",
      "Gradient norm: 1.1498242616653442\n",
      "G loss: 10.851682662963867\n",
      "Iteration 351\n",
      "D loss: -43.97385787963867\n",
      "GP: 7.621068477630615\n",
      "Gradient norm: 1.2016329765319824\n",
      "G loss: 9.851680755615234\n",
      "\n",
      "Epoch 221\n",
      "Iteration 1\n",
      "D loss: -44.786094665527344\n",
      "GP: 6.539846420288086\n",
      "Gradient norm: 1.1803758144378662\n",
      "G loss: 11.045896530151367\n",
      "Iteration 51\n",
      "D loss: -43.43935775756836\n",
      "GP: 5.670430660247803\n",
      "Gradient norm: 1.1605663299560547\n",
      "G loss: 10.207200050354004\n",
      "Iteration 101\n",
      "D loss: -42.185997009277344\n",
      "GP: 5.947103500366211\n",
      "Gradient norm: 1.157765507698059\n",
      "G loss: 12.758707046508789\n",
      "Iteration 151\n",
      "D loss: -41.21320724487305\n",
      "GP: 7.195532321929932\n",
      "Gradient norm: 1.196325421333313\n",
      "G loss: 12.382857322692871\n",
      "Iteration 201\n",
      "D loss: -43.625030517578125\n",
      "GP: 6.990170001983643\n",
      "Gradient norm: 1.179863452911377\n",
      "G loss: 9.86646556854248\n",
      "Iteration 251\n",
      "D loss: -42.239402770996094\n",
      "GP: 7.75869607925415\n",
      "Gradient norm: 1.174062728881836\n",
      "G loss: 10.937849044799805\n",
      "Iteration 301\n",
      "D loss: -41.35890197753906\n",
      "GP: 8.074206352233887\n",
      "Gradient norm: 1.2002098560333252\n",
      "G loss: 11.327398300170898\n",
      "Iteration 351\n",
      "D loss: -44.94727325439453\n",
      "GP: 7.827363967895508\n",
      "Gradient norm: 1.197250247001648\n",
      "G loss: 12.870277404785156\n",
      "\n",
      "Epoch 222\n",
      "Iteration 1\n",
      "D loss: -42.994510650634766\n",
      "GP: 8.91303539276123\n",
      "Gradient norm: 1.2204418182373047\n",
      "G loss: 12.663002014160156\n",
      "Iteration 51\n",
      "D loss: -43.97235870361328\n",
      "GP: 7.893370628356934\n",
      "Gradient norm: 1.2070196866989136\n",
      "G loss: 12.356491088867188\n",
      "Iteration 101\n",
      "D loss: -40.527198791503906\n",
      "GP: 7.782915115356445\n",
      "Gradient norm: 1.1924610137939453\n",
      "G loss: 13.864219665527344\n",
      "Iteration 151\n",
      "D loss: -41.865787506103516\n",
      "GP: 7.763507843017578\n",
      "Gradient norm: 1.1910849809646606\n",
      "G loss: 14.29275894165039\n",
      "Iteration 201\n",
      "D loss: -45.429405212402344\n",
      "GP: 7.162466526031494\n",
      "Gradient norm: 1.1872358322143555\n",
      "G loss: 12.219470977783203\n",
      "Iteration 251\n",
      "D loss: -42.02614974975586\n",
      "GP: 7.068120002746582\n",
      "Gradient norm: 1.1673288345336914\n",
      "G loss: 12.995265007019043\n",
      "Iteration 301\n",
      "D loss: -41.83623504638672\n",
      "GP: 7.393199920654297\n",
      "Gradient norm: 1.2152496576309204\n",
      "G loss: 12.7493896484375\n",
      "Iteration 351\n",
      "D loss: -43.825782775878906\n",
      "GP: 6.206859588623047\n",
      "Gradient norm: 1.172876238822937\n",
      "G loss: 13.453020095825195\n",
      "\n",
      "Epoch 223\n",
      "Iteration 1\n",
      "D loss: -40.9697151184082\n",
      "GP: 5.887820720672607\n",
      "Gradient norm: 1.1623084545135498\n",
      "G loss: 13.204914093017578\n",
      "Iteration 51\n",
      "D loss: -43.8154182434082\n",
      "GP: 5.543172359466553\n",
      "Gradient norm: 1.1512631177902222\n",
      "G loss: 12.751272201538086\n",
      "Iteration 101\n",
      "D loss: -43.08651351928711\n",
      "GP: 5.18634033203125\n",
      "Gradient norm: 1.1529793739318848\n",
      "G loss: 13.795856475830078\n",
      "Iteration 151\n",
      "D loss: -45.468467712402344\n",
      "GP: 6.9057512283325195\n",
      "Gradient norm: 1.1801707744598389\n",
      "G loss: 13.243437767028809\n",
      "Iteration 201\n",
      "D loss: -43.09402084350586\n",
      "GP: 6.020549774169922\n",
      "Gradient norm: 1.1710433959960938\n",
      "G loss: 13.385505676269531\n",
      "Iteration 251\n",
      "D loss: -43.367252349853516\n",
      "GP: 6.67240571975708\n",
      "Gradient norm: 1.1883866786956787\n",
      "G loss: 13.026751518249512\n",
      "Iteration 301\n",
      "D loss: -43.63422393798828\n",
      "GP: 6.360116004943848\n",
      "Gradient norm: 1.1883885860443115\n",
      "G loss: 13.651071548461914\n",
      "Iteration 351\n",
      "D loss: -42.41389083862305\n",
      "GP: 7.423029899597168\n",
      "Gradient norm: 1.196014165878296\n",
      "G loss: 13.399739265441895\n",
      "\n",
      "Epoch 224\n",
      "Iteration 1\n",
      "D loss: -41.35140609741211\n",
      "GP: 5.7430644035339355\n",
      "Gradient norm: 1.1463499069213867\n",
      "G loss: 13.765697479248047\n",
      "Iteration 51\n",
      "D loss: -41.35603332519531\n",
      "GP: 6.008464813232422\n",
      "Gradient norm: 1.1512954235076904\n",
      "G loss: 13.381567001342773\n",
      "Iteration 101\n",
      "D loss: -42.28279113769531\n",
      "GP: 5.36110782623291\n",
      "Gradient norm: 1.1403024196624756\n",
      "G loss: 13.461851119995117\n",
      "Iteration 151\n",
      "D loss: -44.08621597290039\n",
      "GP: 6.465487480163574\n",
      "Gradient norm: 1.1591136455535889\n",
      "G loss: 14.463079452514648\n",
      "Iteration 201\n",
      "D loss: -43.922359466552734\n",
      "GP: 7.710548400878906\n",
      "Gradient norm: 1.1852717399597168\n",
      "G loss: 12.478071212768555\n",
      "Iteration 251\n",
      "D loss: -43.169281005859375\n",
      "GP: 5.294289588928223\n",
      "Gradient norm: 1.1554491519927979\n",
      "G loss: 13.483453750610352\n",
      "Iteration 301\n",
      "D loss: -44.193359375\n",
      "GP: 5.998072624206543\n",
      "Gradient norm: 1.171393632888794\n",
      "G loss: 14.779483795166016\n",
      "Iteration 351\n",
      "D loss: -43.71305847167969\n",
      "GP: 7.5302653312683105\n",
      "Gradient norm: 1.1985082626342773\n",
      "G loss: 14.2910795211792\n",
      "\n",
      "Epoch 225\n",
      "Iteration 1\n",
      "D loss: -43.42555236816406\n",
      "GP: 5.624152183532715\n",
      "Gradient norm: 1.1595927476882935\n",
      "G loss: 13.44622802734375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 51\n",
      "D loss: -44.308475494384766\n",
      "GP: 6.007697582244873\n",
      "Gradient norm: 1.1661113500595093\n",
      "G loss: 13.7423677444458\n",
      "Iteration 101\n",
      "D loss: -42.56300354003906\n",
      "GP: 7.38123893737793\n",
      "Gradient norm: 1.1943824291229248\n",
      "G loss: 13.502446174621582\n",
      "Iteration 151\n",
      "D loss: -43.368621826171875\n",
      "GP: 7.0536041259765625\n",
      "Gradient norm: 1.1955336332321167\n",
      "G loss: 15.036275863647461\n",
      "Iteration 201\n",
      "D loss: -41.993614196777344\n",
      "GP: 7.624921798706055\n",
      "Gradient norm: 1.194485068321228\n",
      "G loss: 15.957395553588867\n",
      "Iteration 251\n",
      "D loss: -40.497623443603516\n",
      "GP: 8.184796333312988\n",
      "Gradient norm: 1.2053877115249634\n",
      "G loss: 13.892626762390137\n",
      "Iteration 301\n",
      "D loss: -46.958824157714844\n",
      "GP: 8.843729019165039\n",
      "Gradient norm: 1.207430362701416\n",
      "G loss: 15.186522483825684\n",
      "Iteration 351\n",
      "D loss: -43.90560531616211\n",
      "GP: 5.686878681182861\n",
      "Gradient norm: 1.1582374572753906\n",
      "G loss: 13.61732292175293\n",
      "\n",
      "Epoch 226\n",
      "Iteration 1\n",
      "D loss: -43.95624923706055\n",
      "GP: 6.612495422363281\n",
      "Gradient norm: 1.17372465133667\n",
      "G loss: 14.922232627868652\n",
      "Iteration 51\n",
      "D loss: -43.795536041259766\n",
      "GP: 8.105594635009766\n",
      "Gradient norm: 1.2136518955230713\n",
      "G loss: 14.494140625\n",
      "Iteration 101\n",
      "D loss: -42.47289276123047\n",
      "GP: 7.400938034057617\n",
      "Gradient norm: 1.1830252408981323\n",
      "G loss: 13.67033863067627\n",
      "Iteration 151\n",
      "D loss: -41.633636474609375\n",
      "GP: 7.9334611892700195\n",
      "Gradient norm: 1.1901421546936035\n",
      "G loss: 14.601699829101562\n",
      "Iteration 201\n",
      "D loss: -43.51835250854492\n",
      "GP: 6.430535316467285\n",
      "Gradient norm: 1.1742817163467407\n",
      "G loss: 15.6883544921875\n",
      "Iteration 251\n",
      "D loss: -44.2078971862793\n",
      "GP: 6.741566181182861\n",
      "Gradient norm: 1.180923342704773\n",
      "G loss: 15.159737586975098\n",
      "Iteration 301\n",
      "D loss: -42.803977966308594\n",
      "GP: 7.2165961265563965\n",
      "Gradient norm: 1.185530185699463\n",
      "G loss: 14.805400848388672\n",
      "Iteration 351\n",
      "D loss: -44.71592712402344\n",
      "GP: 7.201165676116943\n",
      "Gradient norm: 1.1784406900405884\n",
      "G loss: 14.199848175048828\n",
      "\n",
      "Epoch 227\n",
      "Iteration 1\n",
      "D loss: -42.509639739990234\n",
      "GP: 6.648922443389893\n",
      "Gradient norm: 1.1654778718948364\n",
      "G loss: 14.407454490661621\n",
      "Iteration 51\n",
      "D loss: -44.85784912109375\n",
      "GP: 6.581110000610352\n",
      "Gradient norm: 1.1732139587402344\n",
      "G loss: 15.493703842163086\n",
      "Iteration 101\n",
      "D loss: -43.2413330078125\n",
      "GP: 6.401657581329346\n",
      "Gradient norm: 1.1695860624313354\n",
      "G loss: 13.807954788208008\n",
      "Iteration 151\n",
      "D loss: -43.595787048339844\n",
      "GP: 7.035536766052246\n",
      "Gradient norm: 1.1909464597702026\n",
      "G loss: 17.206798553466797\n",
      "Iteration 201\n",
      "D loss: -44.652488708496094\n",
      "GP: 7.686788558959961\n",
      "Gradient norm: 1.200292944908142\n",
      "G loss: 15.211729049682617\n",
      "Iteration 251\n",
      "D loss: -44.351173400878906\n",
      "GP: 7.381394386291504\n",
      "Gradient norm: 1.1932722330093384\n",
      "G loss: 14.895610809326172\n",
      "Iteration 301\n",
      "D loss: -42.635799407958984\n",
      "GP: 7.243371486663818\n",
      "Gradient norm: 1.1861363649368286\n",
      "G loss: 16.06612205505371\n",
      "Iteration 351\n",
      "D loss: -42.729148864746094\n",
      "GP: 9.265860557556152\n",
      "Gradient norm: 1.2190587520599365\n",
      "G loss: 16.260894775390625\n",
      "\n",
      "Epoch 228\n",
      "Iteration 1\n",
      "D loss: -44.642738342285156\n",
      "GP: 7.34643030166626\n",
      "Gradient norm: 1.1924611330032349\n",
      "G loss: 15.280487060546875\n",
      "Iteration 51\n",
      "D loss: -42.67573547363281\n",
      "GP: 7.077645778656006\n",
      "Gradient norm: 1.1828864812850952\n",
      "G loss: 17.2981014251709\n",
      "Iteration 101\n",
      "D loss: -43.018943786621094\n",
      "GP: 5.81356954574585\n",
      "Gradient norm: 1.1534225940704346\n",
      "G loss: 16.066272735595703\n",
      "Iteration 151\n",
      "D loss: -42.31215286254883\n",
      "GP: 8.171116828918457\n",
      "Gradient norm: 1.1919337511062622\n",
      "G loss: 16.129608154296875\n",
      "Iteration 201\n",
      "D loss: -44.01710510253906\n",
      "GP: 6.835031986236572\n",
      "Gradient norm: 1.1711806058883667\n",
      "G loss: 15.529168128967285\n",
      "Iteration 251\n",
      "D loss: -43.19679260253906\n",
      "GP: 6.542724609375\n",
      "Gradient norm: 1.17536461353302\n",
      "G loss: 15.2197265625\n",
      "Iteration 301\n",
      "D loss: -43.56132125854492\n",
      "GP: 6.4704179763793945\n",
      "Gradient norm: 1.1696510314941406\n",
      "G loss: 14.988606452941895\n",
      "Iteration 351\n",
      "D loss: -46.72592544555664\n",
      "GP: 6.3867998123168945\n",
      "Gradient norm: 1.1756548881530762\n",
      "G loss: 16.15890884399414\n",
      "\n",
      "Epoch 229\n",
      "Iteration 1\n",
      "D loss: -43.834529876708984\n",
      "GP: 6.570475101470947\n",
      "Gradient norm: 1.1568948030471802\n",
      "G loss: 16.017786026000977\n",
      "Iteration 51\n",
      "D loss: -43.216712951660156\n",
      "GP: 7.704004764556885\n",
      "Gradient norm: 1.197975993156433\n",
      "G loss: 14.728983879089355\n",
      "Iteration 101\n",
      "D loss: -43.160789489746094\n",
      "GP: 7.226998805999756\n",
      "Gradient norm: 1.1915225982666016\n",
      "G loss: 15.806568145751953\n",
      "Iteration 151\n",
      "D loss: -43.33889389038086\n",
      "GP: 8.680545806884766\n",
      "Gradient norm: 1.2194561958312988\n",
      "G loss: 16.16652488708496\n",
      "Iteration 201\n",
      "D loss: -45.0157585144043\n",
      "GP: 7.088366508483887\n",
      "Gradient norm: 1.1816542148590088\n",
      "G loss: 14.944401741027832\n",
      "Iteration 251\n",
      "D loss: -41.2828483581543\n",
      "GP: 5.916101932525635\n",
      "Gradient norm: 1.1339960098266602\n",
      "G loss: 15.192835807800293\n",
      "Iteration 301\n",
      "D loss: -43.81661605834961\n",
      "GP: 6.742786407470703\n",
      "Gradient norm: 1.1798745393753052\n",
      "G loss: 15.941511154174805\n",
      "Iteration 351\n",
      "D loss: -42.467071533203125\n",
      "GP: 6.370298862457275\n",
      "Gradient norm: 1.1635336875915527\n",
      "G loss: 16.534793853759766\n",
      "\n",
      "Epoch 230\n",
      "Iteration 1\n",
      "D loss: -42.92376708984375\n",
      "GP: 5.777750015258789\n",
      "Gradient norm: 1.169486403465271\n",
      "G loss: 15.412199020385742\n",
      "Iteration 51\n",
      "D loss: -43.15391159057617\n",
      "GP: 6.910694122314453\n",
      "Gradient norm: 1.178860068321228\n",
      "G loss: 15.020800590515137\n",
      "Iteration 101\n",
      "D loss: -43.50689697265625\n",
      "GP: 6.0031514167785645\n",
      "Gradient norm: 1.1657843589782715\n",
      "G loss: 16.2513370513916\n",
      "Iteration 151\n",
      "D loss: -43.49266052246094\n",
      "GP: 7.259795188903809\n",
      "Gradient norm: 1.1971046924591064\n",
      "G loss: 15.46239948272705\n",
      "Iteration 201\n",
      "D loss: -44.63909149169922\n",
      "GP: 6.304693698883057\n",
      "Gradient norm: 1.167220115661621\n",
      "G loss: 15.925525665283203\n",
      "Iteration 251\n",
      "D loss: -44.050453186035156\n",
      "GP: 6.080275058746338\n",
      "Gradient norm: 1.1615718603134155\n",
      "G loss: 15.864175796508789\n",
      "Iteration 301\n",
      "D loss: -43.45204162597656\n",
      "GP: 7.926935195922852\n",
      "Gradient norm: 1.2122864723205566\n",
      "G loss: 15.883822441101074\n",
      "Iteration 351\n",
      "D loss: -42.42538070678711\n",
      "GP: 6.849482536315918\n",
      "Gradient norm: 1.1832408905029297\n",
      "G loss: 16.2396297454834\n",
      "\n",
      "Epoch 231\n",
      "Iteration 1\n",
      "D loss: -42.19677734375\n",
      "GP: 6.317031383514404\n",
      "Gradient norm: 1.1606675386428833\n",
      "G loss: 16.193758010864258\n",
      "Iteration 51\n",
      "D loss: -43.83945083618164\n",
      "GP: 5.916928291320801\n",
      "Gradient norm: 1.1620680093765259\n",
      "G loss: 14.961984634399414\n",
      "Iteration 101\n",
      "D loss: -44.50076675415039\n",
      "GP: 6.807399749755859\n",
      "Gradient norm: 1.1854355335235596\n",
      "G loss: 14.806337356567383\n",
      "Iteration 151\n",
      "D loss: -43.55241394042969\n",
      "GP: 6.620883941650391\n",
      "Gradient norm: 1.1699316501617432\n",
      "G loss: 14.096941947937012\n",
      "Iteration 201\n",
      "D loss: -42.83634948730469\n",
      "GP: 6.269693374633789\n",
      "Gradient norm: 1.1708869934082031\n",
      "G loss: 16.31027603149414\n",
      "Iteration 251\n",
      "D loss: -43.47323989868164\n",
      "GP: 5.468459606170654\n",
      "Gradient norm: 1.1495938301086426\n",
      "G loss: 14.890271186828613\n",
      "Iteration 301\n",
      "D loss: -42.002620697021484\n",
      "GP: 7.397194862365723\n",
      "Gradient norm: 1.1852325201034546\n",
      "G loss: 15.629827499389648\n",
      "Iteration 351\n",
      "D loss: -42.41883850097656\n",
      "GP: 5.688085556030273\n",
      "Gradient norm: 1.1569734811782837\n",
      "G loss: 16.8704833984375\n",
      "\n",
      "Epoch 232\n",
      "Iteration 1\n",
      "D loss: -43.80480194091797\n",
      "GP: 8.374536514282227\n",
      "Gradient norm: 1.19821035861969\n",
      "G loss: 17.002666473388672\n",
      "Iteration 51\n",
      "D loss: -44.41684341430664\n",
      "GP: 7.590808868408203\n",
      "Gradient norm: 1.1999342441558838\n",
      "G loss: 16.82821273803711\n",
      "Iteration 101\n",
      "D loss: -41.08333969116211\n",
      "GP: 9.259807586669922\n",
      "Gradient norm: 1.221472978591919\n",
      "G loss: 14.359671592712402\n",
      "Iteration 151\n",
      "D loss: -42.48158264160156\n",
      "GP: 6.635211944580078\n",
      "Gradient norm: 1.172985553741455\n",
      "G loss: 15.66569709777832\n",
      "Iteration 201\n",
      "D loss: -43.16861343383789\n",
      "GP: 7.7231597900390625\n",
      "Gradient norm: 1.166421890258789\n",
      "G loss: 16.03504180908203\n",
      "Iteration 251\n",
      "D loss: -42.784732818603516\n",
      "GP: 7.310100555419922\n",
      "Gradient norm: 1.1897588968276978\n",
      "G loss: 14.980716705322266\n",
      "Iteration 301\n",
      "D loss: -43.11739730834961\n",
      "GP: 7.121974945068359\n",
      "Gradient norm: 1.1927486658096313\n",
      "G loss: 15.549300193786621\n",
      "Iteration 351\n",
      "D loss: -40.356388092041016\n",
      "GP: 7.2309370040893555\n",
      "Gradient norm: 1.1811667680740356\n",
      "G loss: 16.191917419433594\n",
      "\n",
      "Epoch 233\n",
      "Iteration 1\n",
      "D loss: -43.12593078613281\n",
      "GP: 8.39328670501709\n",
      "Gradient norm: 1.2057592868804932\n",
      "G loss: 16.197221755981445\n",
      "Iteration 51\n",
      "D loss: -42.778724670410156\n",
      "GP: 5.096282958984375\n",
      "Gradient norm: 1.151350498199463\n",
      "G loss: 17.183950424194336\n",
      "Iteration 101\n",
      "D loss: -43.665096282958984\n",
      "GP: 6.6533918380737305\n",
      "Gradient norm: 1.1740249395370483\n",
      "G loss: 15.739901542663574\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 151\n",
      "D loss: -45.38871765136719\n",
      "GP: 7.101119041442871\n",
      "Gradient norm: 1.172286033630371\n",
      "G loss: 17.02842140197754\n",
      "Iteration 201\n",
      "D loss: -42.13370895385742\n",
      "GP: 8.365440368652344\n",
      "Gradient norm: 1.2079668045043945\n",
      "G loss: 16.27903938293457\n",
      "Iteration 251\n",
      "D loss: -41.704833984375\n",
      "GP: 6.697641372680664\n",
      "Gradient norm: 1.1791014671325684\n",
      "G loss: 16.959413528442383\n",
      "Iteration 301\n",
      "D loss: -44.76179504394531\n",
      "GP: 7.453985214233398\n",
      "Gradient norm: 1.1825306415557861\n",
      "G loss: 15.891281127929688\n",
      "Iteration 351\n",
      "D loss: -41.2087287902832\n",
      "GP: 7.528307914733887\n",
      "Gradient norm: 1.1977031230926514\n",
      "G loss: 15.623983383178711\n",
      "\n",
      "Epoch 234\n",
      "Iteration 1\n",
      "D loss: -42.698753356933594\n",
      "GP: 7.1524248123168945\n",
      "Gradient norm: 1.1911706924438477\n",
      "G loss: 15.454798698425293\n",
      "Iteration 51\n",
      "D loss: -44.75806427001953\n",
      "GP: 8.625853538513184\n",
      "Gradient norm: 1.210484266281128\n",
      "G loss: 15.407957077026367\n",
      "Iteration 101\n",
      "D loss: -42.98583221435547\n",
      "GP: 6.462440013885498\n",
      "Gradient norm: 1.172582983970642\n",
      "G loss: 16.427574157714844\n",
      "Iteration 151\n",
      "D loss: -42.254329681396484\n",
      "GP: 6.520252227783203\n",
      "Gradient norm: 1.1612775325775146\n",
      "G loss: 16.69354820251465\n",
      "Iteration 201\n",
      "D loss: -42.29301452636719\n",
      "GP: 8.209245681762695\n",
      "Gradient norm: 1.1997441053390503\n",
      "G loss: 14.86563491821289\n",
      "Iteration 251\n",
      "D loss: -45.75651550292969\n",
      "GP: 5.478085517883301\n",
      "Gradient norm: 1.1672260761260986\n",
      "G loss: 15.616350173950195\n",
      "Iteration 301\n",
      "D loss: -44.33271408081055\n",
      "GP: 6.734304904937744\n",
      "Gradient norm: 1.1918542385101318\n",
      "G loss: 14.995299339294434\n",
      "Iteration 351\n",
      "D loss: -44.18821334838867\n",
      "GP: 6.654599189758301\n",
      "Gradient norm: 1.1781116724014282\n",
      "G loss: 14.937761306762695\n",
      "\n",
      "Epoch 235\n",
      "Iteration 1\n",
      "D loss: -45.425811767578125\n",
      "GP: 9.38414192199707\n",
      "Gradient norm: 1.2337898015975952\n",
      "G loss: 15.07629680633545\n",
      "Iteration 51\n",
      "D loss: -42.23820495605469\n",
      "GP: 5.98982048034668\n",
      "Gradient norm: 1.1639082431793213\n",
      "G loss: 15.12424373626709\n",
      "Iteration 101\n",
      "D loss: -42.7995719909668\n",
      "GP: 7.6266889572143555\n",
      "Gradient norm: 1.1844699382781982\n",
      "G loss: 15.926966667175293\n",
      "Iteration 151\n",
      "D loss: -44.52919006347656\n",
      "GP: 6.5324788093566895\n",
      "Gradient norm: 1.1610710620880127\n",
      "G loss: 16.160991668701172\n",
      "Iteration 201\n",
      "D loss: -42.19697570800781\n",
      "GP: 6.936994552612305\n",
      "Gradient norm: 1.18606698513031\n",
      "G loss: 14.921524047851562\n",
      "Iteration 251\n",
      "D loss: -43.968238830566406\n",
      "GP: 9.463476181030273\n",
      "Gradient norm: 1.2271044254302979\n",
      "G loss: 16.20600128173828\n",
      "Iteration 301\n",
      "D loss: -40.10980224609375\n",
      "GP: 7.469022750854492\n",
      "Gradient norm: 1.1878747940063477\n",
      "G loss: 15.900442123413086\n",
      "Iteration 351\n",
      "D loss: -41.849700927734375\n",
      "GP: 7.392608642578125\n",
      "Gradient norm: 1.181885838508606\n",
      "G loss: 16.544715881347656\n",
      "\n",
      "Epoch 236\n",
      "Iteration 1\n",
      "D loss: -42.871429443359375\n",
      "GP: 7.129518985748291\n",
      "Gradient norm: 1.1833912134170532\n",
      "G loss: 15.533348083496094\n",
      "Iteration 51\n",
      "D loss: -43.93593978881836\n",
      "GP: 7.482906341552734\n",
      "Gradient norm: 1.205559253692627\n",
      "G loss: 15.473621368408203\n",
      "Iteration 101\n",
      "D loss: -41.743263244628906\n",
      "GP: 8.200355529785156\n",
      "Gradient norm: 1.1888231039047241\n",
      "G loss: 15.7340726852417\n",
      "Iteration 151\n",
      "D loss: -43.59844970703125\n",
      "GP: 7.6277852058410645\n",
      "Gradient norm: 1.193569302558899\n",
      "G loss: 14.695174217224121\n",
      "Iteration 201\n",
      "D loss: -45.021240234375\n",
      "GP: 6.167846202850342\n",
      "Gradient norm: 1.160004734992981\n",
      "G loss: 15.053579330444336\n",
      "Iteration 251\n",
      "D loss: -42.19367599487305\n",
      "GP: 5.938468933105469\n",
      "Gradient norm: 1.1685616970062256\n",
      "G loss: 15.841946601867676\n",
      "Iteration 301\n",
      "D loss: -41.91008758544922\n",
      "GP: 5.443710803985596\n",
      "Gradient norm: 1.1400268077850342\n",
      "G loss: 15.05263900756836\n",
      "Iteration 351\n",
      "D loss: -45.62632369995117\n",
      "GP: 7.130439281463623\n",
      "Gradient norm: 1.190713882446289\n",
      "G loss: 15.124311447143555\n",
      "\n",
      "Epoch 237\n",
      "Iteration 1\n",
      "D loss: -44.002037048339844\n",
      "GP: 7.179632663726807\n",
      "Gradient norm: 1.177686333656311\n",
      "G loss: 15.065335273742676\n",
      "Iteration 51\n",
      "D loss: -43.6567268371582\n",
      "GP: 6.897526741027832\n",
      "Gradient norm: 1.1743186712265015\n",
      "G loss: 17.153654098510742\n",
      "Iteration 101\n",
      "D loss: -43.13780212402344\n",
      "GP: 6.784856796264648\n",
      "Gradient norm: 1.1693778038024902\n",
      "G loss: 16.240703582763672\n",
      "Iteration 151\n",
      "D loss: -45.53556442260742\n",
      "GP: 5.884181976318359\n",
      "Gradient norm: 1.1642378568649292\n",
      "G loss: 15.721275329589844\n",
      "Iteration 201\n",
      "D loss: -44.809635162353516\n",
      "GP: 5.827901363372803\n",
      "Gradient norm: 1.1500229835510254\n",
      "G loss: 15.99997329711914\n",
      "Iteration 251\n",
      "D loss: -42.45567321777344\n",
      "GP: 6.702932357788086\n",
      "Gradient norm: 1.1745409965515137\n",
      "G loss: 15.541165351867676\n",
      "Iteration 301\n",
      "D loss: -41.45393753051758\n",
      "GP: 5.91742467880249\n",
      "Gradient norm: 1.1620362997055054\n",
      "G loss: 15.701501846313477\n",
      "Iteration 351\n",
      "D loss: -41.34159851074219\n",
      "GP: 7.614884376525879\n",
      "Gradient norm: 1.1789439916610718\n",
      "G loss: 15.913213729858398\n",
      "\n",
      "Epoch 238\n",
      "Iteration 1\n",
      "D loss: -41.2510986328125\n",
      "GP: 8.020254135131836\n",
      "Gradient norm: 1.1962015628814697\n",
      "G loss: 15.794585227966309\n",
      "Iteration 51\n",
      "D loss: -41.64703369140625\n",
      "GP: 7.777710914611816\n",
      "Gradient norm: 1.194530725479126\n",
      "G loss: 17.46381950378418\n",
      "Iteration 101\n",
      "D loss: -41.49913024902344\n",
      "GP: 6.641374588012695\n",
      "Gradient norm: 1.174798607826233\n",
      "G loss: 20.353967666625977\n",
      "Iteration 151\n",
      "D loss: -39.6760139465332\n",
      "GP: 7.095437526702881\n",
      "Gradient norm: 1.1678352355957031\n",
      "G loss: 21.25836181640625\n",
      "Iteration 201\n",
      "D loss: -43.85978317260742\n",
      "GP: 6.100918769836426\n",
      "Gradient norm: 1.1681442260742188\n",
      "G loss: 19.323013305664062\n",
      "Iteration 251\n",
      "D loss: -43.3299560546875\n",
      "GP: 5.927846431732178\n",
      "Gradient norm: 1.1543827056884766\n",
      "G loss: 20.31702995300293\n",
      "Iteration 301\n",
      "D loss: -39.721275329589844\n",
      "GP: 6.1988301277160645\n",
      "Gradient norm: 1.1642237901687622\n",
      "G loss: 19.69033432006836\n",
      "Iteration 351\n",
      "D loss: -43.11354064941406\n",
      "GP: 7.490034103393555\n",
      "Gradient norm: 1.19912588596344\n",
      "G loss: 20.204343795776367\n",
      "\n",
      "Epoch 239\n",
      "Iteration 1\n",
      "D loss: -43.43736267089844\n",
      "GP: 7.738722801208496\n",
      "Gradient norm: 1.193843126296997\n",
      "G loss: 18.978261947631836\n",
      "Iteration 51\n",
      "D loss: -42.45550537109375\n",
      "GP: 9.377763748168945\n",
      "Gradient norm: 1.2131924629211426\n",
      "G loss: 18.150312423706055\n",
      "Iteration 101\n",
      "D loss: -41.53011703491211\n",
      "GP: 7.609045505523682\n",
      "Gradient norm: 1.1842339038848877\n",
      "G loss: 18.332656860351562\n",
      "Iteration 151\n",
      "D loss: -41.26532745361328\n",
      "GP: 6.610026836395264\n",
      "Gradient norm: 1.1817238330841064\n",
      "G loss: 18.309293746948242\n",
      "Iteration 201\n",
      "D loss: -40.46820068359375\n",
      "GP: 7.25871467590332\n",
      "Gradient norm: 1.190080165863037\n",
      "G loss: 17.74394989013672\n",
      "Iteration 251\n",
      "D loss: -40.16594696044922\n",
      "GP: 7.12059211730957\n",
      "Gradient norm: 1.177650809288025\n",
      "G loss: 19.664321899414062\n",
      "Iteration 301\n",
      "D loss: -42.58018493652344\n",
      "GP: 6.382436752319336\n",
      "Gradient norm: 1.170105218887329\n",
      "G loss: 18.56324005126953\n",
      "Iteration 351\n",
      "D loss: -42.67570114135742\n",
      "GP: 6.678744316101074\n",
      "Gradient norm: 1.1707489490509033\n",
      "G loss: 19.980907440185547\n",
      "\n",
      "Epoch 240\n",
      "Iteration 1\n",
      "D loss: -42.720401763916016\n",
      "GP: 6.490706443786621\n",
      "Gradient norm: 1.1822162866592407\n",
      "G loss: 18.47786521911621\n",
      "Iteration 51\n",
      "D loss: -40.354522705078125\n",
      "GP: 5.889638900756836\n",
      "Gradient norm: 1.1510387659072876\n",
      "G loss: 18.82927131652832\n",
      "Iteration 101\n",
      "D loss: -43.87571716308594\n",
      "GP: 6.686234951019287\n",
      "Gradient norm: 1.1662553548812866\n",
      "G loss: 19.748212814331055\n",
      "Iteration 151\n",
      "D loss: -40.505611419677734\n",
      "GP: 7.023930072784424\n",
      "Gradient norm: 1.194385290145874\n",
      "G loss: 18.42969512939453\n",
      "Iteration 201\n",
      "D loss: -43.04157638549805\n",
      "GP: 6.879328727722168\n",
      "Gradient norm: 1.1688854694366455\n",
      "G loss: 18.494365692138672\n",
      "Iteration 251\n",
      "D loss: -43.290367126464844\n",
      "GP: 6.215684413909912\n",
      "Gradient norm: 1.1782152652740479\n",
      "G loss: 19.518722534179688\n",
      "Iteration 301\n",
      "D loss: -43.22032165527344\n",
      "GP: 6.636632919311523\n",
      "Gradient norm: 1.1851716041564941\n",
      "G loss: 19.70455551147461\n",
      "Iteration 351\n",
      "D loss: -44.63663864135742\n",
      "GP: 5.748445987701416\n",
      "Gradient norm: 1.1661460399627686\n",
      "G loss: 18.356731414794922\n",
      "\n",
      "Epoch 241\n",
      "Iteration 1\n",
      "D loss: -42.10871887207031\n",
      "GP: 8.220260620117188\n",
      "Gradient norm: 1.2190661430358887\n",
      "G loss: 18.076519012451172\n",
      "Iteration 51\n",
      "D loss: -45.09046936035156\n",
      "GP: 6.909069061279297\n",
      "Gradient norm: 1.1733566522598267\n",
      "G loss: 19.803028106689453\n",
      "Iteration 101\n",
      "D loss: -45.83230209350586\n",
      "GP: 5.504863262176514\n",
      "Gradient norm: 1.1655043363571167\n",
      "G loss: 20.83213996887207\n",
      "Iteration 151\n",
      "D loss: -41.69709777832031\n",
      "GP: 7.715936660766602\n",
      "Gradient norm: 1.1868486404418945\n",
      "G loss: 19.846302032470703\n",
      "Iteration 201\n",
      "D loss: -42.04347610473633\n",
      "GP: 7.106435298919678\n",
      "Gradient norm: 1.1608701944351196\n",
      "G loss: 18.949125289916992\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 251\n",
      "D loss: -44.07500457763672\n",
      "GP: 8.732133865356445\n",
      "Gradient norm: 1.2247768640518188\n",
      "G loss: 19.480722427368164\n",
      "Iteration 301\n",
      "D loss: -43.074649810791016\n",
      "GP: 8.68421459197998\n",
      "Gradient norm: 1.2058902978897095\n",
      "G loss: 19.709184646606445\n",
      "Iteration 351\n",
      "D loss: -43.47887420654297\n",
      "GP: 6.66197395324707\n",
      "Gradient norm: 1.1551623344421387\n",
      "G loss: 18.666399002075195\n",
      "\n",
      "Epoch 242\n",
      "Iteration 1\n",
      "D loss: -44.12901306152344\n",
      "GP: 7.485454559326172\n",
      "Gradient norm: 1.1912164688110352\n",
      "G loss: 18.78009033203125\n",
      "Iteration 51\n",
      "D loss: -41.90117263793945\n",
      "GP: 7.133002281188965\n",
      "Gradient norm: 1.1883491277694702\n",
      "G loss: 19.753402709960938\n",
      "Iteration 101\n",
      "D loss: -40.81857681274414\n",
      "GP: 5.865854740142822\n",
      "Gradient norm: 1.1659226417541504\n",
      "G loss: 19.10013198852539\n",
      "Iteration 151\n",
      "D loss: -42.795928955078125\n",
      "GP: 5.566214561462402\n",
      "Gradient norm: 1.1613130569458008\n",
      "G loss: 18.67487907409668\n",
      "Iteration 201\n",
      "D loss: -45.781002044677734\n",
      "GP: 6.347899913787842\n",
      "Gradient norm: 1.1630611419677734\n",
      "G loss: 20.85354232788086\n",
      "Iteration 251\n",
      "D loss: -42.30635070800781\n",
      "GP: 7.649657249450684\n",
      "Gradient norm: 1.1931833028793335\n",
      "G loss: 20.458600997924805\n",
      "Iteration 301\n",
      "D loss: -43.5637321472168\n",
      "GP: 7.918376922607422\n",
      "Gradient norm: 1.194172739982605\n",
      "G loss: 19.260515213012695\n",
      "Iteration 351\n",
      "D loss: -40.65199279785156\n",
      "GP: 8.737791061401367\n",
      "Gradient norm: 1.2062597274780273\n",
      "G loss: 20.36494255065918\n",
      "\n",
      "Epoch 243\n",
      "Iteration 1\n",
      "D loss: -41.72492599487305\n",
      "GP: 7.874423027038574\n",
      "Gradient norm: 1.2107956409454346\n",
      "G loss: 19.55267333984375\n",
      "Iteration 51\n",
      "D loss: -41.84965515136719\n",
      "GP: 5.761528968811035\n",
      "Gradient norm: 1.1725560426712036\n",
      "G loss: 19.095849990844727\n",
      "Iteration 101\n",
      "D loss: -43.276058197021484\n",
      "GP: 6.492428779602051\n",
      "Gradient norm: 1.1640151739120483\n",
      "G loss: 18.755046844482422\n",
      "Iteration 151\n",
      "D loss: -42.17483139038086\n",
      "GP: 7.464108467102051\n",
      "Gradient norm: 1.1869930028915405\n",
      "G loss: 19.152387619018555\n",
      "Iteration 201\n",
      "D loss: -43.87907409667969\n",
      "GP: 6.19979190826416\n",
      "Gradient norm: 1.1614627838134766\n",
      "G loss: 18.933109283447266\n",
      "Iteration 251\n",
      "D loss: -45.20541000366211\n",
      "GP: 7.112834453582764\n",
      "Gradient norm: 1.169938564300537\n",
      "G loss: 20.177494049072266\n",
      "Iteration 301\n",
      "D loss: -41.16455078125\n",
      "GP: 8.032209396362305\n",
      "Gradient norm: 1.1854703426361084\n",
      "G loss: 19.73626708984375\n",
      "Iteration 351\n",
      "D loss: -42.377479553222656\n",
      "GP: 9.645151138305664\n",
      "Gradient norm: 1.216648817062378\n",
      "G loss: 19.598955154418945\n",
      "\n",
      "Epoch 244\n",
      "Iteration 1\n",
      "D loss: -42.6451416015625\n",
      "GP: 7.372152328491211\n",
      "Gradient norm: 1.181847095489502\n",
      "G loss: 20.427366256713867\n",
      "Iteration 51\n",
      "D loss: -45.46210861206055\n",
      "GP: 5.079683780670166\n",
      "Gradient norm: 1.1377885341644287\n",
      "G loss: 19.14765739440918\n",
      "Iteration 101\n",
      "D loss: -44.541988372802734\n",
      "GP: 8.601032257080078\n",
      "Gradient norm: 1.2126808166503906\n",
      "G loss: 19.543094635009766\n",
      "Iteration 151\n",
      "D loss: -43.47390365600586\n",
      "GP: 7.614335060119629\n",
      "Gradient norm: 1.2121546268463135\n",
      "G loss: 18.206092834472656\n",
      "Iteration 201\n",
      "D loss: -43.98225784301758\n",
      "GP: 8.080089569091797\n",
      "Gradient norm: 1.2159860134124756\n",
      "G loss: 18.965757369995117\n",
      "Iteration 251\n",
      "D loss: -44.145023345947266\n",
      "GP: 7.602121829986572\n",
      "Gradient norm: 1.1872451305389404\n",
      "G loss: 18.406644821166992\n",
      "Iteration 301\n",
      "D loss: -44.75495910644531\n",
      "GP: 5.860007286071777\n",
      "Gradient norm: 1.1639357805252075\n",
      "G loss: 19.062191009521484\n",
      "Iteration 351\n",
      "D loss: -45.031341552734375\n",
      "GP: 6.1679768562316895\n",
      "Gradient norm: 1.161070704460144\n",
      "G loss: 18.022212982177734\n",
      "\n",
      "Epoch 245\n",
      "Iteration 1\n",
      "D loss: -42.55038833618164\n",
      "GP: 5.953822612762451\n",
      "Gradient norm: 1.1625699996948242\n",
      "G loss: 19.36358642578125\n",
      "Iteration 51\n",
      "D loss: -44.27178955078125\n",
      "GP: 8.816742897033691\n",
      "Gradient norm: 1.2289812564849854\n",
      "G loss: 18.897729873657227\n",
      "Iteration 101\n",
      "D loss: -43.209266662597656\n",
      "GP: 6.103823184967041\n",
      "Gradient norm: 1.1688439846038818\n",
      "G loss: 18.777482986450195\n",
      "Iteration 151\n",
      "D loss: -45.04421615600586\n",
      "GP: 6.6153998374938965\n",
      "Gradient norm: 1.171118140220642\n",
      "G loss: 19.749074935913086\n",
      "Iteration 201\n",
      "D loss: -41.06660842895508\n",
      "GP: 6.8074517250061035\n",
      "Gradient norm: 1.1853749752044678\n",
      "G loss: 17.994464874267578\n",
      "Iteration 251\n",
      "D loss: -42.18315124511719\n",
      "GP: 6.920738220214844\n",
      "Gradient norm: 1.17813241481781\n",
      "G loss: 19.030555725097656\n",
      "Iteration 301\n",
      "D loss: -43.62177658081055\n",
      "GP: 8.99736499786377\n",
      "Gradient norm: 1.2224702835083008\n",
      "G loss: 18.214370727539062\n",
      "Iteration 351\n",
      "D loss: -42.210960388183594\n",
      "GP: 6.743007183074951\n",
      "Gradient norm: 1.1805148124694824\n",
      "G loss: 19.176427841186523\n",
      "\n",
      "Epoch 246\n",
      "Iteration 1\n",
      "D loss: -44.03756332397461\n",
      "GP: 8.827649116516113\n",
      "Gradient norm: 1.2218549251556396\n",
      "G loss: 18.50242042541504\n",
      "Iteration 51\n",
      "D loss: -42.6931266784668\n",
      "GP: 6.284665584564209\n",
      "Gradient norm: 1.170708417892456\n",
      "G loss: 19.052278518676758\n",
      "Iteration 101\n",
      "D loss: -42.986114501953125\n",
      "GP: 6.281402111053467\n",
      "Gradient norm: 1.1650102138519287\n",
      "G loss: 18.082658767700195\n",
      "Iteration 151\n",
      "D loss: -44.199920654296875\n",
      "GP: 7.171358108520508\n",
      "Gradient norm: 1.1930774450302124\n",
      "G loss: 17.113964080810547\n",
      "Iteration 201\n",
      "D loss: -43.82914733886719\n",
      "GP: 9.592371940612793\n",
      "Gradient norm: 1.2368643283843994\n",
      "G loss: 16.20643424987793\n",
      "Iteration 251\n",
      "D loss: -43.76726531982422\n",
      "GP: 5.982017993927002\n",
      "Gradient norm: 1.1802866458892822\n",
      "G loss: 18.771543502807617\n",
      "Iteration 301\n",
      "D loss: -42.5316276550293\n",
      "GP: 6.909830570220947\n",
      "Gradient norm: 1.1701083183288574\n",
      "G loss: 18.20995330810547\n",
      "Iteration 351\n",
      "D loss: -42.9676513671875\n",
      "GP: 5.145636558532715\n",
      "Gradient norm: 1.1459393501281738\n",
      "G loss: 18.83759117126465\n",
      "\n",
      "Epoch 247\n",
      "Iteration 1\n",
      "D loss: -43.276824951171875\n",
      "GP: 7.656949996948242\n",
      "Gradient norm: 1.171447515487671\n",
      "G loss: 17.536563873291016\n",
      "Iteration 51\n",
      "D loss: -43.519290924072266\n",
      "GP: 7.346287250518799\n",
      "Gradient norm: 1.192682147026062\n",
      "G loss: 17.666419982910156\n",
      "Iteration 101\n",
      "D loss: -41.985687255859375\n",
      "GP: 7.15876579284668\n",
      "Gradient norm: 1.183214783668518\n",
      "G loss: 17.22179412841797\n",
      "Iteration 151\n",
      "D loss: -44.92137908935547\n",
      "GP: 5.44810676574707\n",
      "Gradient norm: 1.1397300958633423\n",
      "G loss: 16.471708297729492\n",
      "Iteration 201\n",
      "D loss: -43.67059326171875\n",
      "GP: 5.557703018188477\n",
      "Gradient norm: 1.1526340246200562\n",
      "G loss: 14.550374984741211\n",
      "Iteration 251\n",
      "D loss: -45.072139739990234\n",
      "GP: 5.9169487953186035\n",
      "Gradient norm: 1.1904102563858032\n",
      "G loss: 16.119062423706055\n",
      "Iteration 301\n",
      "D loss: -44.70059585571289\n",
      "GP: 7.854001522064209\n",
      "Gradient norm: 1.1909958124160767\n",
      "G loss: 17.858726501464844\n",
      "Iteration 351\n",
      "D loss: -40.95562744140625\n",
      "GP: 6.715631484985352\n",
      "Gradient norm: 1.1670994758605957\n",
      "G loss: 16.104143142700195\n",
      "\n",
      "Epoch 248\n",
      "Iteration 1\n",
      "D loss: -44.320987701416016\n",
      "GP: 6.211850643157959\n",
      "Gradient norm: 1.1468623876571655\n",
      "G loss: 15.751203536987305\n",
      "Iteration 51\n",
      "D loss: -43.27073669433594\n",
      "GP: 6.740801811218262\n",
      "Gradient norm: 1.1875526905059814\n",
      "G loss: 16.643997192382812\n",
      "Iteration 101\n",
      "D loss: -42.64348602294922\n",
      "GP: 7.0239949226379395\n",
      "Gradient norm: 1.164397954940796\n",
      "G loss: 16.237567901611328\n",
      "Iteration 151\n",
      "D loss: -42.50208282470703\n",
      "GP: 7.694594860076904\n",
      "Gradient norm: 1.2027370929718018\n",
      "G loss: 14.042274475097656\n",
      "Iteration 201\n",
      "D loss: -43.04922866821289\n",
      "GP: 6.0498762130737305\n",
      "Gradient norm: 1.166471004486084\n",
      "G loss: 15.732718467712402\n",
      "Iteration 251\n",
      "D loss: -41.88969421386719\n",
      "GP: 6.475144386291504\n",
      "Gradient norm: 1.1832892894744873\n",
      "G loss: 15.753412246704102\n",
      "Iteration 301\n",
      "D loss: -41.702117919921875\n",
      "GP: 7.202177047729492\n",
      "Gradient norm: 1.1595664024353027\n",
      "G loss: 16.169227600097656\n",
      "Iteration 351\n",
      "D loss: -43.25759506225586\n",
      "GP: 5.561251163482666\n",
      "Gradient norm: 1.1365317106246948\n",
      "G loss: 16.238527297973633\n",
      "\n",
      "Epoch 249\n",
      "Iteration 1\n",
      "D loss: -43.70067596435547\n",
      "GP: 7.8664398193359375\n",
      "Gradient norm: 1.2037439346313477\n",
      "G loss: 16.550251007080078\n",
      "Iteration 51\n",
      "D loss: -43.43427276611328\n",
      "GP: 6.491762161254883\n",
      "Gradient norm: 1.1619349718093872\n",
      "G loss: 14.607338905334473\n",
      "Iteration 101\n",
      "D loss: -44.986568450927734\n",
      "GP: 7.311429023742676\n",
      "Gradient norm: 1.1701691150665283\n",
      "G loss: 14.506827354431152\n",
      "Iteration 151\n",
      "D loss: -43.500205993652344\n",
      "GP: 9.228681564331055\n",
      "Gradient norm: 1.205449104309082\n",
      "G loss: 13.443967819213867\n",
      "Iteration 201\n",
      "D loss: -43.67621994018555\n",
      "GP: 6.884417533874512\n",
      "Gradient norm: 1.166807770729065\n",
      "G loss: 15.749361991882324\n",
      "Iteration 251\n",
      "D loss: -42.05583190917969\n",
      "GP: 7.673545837402344\n",
      "Gradient norm: 1.189950704574585\n",
      "G loss: 15.993448257446289\n",
      "Iteration 301\n",
      "D loss: -43.53313446044922\n",
      "GP: 6.602324485778809\n",
      "Gradient norm: 1.1751741170883179\n",
      "G loss: 15.918216705322266\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 351\n",
      "D loss: -43.46055221557617\n",
      "GP: 7.189422130584717\n",
      "Gradient norm: 1.1971282958984375\n",
      "G loss: 14.50506591796875\n",
      "\n",
      "Epoch 250\n",
      "Iteration 1\n",
      "D loss: -42.956703186035156\n",
      "GP: 7.85389518737793\n",
      "Gradient norm: 1.212633728981018\n",
      "G loss: 14.839545249938965\n",
      "Iteration 51\n",
      "D loss: -41.49771499633789\n",
      "GP: 8.130507469177246\n",
      "Gradient norm: 1.1986383199691772\n",
      "G loss: 13.31749439239502\n",
      "Iteration 101\n",
      "D loss: -42.450416564941406\n",
      "GP: 7.819027900695801\n",
      "Gradient norm: 1.1867088079452515\n",
      "G loss: 16.166255950927734\n",
      "Iteration 151\n",
      "D loss: -41.200008392333984\n",
      "GP: 9.349284172058105\n",
      "Gradient norm: 1.2222082614898682\n",
      "G loss: 16.518821716308594\n",
      "Iteration 201\n",
      "D loss: -43.46018981933594\n",
      "GP: 7.102601528167725\n",
      "Gradient norm: 1.183043360710144\n",
      "G loss: 15.408738136291504\n",
      "Iteration 251\n",
      "D loss: -40.696128845214844\n",
      "GP: 8.743446350097656\n",
      "Gradient norm: 1.2221602201461792\n",
      "G loss: 16.198471069335938\n",
      "Iteration 301\n",
      "D loss: -42.56390380859375\n",
      "GP: 8.09668254852295\n",
      "Gradient norm: 1.2086881399154663\n",
      "G loss: 16.387699127197266\n",
      "Iteration 351\n",
      "D loss: -42.75444793701172\n",
      "GP: 8.756227493286133\n",
      "Gradient norm: 1.1995859146118164\n",
      "G loss: 14.203470230102539\n",
      "\n",
      "Epoch 251\n",
      "Iteration 1\n",
      "D loss: -42.56705093383789\n",
      "GP: 7.013744354248047\n",
      "Gradient norm: 1.1657800674438477\n",
      "G loss: 13.868032455444336\n",
      "Iteration 51\n",
      "D loss: -45.19499969482422\n",
      "GP: 6.935500621795654\n",
      "Gradient norm: 1.184562087059021\n",
      "G loss: 14.740689277648926\n",
      "Iteration 101\n",
      "D loss: -42.44624328613281\n",
      "GP: 8.069660186767578\n",
      "Gradient norm: 1.2000173330307007\n",
      "G loss: 14.81399154663086\n",
      "Iteration 151\n",
      "D loss: -43.43547058105469\n",
      "GP: 5.809234619140625\n",
      "Gradient norm: 1.155928134918213\n",
      "G loss: 15.668625831604004\n",
      "Iteration 201\n",
      "D loss: -42.4798469543457\n",
      "GP: 7.476650238037109\n",
      "Gradient norm: 1.1877729892730713\n",
      "G loss: 14.861172676086426\n",
      "Iteration 251\n",
      "D loss: -42.063072204589844\n",
      "GP: 7.955359935760498\n",
      "Gradient norm: 1.1800806522369385\n",
      "G loss: 15.688024520874023\n",
      "Iteration 301\n",
      "D loss: -42.296142578125\n",
      "GP: 5.99517297744751\n",
      "Gradient norm: 1.161912441253662\n",
      "G loss: 13.93722915649414\n",
      "Iteration 351\n",
      "D loss: -42.365821838378906\n",
      "GP: 6.096004962921143\n",
      "Gradient norm: 1.163523554801941\n",
      "G loss: 15.271032333374023\n",
      "\n",
      "Epoch 252\n",
      "Iteration 1\n",
      "D loss: -43.347618103027344\n",
      "GP: 8.045427322387695\n",
      "Gradient norm: 1.202328085899353\n",
      "G loss: 14.855506896972656\n",
      "Iteration 51\n",
      "D loss: -44.80245590209961\n",
      "GP: 7.678828239440918\n",
      "Gradient norm: 1.1852396726608276\n",
      "G loss: 15.275527954101562\n",
      "Iteration 101\n",
      "D loss: -44.55663299560547\n",
      "GP: 6.251516819000244\n",
      "Gradient norm: 1.1706252098083496\n",
      "G loss: 15.11191177368164\n",
      "Iteration 151\n",
      "D loss: -42.26518630981445\n",
      "GP: 7.455096244812012\n",
      "Gradient norm: 1.174108624458313\n",
      "G loss: 15.165596008300781\n",
      "Iteration 201\n",
      "D loss: -42.76008987426758\n",
      "GP: 8.223125457763672\n",
      "Gradient norm: 1.2109230756759644\n",
      "G loss: 14.978535652160645\n",
      "Iteration 251\n",
      "D loss: -43.454654693603516\n",
      "GP: 6.892948627471924\n",
      "Gradient norm: 1.1715576648712158\n",
      "G loss: 16.03228759765625\n",
      "Iteration 301\n",
      "D loss: -44.61368179321289\n",
      "GP: 7.323346138000488\n",
      "Gradient norm: 1.1865167617797852\n",
      "G loss: 15.951431274414062\n",
      "Iteration 351\n",
      "D loss: -41.71290969848633\n",
      "GP: 5.874783992767334\n",
      "Gradient norm: 1.152669906616211\n",
      "G loss: 16.231273651123047\n",
      "\n",
      "Epoch 253\n",
      "Iteration 1\n",
      "D loss: -42.816009521484375\n",
      "GP: 7.4898362159729\n",
      "Gradient norm: 1.2045869827270508\n",
      "G loss: 14.305171966552734\n",
      "Iteration 51\n",
      "D loss: -42.84862518310547\n",
      "GP: 6.833388805389404\n",
      "Gradient norm: 1.1875426769256592\n",
      "G loss: 15.731481552124023\n",
      "Iteration 101\n",
      "D loss: -41.24608612060547\n",
      "GP: 8.631628036499023\n",
      "Gradient norm: 1.204750895500183\n",
      "G loss: 14.304488182067871\n",
      "Iteration 151\n",
      "D loss: -42.65890121459961\n",
      "GP: 6.2747979164123535\n",
      "Gradient norm: 1.1635487079620361\n",
      "G loss: 15.031752586364746\n",
      "Iteration 201\n",
      "D loss: -43.54608154296875\n",
      "GP: 6.31007194519043\n",
      "Gradient norm: 1.1656230688095093\n",
      "G loss: 15.537870407104492\n",
      "Iteration 251\n",
      "D loss: -44.55406188964844\n",
      "GP: 6.628347396850586\n",
      "Gradient norm: 1.184553861618042\n",
      "G loss: 15.533478736877441\n",
      "Iteration 301\n",
      "D loss: -40.5126838684082\n",
      "GP: 8.199634552001953\n",
      "Gradient norm: 1.1973252296447754\n",
      "G loss: 15.632539749145508\n",
      "Iteration 351\n",
      "D loss: -42.224761962890625\n",
      "GP: 7.701677322387695\n",
      "Gradient norm: 1.1855591535568237\n",
      "G loss: 13.379302978515625\n",
      "\n",
      "Epoch 254\n",
      "Iteration 1\n",
      "D loss: -41.668460845947266\n",
      "GP: 6.707438945770264\n",
      "Gradient norm: 1.1844769716262817\n",
      "G loss: 15.310717582702637\n",
      "Iteration 51\n",
      "D loss: -43.16642379760742\n",
      "GP: 7.599177360534668\n",
      "Gradient norm: 1.1868155002593994\n",
      "G loss: 14.129109382629395\n",
      "Iteration 101\n",
      "D loss: -43.79515075683594\n",
      "GP: 6.856103897094727\n",
      "Gradient norm: 1.1756445169448853\n",
      "G loss: 14.858766555786133\n",
      "Iteration 151\n",
      "D loss: -46.61811065673828\n",
      "GP: 6.846435070037842\n",
      "Gradient norm: 1.1777125597000122\n",
      "G loss: 14.645842552185059\n",
      "Iteration 201\n",
      "D loss: -42.48321533203125\n",
      "GP: 5.034646987915039\n",
      "Gradient norm: 1.1291450262069702\n",
      "G loss: 15.390294075012207\n",
      "Iteration 251\n",
      "D loss: -42.94434356689453\n",
      "GP: 5.340505123138428\n",
      "Gradient norm: 1.149372935295105\n",
      "G loss: 15.350383758544922\n",
      "Iteration 301\n",
      "D loss: -43.70948028564453\n",
      "GP: 6.445093154907227\n",
      "Gradient norm: 1.180975317955017\n",
      "G loss: 16.57492446899414\n",
      "Iteration 351\n",
      "D loss: -42.378623962402344\n",
      "GP: 8.156167984008789\n",
      "Gradient norm: 1.2036082744598389\n",
      "G loss: 15.1781005859375\n",
      "\n",
      "Epoch 255\n",
      "Iteration 1\n",
      "D loss: -43.96941375732422\n",
      "GP: 7.483952522277832\n",
      "Gradient norm: 1.170277714729309\n",
      "G loss: 14.761991500854492\n",
      "Iteration 51\n",
      "D loss: -43.97305679321289\n",
      "GP: 6.7188568115234375\n",
      "Gradient norm: 1.1785064935684204\n",
      "G loss: 14.525737762451172\n",
      "Iteration 101\n",
      "D loss: -42.379486083984375\n",
      "GP: 6.63018274307251\n",
      "Gradient norm: 1.1808818578720093\n",
      "G loss: 14.402338027954102\n",
      "Iteration 151\n",
      "D loss: -43.81084060668945\n",
      "GP: 7.728843688964844\n",
      "Gradient norm: 1.1995258331298828\n",
      "G loss: 15.873289108276367\n",
      "Iteration 201\n",
      "D loss: -42.183135986328125\n",
      "GP: 8.200349807739258\n",
      "Gradient norm: 1.2129956483840942\n",
      "G loss: 16.152244567871094\n",
      "Iteration 251\n",
      "D loss: -43.478492736816406\n",
      "GP: 6.640877723693848\n",
      "Gradient norm: 1.1716008186340332\n",
      "G loss: 17.431594848632812\n",
      "Iteration 301\n",
      "D loss: -43.20781326293945\n",
      "GP: 6.103395938873291\n",
      "Gradient norm: 1.1675302982330322\n",
      "G loss: 15.439339637756348\n",
      "Iteration 351\n",
      "D loss: -42.20113754272461\n",
      "GP: 6.956897735595703\n",
      "Gradient norm: 1.170419692993164\n",
      "G loss: 16.05777931213379\n",
      "\n",
      "Epoch 256\n",
      "Iteration 1\n",
      "D loss: -43.24888610839844\n",
      "GP: 7.1191511154174805\n",
      "Gradient norm: 1.189172625541687\n",
      "G loss: 15.585281372070312\n",
      "Iteration 51\n",
      "D loss: -45.51851272583008\n",
      "GP: 5.66100549697876\n",
      "Gradient norm: 1.1715738773345947\n",
      "G loss: 15.767626762390137\n",
      "Iteration 101\n",
      "D loss: -45.00934600830078\n",
      "GP: 7.056708335876465\n",
      "Gradient norm: 1.18674898147583\n",
      "G loss: 15.810429573059082\n",
      "Iteration 151\n",
      "D loss: -41.08174514770508\n",
      "GP: 6.883615970611572\n",
      "Gradient norm: 1.1707115173339844\n",
      "G loss: 15.594016075134277\n",
      "Iteration 201\n",
      "D loss: -43.3386344909668\n",
      "GP: 6.492138862609863\n",
      "Gradient norm: 1.168452262878418\n",
      "G loss: 15.870681762695312\n",
      "Iteration 251\n",
      "D loss: -44.46420669555664\n",
      "GP: 7.914588451385498\n",
      "Gradient norm: 1.1919453144073486\n",
      "G loss: 16.318941116333008\n",
      "Iteration 301\n",
      "D loss: -42.00709915161133\n",
      "GP: 7.626086235046387\n",
      "Gradient norm: 1.1940338611602783\n",
      "G loss: 15.662284851074219\n",
      "Iteration 351\n",
      "D loss: -43.475555419921875\n",
      "GP: 7.19860315322876\n",
      "Gradient norm: 1.1815882921218872\n",
      "G loss: 14.915635108947754\n",
      "\n",
      "Epoch 257\n",
      "Iteration 1\n",
      "D loss: -42.63098907470703\n",
      "GP: 5.944998264312744\n",
      "Gradient norm: 1.1608755588531494\n",
      "G loss: 14.86370849609375\n",
      "Iteration 51\n",
      "D loss: -43.04020690917969\n",
      "GP: 5.387727737426758\n",
      "Gradient norm: 1.158626675605774\n",
      "G loss: 16.02936363220215\n",
      "Iteration 101\n",
      "D loss: -43.32202911376953\n",
      "GP: 7.353988170623779\n",
      "Gradient norm: 1.1997663974761963\n",
      "G loss: 14.952802658081055\n",
      "Iteration 151\n",
      "D loss: -41.670711517333984\n",
      "GP: 7.581507682800293\n",
      "Gradient norm: 1.1739039421081543\n",
      "G loss: 16.443742752075195\n",
      "Iteration 201\n",
      "D loss: -45.29093933105469\n",
      "GP: 7.235122203826904\n",
      "Gradient norm: 1.185613751411438\n",
      "G loss: 16.115589141845703\n",
      "Iteration 251\n",
      "D loss: -41.22412872314453\n",
      "GP: 6.264107704162598\n",
      "Gradient norm: 1.1751129627227783\n",
      "G loss: 14.96597957611084\n",
      "Iteration 301\n",
      "D loss: -43.721370697021484\n",
      "GP: 6.4591064453125\n",
      "Gradient norm: 1.162851095199585\n",
      "G loss: 15.380764961242676\n",
      "Iteration 351\n",
      "D loss: -43.6314582824707\n",
      "GP: 7.331815242767334\n",
      "Gradient norm: 1.1921344995498657\n",
      "G loss: 13.982422828674316\n",
      "\n",
      "Epoch 258\n",
      "Iteration 1\n",
      "D loss: -42.74976348876953\n",
      "GP: 8.05018424987793\n",
      "Gradient norm: 1.1839762926101685\n",
      "G loss: 14.762669563293457\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 51\n",
      "D loss: -43.93780517578125\n",
      "GP: 5.838376998901367\n",
      "Gradient norm: 1.165832757949829\n",
      "G loss: 15.540416717529297\n",
      "Iteration 101\n",
      "D loss: -41.30647659301758\n",
      "GP: 6.976503849029541\n",
      "Gradient norm: 1.1700738668441772\n",
      "G loss: 15.592695236206055\n",
      "Iteration 151\n",
      "D loss: -43.99894714355469\n",
      "GP: 8.010452270507812\n",
      "Gradient norm: 1.199060082435608\n",
      "G loss: 15.587430000305176\n",
      "Iteration 201\n",
      "D loss: -42.892730712890625\n",
      "GP: 8.920778274536133\n",
      "Gradient norm: 1.20290207862854\n",
      "G loss: 16.22313117980957\n",
      "Iteration 251\n",
      "D loss: -38.898963928222656\n",
      "GP: 5.947751998901367\n",
      "Gradient norm: 1.1373014450073242\n",
      "G loss: 14.842801094055176\n",
      "Iteration 301\n",
      "D loss: -44.03556823730469\n",
      "GP: 9.16130256652832\n",
      "Gradient norm: 1.2101943492889404\n",
      "G loss: 15.772318840026855\n",
      "Iteration 351\n",
      "D loss: -44.892578125\n",
      "GP: 6.928205490112305\n",
      "Gradient norm: 1.169744610786438\n",
      "G loss: 16.060012817382812\n",
      "\n",
      "Epoch 259\n",
      "Iteration 1\n",
      "D loss: -41.66293716430664\n",
      "GP: 5.961245536804199\n",
      "Gradient norm: 1.1467365026474\n",
      "G loss: 15.15403938293457\n",
      "Iteration 51\n",
      "D loss: -45.72923278808594\n",
      "GP: 5.857341289520264\n",
      "Gradient norm: 1.1590582132339478\n",
      "G loss: 16.865066528320312\n",
      "Iteration 101\n",
      "D loss: -41.603111267089844\n",
      "GP: 6.236929893493652\n",
      "Gradient norm: 1.176990032196045\n",
      "G loss: 16.85846710205078\n",
      "Iteration 151\n",
      "D loss: -40.88475036621094\n",
      "GP: 8.175080299377441\n",
      "Gradient norm: 1.2055079936981201\n",
      "G loss: 16.519241333007812\n",
      "Iteration 201\n",
      "D loss: -44.02547073364258\n",
      "GP: 7.4707536697387695\n",
      "Gradient norm: 1.206299066543579\n",
      "G loss: 15.015374183654785\n",
      "Iteration 251\n",
      "D loss: -43.20922088623047\n",
      "GP: 6.04084587097168\n",
      "Gradient norm: 1.1442509889602661\n",
      "G loss: 16.14708709716797\n",
      "Iteration 301\n",
      "D loss: -43.24861145019531\n",
      "GP: 5.599748611450195\n",
      "Gradient norm: 1.1448479890823364\n",
      "G loss: 15.179970741271973\n",
      "Iteration 351\n",
      "D loss: -43.31692886352539\n",
      "GP: 7.749521732330322\n",
      "Gradient norm: 1.1983815431594849\n",
      "G loss: 16.579492568969727\n",
      "\n",
      "Epoch 260\n",
      "Iteration 1\n",
      "D loss: -41.99779510498047\n",
      "GP: 9.732595443725586\n",
      "Gradient norm: 1.2228050231933594\n",
      "G loss: 14.91310977935791\n",
      "Iteration 51\n",
      "D loss: -43.39977264404297\n",
      "GP: 8.027191162109375\n",
      "Gradient norm: 1.1988070011138916\n",
      "G loss: 16.754690170288086\n",
      "Iteration 101\n",
      "D loss: -44.80588150024414\n",
      "GP: 7.578643798828125\n",
      "Gradient norm: 1.198884129524231\n",
      "G loss: 15.074533462524414\n",
      "Iteration 151\n",
      "D loss: -39.991790771484375\n",
      "GP: 8.654474258422852\n",
      "Gradient norm: 1.1955517530441284\n",
      "G loss: 15.179466247558594\n",
      "Iteration 201\n",
      "D loss: -42.27267837524414\n",
      "GP: 7.554811954498291\n",
      "Gradient norm: 1.193009853363037\n",
      "G loss: 15.366410255432129\n",
      "Iteration 251\n",
      "D loss: -43.79681396484375\n",
      "GP: 5.9920172691345215\n",
      "Gradient norm: 1.1686346530914307\n",
      "G loss: 14.816243171691895\n",
      "Iteration 301\n",
      "D loss: -44.43413543701172\n",
      "GP: 6.779024124145508\n",
      "Gradient norm: 1.1579854488372803\n",
      "G loss: 15.055445671081543\n",
      "Iteration 351\n",
      "D loss: -42.527320861816406\n",
      "GP: 6.2958831787109375\n",
      "Gradient norm: 1.1627429723739624\n",
      "G loss: 15.546964645385742\n",
      "\n",
      "Epoch 261\n",
      "Iteration 1\n",
      "D loss: -45.052406311035156\n",
      "GP: 8.995050430297852\n",
      "Gradient norm: 1.2129449844360352\n",
      "G loss: 16.38330078125\n",
      "Iteration 51\n",
      "D loss: -42.42656326293945\n",
      "GP: 7.261296272277832\n",
      "Gradient norm: 1.1749110221862793\n",
      "G loss: 15.461527824401855\n",
      "Iteration 101\n",
      "D loss: -43.069923400878906\n",
      "GP: 7.284294128417969\n",
      "Gradient norm: 1.19474458694458\n",
      "G loss: 16.326248168945312\n",
      "Iteration 151\n",
      "D loss: -41.308284759521484\n",
      "GP: 7.828094959259033\n",
      "Gradient norm: 1.1896203756332397\n",
      "G loss: 14.122964859008789\n",
      "Iteration 201\n",
      "D loss: -41.14727783203125\n",
      "GP: 8.45747184753418\n",
      "Gradient norm: 1.1872601509094238\n",
      "G loss: 14.581377983093262\n",
      "Iteration 251\n",
      "D loss: -40.40263748168945\n",
      "GP: 7.8607988357543945\n",
      "Gradient norm: 1.1996203660964966\n",
      "G loss: 16.042146682739258\n",
      "Iteration 301\n",
      "D loss: -41.9705696105957\n",
      "GP: 9.017239570617676\n",
      "Gradient norm: 1.2185698747634888\n",
      "G loss: 17.020708084106445\n",
      "Iteration 351\n",
      "D loss: -43.373924255371094\n",
      "GP: 6.068063259124756\n",
      "Gradient norm: 1.142463207244873\n",
      "G loss: 14.458366394042969\n",
      "\n",
      "Epoch 262\n",
      "Iteration 1\n",
      "D loss: -40.081275939941406\n",
      "GP: 7.344347953796387\n",
      "Gradient norm: 1.1887210607528687\n",
      "G loss: 14.668135643005371\n",
      "Iteration 51\n",
      "D loss: -42.923065185546875\n",
      "GP: 6.0342020988464355\n",
      "Gradient norm: 1.1388895511627197\n",
      "G loss: 14.693246841430664\n",
      "Iteration 101\n",
      "D loss: -43.74833679199219\n",
      "GP: 6.3957839012146\n",
      "Gradient norm: 1.1642377376556396\n",
      "G loss: 13.511822700500488\n",
      "Iteration 151\n",
      "D loss: -40.571533203125\n",
      "GP: 5.149293422698975\n",
      "Gradient norm: 1.153633952140808\n",
      "G loss: 14.409531593322754\n",
      "Iteration 201\n",
      "D loss: -44.30465316772461\n",
      "GP: 7.466649532318115\n",
      "Gradient norm: 1.1885576248168945\n",
      "G loss: 15.224099159240723\n",
      "Iteration 251\n",
      "D loss: -44.050933837890625\n",
      "GP: 7.162836074829102\n",
      "Gradient norm: 1.1824674606323242\n",
      "G loss: 14.198347091674805\n",
      "Iteration 301\n",
      "D loss: -42.13893508911133\n",
      "GP: 5.942027568817139\n",
      "Gradient norm: 1.1575615406036377\n",
      "G loss: 15.02944278717041\n",
      "Iteration 351\n",
      "D loss: -45.620948791503906\n",
      "GP: 6.676271438598633\n",
      "Gradient norm: 1.1695973873138428\n",
      "G loss: 15.293244361877441\n",
      "\n",
      "Epoch 263\n",
      "Iteration 1\n",
      "D loss: -42.43383026123047\n",
      "GP: 7.036977767944336\n",
      "Gradient norm: 1.1898800134658813\n",
      "G loss: 15.6934175491333\n",
      "Iteration 51\n",
      "D loss: -43.538795471191406\n",
      "GP: 6.1367268562316895\n",
      "Gradient norm: 1.1640244722366333\n",
      "G loss: 15.152815818786621\n",
      "Iteration 101\n",
      "D loss: -42.32022476196289\n",
      "GP: 6.091692924499512\n",
      "Gradient norm: 1.1642810106277466\n",
      "G loss: 14.019390106201172\n",
      "Iteration 151\n",
      "D loss: -42.1846809387207\n",
      "GP: 7.359310150146484\n",
      "Gradient norm: 1.1934629678726196\n",
      "G loss: 13.946571350097656\n",
      "Iteration 201\n",
      "D loss: -38.362911224365234\n",
      "GP: 7.106155872344971\n",
      "Gradient norm: 1.1756033897399902\n",
      "G loss: 14.442275047302246\n",
      "Iteration 251\n",
      "D loss: -39.479217529296875\n",
      "GP: 6.726386547088623\n",
      "Gradient norm: 1.1693652868270874\n",
      "G loss: 16.085655212402344\n",
      "Iteration 301\n",
      "D loss: -43.463661193847656\n",
      "GP: 6.541197299957275\n",
      "Gradient norm: 1.1699244976043701\n",
      "G loss: 14.91909408569336\n",
      "Iteration 351\n",
      "D loss: -43.78306579589844\n",
      "GP: 6.489872932434082\n",
      "Gradient norm: 1.1577703952789307\n",
      "G loss: 14.73859977722168\n",
      "\n",
      "Epoch 264\n",
      "Iteration 1\n",
      "D loss: -42.39196014404297\n",
      "GP: 8.06290054321289\n",
      "Gradient norm: 1.1930830478668213\n",
      "G loss: 14.011336326599121\n",
      "Iteration 51\n",
      "D loss: -42.154022216796875\n",
      "GP: 7.492604732513428\n",
      "Gradient norm: 1.2052688598632812\n",
      "G loss: 14.79347038269043\n",
      "Iteration 101\n",
      "D loss: -42.27204895019531\n",
      "GP: 8.942330360412598\n",
      "Gradient norm: 1.2040523290634155\n",
      "G loss: 14.214807510375977\n",
      "Iteration 151\n",
      "D loss: -42.57835006713867\n",
      "GP: 6.647076606750488\n",
      "Gradient norm: 1.1645798683166504\n",
      "G loss: 15.015226364135742\n",
      "Iteration 201\n",
      "D loss: -43.15601348876953\n",
      "GP: 5.317275524139404\n",
      "Gradient norm: 1.1469149589538574\n",
      "G loss: 14.940742492675781\n",
      "Iteration 251\n",
      "D loss: -42.53886413574219\n",
      "GP: 7.460042953491211\n",
      "Gradient norm: 1.1968814134597778\n",
      "G loss: 15.70893669128418\n",
      "Iteration 301\n",
      "D loss: -44.76122283935547\n",
      "GP: 6.481978893280029\n",
      "Gradient norm: 1.170946478843689\n",
      "G loss: 16.19184112548828\n",
      "Iteration 351\n",
      "D loss: -39.77766799926758\n",
      "GP: 7.505072593688965\n",
      "Gradient norm: 1.1681945323944092\n",
      "G loss: 14.265093803405762\n",
      "\n",
      "Epoch 265\n",
      "Iteration 1\n",
      "D loss: -42.13843536376953\n",
      "GP: 6.382059097290039\n",
      "Gradient norm: 1.1703969240188599\n",
      "G loss: 15.405352592468262\n",
      "Iteration 51\n",
      "D loss: -44.909915924072266\n",
      "GP: 8.0830717086792\n",
      "Gradient norm: 1.2033445835113525\n",
      "G loss: 15.0202054977417\n",
      "Iteration 101\n",
      "D loss: -40.536895751953125\n",
      "GP: 5.8535919189453125\n",
      "Gradient norm: 1.151932954788208\n",
      "G loss: 13.524882316589355\n",
      "Iteration 151\n",
      "D loss: -44.467185974121094\n",
      "GP: 6.124037742614746\n",
      "Gradient norm: 1.1654953956604004\n",
      "G loss: 15.576839447021484\n",
      "Iteration 201\n",
      "D loss: -41.340553283691406\n",
      "GP: 7.599664688110352\n",
      "Gradient norm: 1.1853506565093994\n",
      "G loss: 15.578128814697266\n",
      "Iteration 251\n",
      "D loss: -44.37468338012695\n",
      "GP: 8.0123929977417\n",
      "Gradient norm: 1.1989104747772217\n",
      "G loss: 15.929245948791504\n",
      "Iteration 301\n",
      "D loss: -42.95909881591797\n",
      "GP: 4.939958095550537\n",
      "Gradient norm: 1.14877188205719\n",
      "G loss: 15.090117454528809\n",
      "Iteration 351\n",
      "D loss: -45.60734176635742\n",
      "GP: 7.897767543792725\n",
      "Gradient norm: 1.1882014274597168\n",
      "G loss: 15.129968643188477\n",
      "\n",
      "Epoch 266\n",
      "Iteration 1\n",
      "D loss: -42.59370040893555\n",
      "GP: 6.561071872711182\n",
      "Gradient norm: 1.171960711479187\n",
      "G loss: 14.7206449508667\n",
      "Iteration 51\n",
      "D loss: -44.41223907470703\n",
      "GP: 8.194574356079102\n",
      "Gradient norm: 1.1979975700378418\n",
      "G loss: 15.321910858154297\n",
      "Iteration 101\n",
      "D loss: -43.71973419189453\n",
      "GP: 8.230724334716797\n",
      "Gradient norm: 1.2039371728897095\n",
      "G loss: 14.055354118347168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 151\n",
      "D loss: -41.703575134277344\n",
      "GP: 7.465539932250977\n",
      "Gradient norm: 1.1936274766921997\n",
      "G loss: 14.735599517822266\n",
      "Iteration 201\n",
      "D loss: -43.5181884765625\n",
      "GP: 7.71910285949707\n",
      "Gradient norm: 1.202045202255249\n",
      "G loss: 15.989188194274902\n",
      "Iteration 251\n",
      "D loss: -44.11810302734375\n",
      "GP: 7.364922046661377\n",
      "Gradient norm: 1.1744009256362915\n",
      "G loss: 14.647343635559082\n",
      "Iteration 301\n",
      "D loss: -44.31535720825195\n",
      "GP: 6.2062764167785645\n",
      "Gradient norm: 1.1684383153915405\n",
      "G loss: 16.420452117919922\n",
      "Iteration 351\n",
      "D loss: -42.98955535888672\n",
      "GP: 5.9986186027526855\n",
      "Gradient norm: 1.1623032093048096\n",
      "G loss: 13.939338684082031\n",
      "\n",
      "Epoch 267\n",
      "Iteration 1\n",
      "D loss: -41.69371795654297\n",
      "GP: 7.662290573120117\n",
      "Gradient norm: 1.1802194118499756\n",
      "G loss: 14.207530975341797\n",
      "Iteration 51\n",
      "D loss: -42.192806243896484\n",
      "GP: 8.433792114257812\n",
      "Gradient norm: 1.211634874343872\n",
      "G loss: 14.292693138122559\n",
      "Iteration 101\n",
      "D loss: -39.44689178466797\n",
      "GP: 8.898591041564941\n",
      "Gradient norm: 1.1998573541641235\n",
      "G loss: 15.752971649169922\n",
      "Iteration 151\n",
      "D loss: -42.735172271728516\n",
      "GP: 7.191313743591309\n",
      "Gradient norm: 1.1888171434402466\n",
      "G loss: 16.86585235595703\n",
      "Iteration 201\n",
      "D loss: -40.40870666503906\n",
      "GP: 8.042136192321777\n",
      "Gradient norm: 1.1870841979980469\n",
      "G loss: 15.659825325012207\n",
      "Iteration 251\n",
      "D loss: -44.557308197021484\n",
      "GP: 7.473167896270752\n",
      "Gradient norm: 1.2063060998916626\n",
      "G loss: 15.365225791931152\n",
      "Iteration 301\n",
      "D loss: -44.49258804321289\n",
      "GP: 5.680065631866455\n",
      "Gradient norm: 1.1565393209457397\n",
      "G loss: 16.095888137817383\n",
      "Iteration 351\n",
      "D loss: -42.95677947998047\n",
      "GP: 5.708170413970947\n",
      "Gradient norm: 1.1575651168823242\n",
      "G loss: 15.695056915283203\n",
      "\n",
      "Epoch 268\n",
      "Iteration 1\n",
      "D loss: -42.38970184326172\n",
      "GP: 8.026843070983887\n",
      "Gradient norm: 1.210622787475586\n",
      "G loss: 15.600594520568848\n",
      "Iteration 51\n",
      "D loss: -41.3557243347168\n",
      "GP: 7.916778564453125\n",
      "Gradient norm: 1.200827956199646\n",
      "G loss: 14.72952651977539\n",
      "Iteration 101\n",
      "D loss: -43.55873107910156\n",
      "GP: 5.901409149169922\n",
      "Gradient norm: 1.160914659500122\n",
      "G loss: 16.8702449798584\n",
      "Iteration 151\n",
      "D loss: -41.02941131591797\n",
      "GP: 5.955062389373779\n",
      "Gradient norm: 1.1665393114089966\n",
      "G loss: 15.032206535339355\n",
      "Iteration 201\n",
      "D loss: -41.29765701293945\n",
      "GP: 6.067028045654297\n",
      "Gradient norm: 1.1668000221252441\n",
      "G loss: 16.173601150512695\n",
      "Iteration 251\n",
      "D loss: -40.91963195800781\n",
      "GP: 6.188234329223633\n",
      "Gradient norm: 1.1657558679580688\n",
      "G loss: 15.79672622680664\n",
      "Iteration 301\n",
      "D loss: -44.13180160522461\n",
      "GP: 8.609240531921387\n",
      "Gradient norm: 1.2028894424438477\n",
      "G loss: 14.845932006835938\n",
      "Iteration 351\n",
      "D loss: -42.08757781982422\n",
      "GP: 7.01413631439209\n",
      "Gradient norm: 1.1876428127288818\n",
      "G loss: 16.285118103027344\n",
      "\n",
      "Epoch 269\n",
      "Iteration 1\n",
      "D loss: -44.29542922973633\n",
      "GP: 7.195224761962891\n",
      "Gradient norm: 1.1950193643569946\n",
      "G loss: 15.014668464660645\n",
      "Iteration 51\n",
      "D loss: -45.25959777832031\n",
      "GP: 6.678872108459473\n",
      "Gradient norm: 1.1704338788986206\n",
      "G loss: 14.990509033203125\n",
      "Iteration 101\n",
      "D loss: -41.29902648925781\n",
      "GP: 7.339334964752197\n",
      "Gradient norm: 1.1799609661102295\n",
      "G loss: 15.799220085144043\n",
      "Iteration 151\n",
      "D loss: -43.791603088378906\n",
      "GP: 6.097040176391602\n",
      "Gradient norm: 1.162021517753601\n",
      "G loss: 16.469846725463867\n",
      "Iteration 201\n",
      "D loss: -42.04444885253906\n",
      "GP: 6.303560256958008\n",
      "Gradient norm: 1.1648317575454712\n",
      "G loss: 17.048839569091797\n",
      "Iteration 251\n",
      "D loss: -43.14253616333008\n",
      "GP: 6.1291399002075195\n",
      "Gradient norm: 1.1709489822387695\n",
      "G loss: 16.2613525390625\n",
      "Iteration 301\n",
      "D loss: -45.112640380859375\n",
      "GP: 7.806864261627197\n",
      "Gradient norm: 1.186079740524292\n",
      "G loss: 14.9425687789917\n",
      "Iteration 351\n",
      "D loss: -41.27206802368164\n",
      "GP: 8.030349731445312\n",
      "Gradient norm: 1.1806405782699585\n",
      "G loss: 15.320817947387695\n",
      "\n",
      "Epoch 270\n",
      "Iteration 1\n",
      "D loss: -41.41454315185547\n",
      "GP: 7.095820903778076\n",
      "Gradient norm: 1.1831910610198975\n",
      "G loss: 14.923928260803223\n",
      "Iteration 51\n",
      "D loss: -45.21851348876953\n",
      "GP: 7.00459098815918\n",
      "Gradient norm: 1.1845494508743286\n",
      "G loss: 16.683818817138672\n",
      "Iteration 101\n",
      "D loss: -40.058433532714844\n",
      "GP: 7.598423004150391\n",
      "Gradient norm: 1.1863584518432617\n",
      "G loss: 14.541698455810547\n",
      "Iteration 151\n",
      "D loss: -42.301788330078125\n",
      "GP: 6.626980781555176\n",
      "Gradient norm: 1.1729025840759277\n",
      "G loss: 17.12807273864746\n",
      "Iteration 201\n",
      "D loss: -44.008506774902344\n",
      "GP: 7.576226711273193\n",
      "Gradient norm: 1.1893287897109985\n",
      "G loss: 16.101478576660156\n",
      "Iteration 251\n",
      "D loss: -41.448219299316406\n",
      "GP: 7.127642631530762\n",
      "Gradient norm: 1.175836205482483\n",
      "G loss: 14.859957695007324\n",
      "Iteration 301\n",
      "D loss: -43.738441467285156\n",
      "GP: 8.282617568969727\n",
      "Gradient norm: 1.2144451141357422\n",
      "G loss: 15.721529006958008\n",
      "Iteration 351\n",
      "D loss: -42.334083557128906\n",
      "GP: 7.69645881652832\n",
      "Gradient norm: 1.1860525608062744\n",
      "G loss: 14.569814682006836\n",
      "\n",
      "Epoch 271\n",
      "Iteration 1\n",
      "D loss: -42.057106018066406\n",
      "GP: 7.88905143737793\n",
      "Gradient norm: 1.2045866250991821\n",
      "G loss: 14.425434112548828\n",
      "Iteration 51\n",
      "D loss: -44.19675064086914\n",
      "GP: 10.077576637268066\n",
      "Gradient norm: 1.229379415512085\n",
      "G loss: 15.20036792755127\n",
      "Iteration 101\n",
      "D loss: -43.6213493347168\n",
      "GP: 8.15966796875\n",
      "Gradient norm: 1.207592248916626\n",
      "G loss: 15.927633285522461\n",
      "Iteration 151\n",
      "D loss: -43.41044616699219\n",
      "GP: 6.468728065490723\n",
      "Gradient norm: 1.1833648681640625\n",
      "G loss: 14.93857192993164\n",
      "Iteration 201\n",
      "D loss: -40.98983383178711\n",
      "GP: 6.696552276611328\n",
      "Gradient norm: 1.155094027519226\n",
      "G loss: 16.988475799560547\n",
      "Iteration 251\n",
      "D loss: -41.66483688354492\n",
      "GP: 7.7425055503845215\n",
      "Gradient norm: 1.1953086853027344\n",
      "G loss: 15.877086639404297\n",
      "Iteration 301\n",
      "D loss: -41.67626953125\n",
      "GP: 6.560540676116943\n",
      "Gradient norm: 1.1504347324371338\n",
      "G loss: 14.991070747375488\n",
      "Iteration 351\n",
      "D loss: -41.66096496582031\n",
      "GP: 7.002908706665039\n",
      "Gradient norm: 1.1728883981704712\n",
      "G loss: 13.924774169921875\n",
      "\n",
      "Epoch 272\n",
      "Iteration 1\n",
      "D loss: -45.154232025146484\n",
      "GP: 6.3405938148498535\n",
      "Gradient norm: 1.173136591911316\n",
      "G loss: 13.257547378540039\n",
      "Iteration 51\n",
      "D loss: -42.32770538330078\n",
      "GP: 8.561540603637695\n",
      "Gradient norm: 1.210870623588562\n",
      "G loss: 14.808878898620605\n",
      "Iteration 101\n",
      "D loss: -43.279380798339844\n",
      "GP: 7.206124782562256\n",
      "Gradient norm: 1.1824795007705688\n",
      "G loss: 14.646768569946289\n",
      "Iteration 151\n",
      "D loss: -44.48545455932617\n",
      "GP: 6.975192546844482\n",
      "Gradient norm: 1.1786777973175049\n",
      "G loss: 14.489856719970703\n",
      "Iteration 201\n",
      "D loss: -41.06696701049805\n",
      "GP: 6.942761421203613\n",
      "Gradient norm: 1.1900399923324585\n",
      "G loss: 15.096579551696777\n",
      "Iteration 251\n",
      "D loss: -42.54767990112305\n",
      "GP: 6.318882942199707\n",
      "Gradient norm: 1.172943115234375\n",
      "G loss: 15.237092018127441\n",
      "Iteration 301\n",
      "D loss: -41.99906921386719\n",
      "GP: 8.008081436157227\n",
      "Gradient norm: 1.193705439567566\n",
      "G loss: 15.112467765808105\n",
      "Iteration 351\n",
      "D loss: -45.03611755371094\n",
      "GP: 8.448892593383789\n",
      "Gradient norm: 1.202345371246338\n",
      "G loss: 15.936501502990723\n",
      "\n",
      "Epoch 273\n",
      "Iteration 1\n",
      "D loss: -44.02043151855469\n",
      "GP: 7.5505218505859375\n",
      "Gradient norm: 1.180654525756836\n",
      "G loss: 14.461384773254395\n",
      "Iteration 51\n",
      "D loss: -40.72645568847656\n",
      "GP: 5.660647869110107\n",
      "Gradient norm: 1.149032711982727\n",
      "G loss: 14.823830604553223\n",
      "Iteration 101\n",
      "D loss: -41.02625274658203\n",
      "GP: 8.717080116271973\n",
      "Gradient norm: 1.2194784879684448\n",
      "G loss: 14.510980606079102\n",
      "Iteration 151\n",
      "D loss: -45.33374786376953\n",
      "GP: 6.996773719787598\n",
      "Gradient norm: 1.1935268640518188\n",
      "G loss: 15.397068977355957\n",
      "Iteration 201\n",
      "D loss: -42.3049430847168\n",
      "GP: 7.463555335998535\n",
      "Gradient norm: 1.1878407001495361\n",
      "G loss: 15.037304878234863\n",
      "Iteration 251\n",
      "D loss: -43.830345153808594\n",
      "GP: 8.311530113220215\n",
      "Gradient norm: 1.2038878202438354\n",
      "G loss: 15.581554412841797\n",
      "Iteration 301\n",
      "D loss: -41.73897933959961\n",
      "GP: 5.871295928955078\n",
      "Gradient norm: 1.1601593494415283\n",
      "G loss: 15.439401626586914\n",
      "Iteration 351\n",
      "D loss: -39.7593994140625\n",
      "GP: 7.098367691040039\n",
      "Gradient norm: 1.188644289970398\n",
      "G loss: 14.806380271911621\n",
      "\n",
      "Epoch 274\n",
      "Iteration 1\n",
      "D loss: -42.54323196411133\n",
      "GP: 7.66878604888916\n",
      "Gradient norm: 1.2011901140213013\n",
      "G loss: 15.683358192443848\n",
      "Iteration 51\n",
      "D loss: -44.21426773071289\n",
      "GP: 5.843317985534668\n",
      "Gradient norm: 1.1488381624221802\n",
      "G loss: 14.677085876464844\n",
      "Iteration 101\n",
      "D loss: -41.94823455810547\n",
      "GP: 7.612614631652832\n",
      "Gradient norm: 1.180360198020935\n",
      "G loss: 15.493802070617676\n",
      "Iteration 151\n",
      "D loss: -42.1094970703125\n",
      "GP: 6.54293155670166\n",
      "Gradient norm: 1.1692386865615845\n",
      "G loss: 16.0771484375\n",
      "Iteration 201\n",
      "D loss: -43.474632263183594\n",
      "GP: 8.979552268981934\n",
      "Gradient norm: 1.2030078172683716\n",
      "G loss: 16.019533157348633\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 251\n",
      "D loss: -41.94754409790039\n",
      "GP: 6.560173988342285\n",
      "Gradient norm: 1.1617141962051392\n",
      "G loss: 16.325254440307617\n",
      "Iteration 301\n",
      "D loss: -42.545684814453125\n",
      "GP: 7.971218109130859\n",
      "Gradient norm: 1.2033088207244873\n",
      "G loss: 15.831718444824219\n",
      "Iteration 351\n",
      "D loss: -41.726444244384766\n",
      "GP: 7.6156816482543945\n",
      "Gradient norm: 1.197064995765686\n",
      "G loss: 15.243773460388184\n",
      "\n",
      "Epoch 275\n",
      "Iteration 1\n",
      "D loss: -45.72600555419922\n",
      "GP: 5.808618545532227\n",
      "Gradient norm: 1.141538381576538\n",
      "G loss: 14.437936782836914\n",
      "Iteration 51\n",
      "D loss: -42.735137939453125\n",
      "GP: 7.68147087097168\n",
      "Gradient norm: 1.1950316429138184\n",
      "G loss: 13.554898262023926\n",
      "Iteration 101\n",
      "D loss: -41.72705841064453\n",
      "GP: 7.87423038482666\n",
      "Gradient norm: 1.206007719039917\n",
      "G loss: 15.241881370544434\n",
      "Iteration 151\n",
      "D loss: -45.210166931152344\n",
      "GP: 6.099676609039307\n",
      "Gradient norm: 1.1591460704803467\n",
      "G loss: 15.481915473937988\n",
      "Iteration 201\n",
      "D loss: -42.041656494140625\n",
      "GP: 7.514081954956055\n",
      "Gradient norm: 1.1972347497940063\n",
      "G loss: 16.489416122436523\n",
      "Iteration 251\n",
      "D loss: -44.34784698486328\n",
      "GP: 7.604806900024414\n",
      "Gradient norm: 1.1854751110076904\n",
      "G loss: 15.806151390075684\n",
      "Iteration 301\n",
      "D loss: -43.12671661376953\n",
      "GP: 5.601470470428467\n",
      "Gradient norm: 1.1498792171478271\n",
      "G loss: 15.864856719970703\n",
      "Iteration 351\n",
      "D loss: -41.29888916015625\n",
      "GP: 6.199766159057617\n",
      "Gradient norm: 1.1800317764282227\n",
      "G loss: 16.380552291870117\n",
      "\n",
      "Epoch 276\n",
      "Iteration 1\n",
      "D loss: -39.87910461425781\n",
      "GP: 7.442117691040039\n",
      "Gradient norm: 1.195898413658142\n",
      "G loss: 15.847827911376953\n",
      "Iteration 51\n",
      "D loss: -44.0390739440918\n",
      "GP: 5.68088960647583\n",
      "Gradient norm: 1.1504603624343872\n",
      "G loss: 15.469914436340332\n",
      "Iteration 101\n",
      "D loss: -41.026676177978516\n",
      "GP: 7.173043251037598\n",
      "Gradient norm: 1.162390947341919\n",
      "G loss: 14.630998611450195\n",
      "Iteration 151\n",
      "D loss: -43.886741638183594\n",
      "GP: 7.093924522399902\n",
      "Gradient norm: 1.191139578819275\n",
      "G loss: 14.29409408569336\n",
      "Iteration 201\n",
      "D loss: -43.95057678222656\n",
      "GP: 8.464512825012207\n",
      "Gradient norm: 1.2181023359298706\n",
      "G loss: 14.755017280578613\n",
      "Iteration 251\n",
      "D loss: -41.76942443847656\n",
      "GP: 7.175110816955566\n",
      "Gradient norm: 1.1695342063903809\n",
      "G loss: 15.807218551635742\n",
      "Iteration 301\n",
      "D loss: -41.92989730834961\n",
      "GP: 5.726565361022949\n",
      "Gradient norm: 1.1398836374282837\n",
      "G loss: 15.200030326843262\n",
      "Iteration 351\n",
      "D loss: -43.85249328613281\n",
      "GP: 5.059985160827637\n",
      "Gradient norm: 1.1407874822616577\n",
      "G loss: 15.97614574432373\n",
      "\n",
      "Epoch 277\n",
      "Iteration 1\n",
      "D loss: -43.3323974609375\n",
      "GP: 7.142596244812012\n",
      "Gradient norm: 1.1882435083389282\n",
      "G loss: 17.279367446899414\n",
      "Iteration 51\n",
      "D loss: -43.341468811035156\n",
      "GP: 6.96177864074707\n",
      "Gradient norm: 1.1778441667556763\n",
      "G loss: 14.785730361938477\n",
      "Iteration 101\n",
      "D loss: -44.120994567871094\n",
      "GP: 7.490320205688477\n",
      "Gradient norm: 1.2034112215042114\n",
      "G loss: 15.033390998840332\n",
      "Iteration 151\n",
      "D loss: -41.2205810546875\n",
      "GP: 6.081291198730469\n",
      "Gradient norm: 1.1507277488708496\n",
      "G loss: 13.385108947753906\n",
      "Iteration 201\n",
      "D loss: -43.77265167236328\n",
      "GP: 6.161855697631836\n",
      "Gradient norm: 1.1661171913146973\n",
      "G loss: 14.537492752075195\n",
      "Iteration 251\n",
      "D loss: -42.96370315551758\n",
      "GP: 7.320333003997803\n",
      "Gradient norm: 1.1903547048568726\n",
      "G loss: 14.908806800842285\n",
      "Iteration 301\n",
      "D loss: -41.18677520751953\n",
      "GP: 8.219133377075195\n",
      "Gradient norm: 1.195357084274292\n",
      "G loss: 14.559028625488281\n",
      "Iteration 351\n",
      "D loss: -43.569175720214844\n",
      "GP: 7.867319107055664\n",
      "Gradient norm: 1.1962165832519531\n",
      "G loss: 16.731494903564453\n",
      "\n",
      "Epoch 278\n",
      "Iteration 1\n",
      "D loss: -39.69811248779297\n",
      "GP: 5.195456027984619\n",
      "Gradient norm: 1.147120714187622\n",
      "G loss: 16.82459259033203\n",
      "Iteration 51\n",
      "D loss: -43.29191207885742\n",
      "GP: 5.960209846496582\n",
      "Gradient norm: 1.1560009717941284\n",
      "G loss: 16.46021842956543\n",
      "Iteration 101\n",
      "D loss: -42.9061393737793\n",
      "GP: 7.2451677322387695\n",
      "Gradient norm: 1.1829136610031128\n",
      "G loss: 16.0063533782959\n",
      "Iteration 151\n",
      "D loss: -42.55976104736328\n",
      "GP: 6.070085525512695\n",
      "Gradient norm: 1.1786011457443237\n",
      "G loss: 14.588473320007324\n",
      "Iteration 201\n",
      "D loss: -40.20965576171875\n",
      "GP: 7.708532333374023\n",
      "Gradient norm: 1.1952881813049316\n",
      "G loss: 14.068816184997559\n",
      "Iteration 251\n",
      "D loss: -43.214439392089844\n",
      "GP: 8.669780731201172\n",
      "Gradient norm: 1.2050472497940063\n",
      "G loss: 14.9074125289917\n",
      "Iteration 301\n",
      "D loss: -42.70165252685547\n",
      "GP: 6.056188583374023\n",
      "Gradient norm: 1.178787112236023\n",
      "G loss: 14.994641304016113\n",
      "Iteration 351\n",
      "D loss: -42.878082275390625\n",
      "GP: 7.269433975219727\n",
      "Gradient norm: 1.1749260425567627\n",
      "G loss: 15.571857452392578\n",
      "\n",
      "Epoch 279\n",
      "Iteration 1\n",
      "D loss: -42.120384216308594\n",
      "GP: 5.720044136047363\n",
      "Gradient norm: 1.1535495519638062\n",
      "G loss: 16.203062057495117\n",
      "Iteration 51\n",
      "D loss: -43.931846618652344\n",
      "GP: 7.476134300231934\n",
      "Gradient norm: 1.1943365335464478\n",
      "G loss: 16.993253707885742\n",
      "Iteration 101\n",
      "D loss: -44.68708419799805\n",
      "GP: 6.58262300491333\n",
      "Gradient norm: 1.1695561408996582\n",
      "G loss: 16.69196319580078\n",
      "Iteration 151\n",
      "D loss: -39.99774932861328\n",
      "GP: 9.513113021850586\n",
      "Gradient norm: 1.2209827899932861\n",
      "G loss: 15.452041625976562\n",
      "Iteration 201\n",
      "D loss: -42.786155700683594\n",
      "GP: 7.263952255249023\n",
      "Gradient norm: 1.181965947151184\n",
      "G loss: 15.793024063110352\n",
      "Iteration 251\n",
      "D loss: -42.509429931640625\n",
      "GP: 6.019028663635254\n",
      "Gradient norm: 1.167981743812561\n",
      "G loss: 16.216005325317383\n",
      "Iteration 301\n",
      "D loss: -42.64435958862305\n",
      "GP: 6.953572750091553\n",
      "Gradient norm: 1.1690961122512817\n",
      "G loss: 15.013864517211914\n",
      "Iteration 351\n",
      "D loss: -42.19788360595703\n",
      "GP: 9.037145614624023\n",
      "Gradient norm: 1.2052806615829468\n",
      "G loss: 16.342056274414062\n",
      "\n",
      "Epoch 280\n",
      "Iteration 1\n",
      "D loss: -41.87677764892578\n",
      "GP: 6.01362943649292\n",
      "Gradient norm: 1.157715916633606\n",
      "G loss: 15.774340629577637\n",
      "Iteration 51\n",
      "D loss: -42.01306915283203\n",
      "GP: 6.377531051635742\n",
      "Gradient norm: 1.1782878637313843\n",
      "G loss: 16.826589584350586\n",
      "Iteration 101\n",
      "D loss: -42.46607971191406\n",
      "GP: 8.266443252563477\n",
      "Gradient norm: 1.1913915872573853\n",
      "G loss: 15.738987922668457\n",
      "Iteration 151\n",
      "D loss: -41.698421478271484\n",
      "GP: 7.893360137939453\n",
      "Gradient norm: 1.1659351587295532\n",
      "G loss: 15.660815238952637\n",
      "Iteration 201\n",
      "D loss: -43.77587127685547\n",
      "GP: 6.839954853057861\n",
      "Gradient norm: 1.1680448055267334\n",
      "G loss: 15.026094436645508\n",
      "Iteration 251\n",
      "D loss: -42.5670280456543\n",
      "GP: 7.350986480712891\n",
      "Gradient norm: 1.1902539730072021\n",
      "G loss: 15.44632339477539\n",
      "Iteration 301\n",
      "D loss: -43.497615814208984\n",
      "GP: 5.987344264984131\n",
      "Gradient norm: 1.1555967330932617\n",
      "G loss: 15.161097526550293\n",
      "Iteration 351\n",
      "D loss: -43.76445770263672\n",
      "GP: 6.574258327484131\n",
      "Gradient norm: 1.1812697649002075\n",
      "G loss: 14.295604705810547\n",
      "\n",
      "Epoch 281\n",
      "Iteration 1\n",
      "D loss: -43.50694274902344\n",
      "GP: 6.569321155548096\n",
      "Gradient norm: 1.1944749355316162\n",
      "G loss: 15.552987098693848\n",
      "Iteration 51\n",
      "D loss: -42.76438903808594\n",
      "GP: 8.020355224609375\n",
      "Gradient norm: 1.1957950592041016\n",
      "G loss: 14.659832954406738\n",
      "Iteration 101\n",
      "D loss: -43.22854995727539\n",
      "GP: 5.2140092849731445\n",
      "Gradient norm: 1.1397333145141602\n",
      "G loss: 16.766921997070312\n",
      "Iteration 151\n",
      "D loss: -43.33357620239258\n",
      "GP: 6.916213035583496\n",
      "Gradient norm: 1.1723992824554443\n",
      "G loss: 16.456628799438477\n",
      "Iteration 201\n",
      "D loss: -43.160865783691406\n",
      "GP: 9.297157287597656\n",
      "Gradient norm: 1.2159992456436157\n",
      "G loss: 16.120464324951172\n",
      "Iteration 251\n",
      "D loss: -42.16327667236328\n",
      "GP: 5.503833293914795\n",
      "Gradient norm: 1.1568045616149902\n",
      "G loss: 15.678217887878418\n",
      "Iteration 301\n",
      "D loss: -42.90526580810547\n",
      "GP: 6.948055267333984\n",
      "Gradient norm: 1.1825823783874512\n",
      "G loss: 14.539011001586914\n",
      "Iteration 351\n",
      "D loss: -44.6451416015625\n",
      "GP: 7.596235275268555\n",
      "Gradient norm: 1.2090389728546143\n",
      "G loss: 15.443404197692871\n",
      "\n",
      "Epoch 282\n",
      "Iteration 1\n",
      "D loss: -40.731998443603516\n",
      "GP: 4.896407127380371\n",
      "Gradient norm: 1.1425447463989258\n",
      "G loss: 15.01260757446289\n",
      "Iteration 51\n",
      "D loss: -41.736297607421875\n",
      "GP: 6.942298889160156\n",
      "Gradient norm: 1.1814486980438232\n",
      "G loss: 15.327066421508789\n",
      "Iteration 101\n",
      "D loss: -41.01837921142578\n",
      "GP: 8.477193832397461\n",
      "Gradient norm: 1.2227938175201416\n",
      "G loss: 15.699063301086426\n",
      "Iteration 151\n",
      "D loss: -41.03314208984375\n",
      "GP: 8.77938175201416\n",
      "Gradient norm: 1.2161840200424194\n",
      "G loss: 15.331823348999023\n",
      "Iteration 201\n",
      "D loss: -44.18258285522461\n",
      "GP: 7.58010196685791\n",
      "Gradient norm: 1.1895649433135986\n",
      "G loss: 15.955909729003906\n",
      "Iteration 251\n",
      "D loss: -40.58207321166992\n",
      "GP: 7.877179145812988\n",
      "Gradient norm: 1.1890827417373657\n",
      "G loss: 15.688272476196289\n",
      "Iteration 301\n",
      "D loss: -40.79619598388672\n",
      "GP: 6.747308731079102\n",
      "Gradient norm: 1.1862719058990479\n",
      "G loss: 14.668266296386719\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 351\n",
      "D loss: -43.332550048828125\n",
      "GP: 7.641301155090332\n",
      "Gradient norm: 1.1952316761016846\n",
      "G loss: 15.016880989074707\n",
      "\n",
      "Epoch 283\n",
      "Iteration 1\n",
      "D loss: -43.85664367675781\n",
      "GP: 7.85399055480957\n",
      "Gradient norm: 1.2115987539291382\n",
      "G loss: 16.381656646728516\n",
      "Iteration 51\n",
      "D loss: -43.17751693725586\n",
      "GP: 7.8343048095703125\n",
      "Gradient norm: 1.192366361618042\n",
      "G loss: 15.00597858428955\n",
      "Iteration 101\n",
      "D loss: -42.62339401245117\n",
      "GP: 5.593357086181641\n",
      "Gradient norm: 1.1553162336349487\n",
      "G loss: 16.49304962158203\n",
      "Iteration 151\n",
      "D loss: -44.8233642578125\n",
      "GP: 6.9164605140686035\n",
      "Gradient norm: 1.1772077083587646\n",
      "G loss: 15.647258758544922\n",
      "Iteration 201\n",
      "D loss: -42.105194091796875\n",
      "GP: 7.586920261383057\n",
      "Gradient norm: 1.1781343221664429\n",
      "G loss: 16.10114860534668\n",
      "Iteration 251\n",
      "D loss: -41.8641471862793\n",
      "GP: 5.489903450012207\n",
      "Gradient norm: 1.1638792753219604\n",
      "G loss: 16.530792236328125\n",
      "Iteration 301\n",
      "D loss: -42.44974899291992\n",
      "GP: 5.954685211181641\n",
      "Gradient norm: 1.159765601158142\n",
      "G loss: 15.731534004211426\n",
      "Iteration 351\n",
      "D loss: -40.85036087036133\n",
      "GP: 7.210444450378418\n",
      "Gradient norm: 1.160656452178955\n",
      "G loss: 15.240038871765137\n",
      "\n",
      "Epoch 284\n",
      "Iteration 1\n",
      "D loss: -44.25672149658203\n",
      "GP: 7.335872650146484\n",
      "Gradient norm: 1.1862986087799072\n",
      "G loss: 15.209975242614746\n",
      "Iteration 51\n",
      "D loss: -44.90937805175781\n",
      "GP: 6.083703517913818\n",
      "Gradient norm: 1.1645983457565308\n",
      "G loss: 16.062868118286133\n",
      "Iteration 101\n",
      "D loss: -41.728065490722656\n",
      "GP: 7.622885704040527\n",
      "Gradient norm: 1.1951955556869507\n",
      "G loss: 16.651716232299805\n",
      "Iteration 151\n",
      "D loss: -43.05136489868164\n",
      "GP: 6.422520160675049\n",
      "Gradient norm: 1.1555678844451904\n",
      "G loss: 16.163644790649414\n",
      "Iteration 201\n",
      "D loss: -43.295745849609375\n",
      "GP: 6.412281036376953\n",
      "Gradient norm: 1.1620795726776123\n",
      "G loss: 14.910382270812988\n",
      "Iteration 251\n",
      "D loss: -41.94217300415039\n",
      "GP: 7.288327693939209\n",
      "Gradient norm: 1.1833348274230957\n",
      "G loss: 16.144166946411133\n",
      "Iteration 301\n",
      "D loss: -41.208717346191406\n",
      "GP: 6.396044731140137\n",
      "Gradient norm: 1.1722644567489624\n",
      "G loss: 15.115551948547363\n",
      "Iteration 351\n",
      "D loss: -42.907859802246094\n",
      "GP: 6.910486221313477\n",
      "Gradient norm: 1.1787469387054443\n",
      "G loss: 15.252570152282715\n",
      "\n",
      "Epoch 285\n",
      "Iteration 1\n",
      "D loss: -42.80799865722656\n",
      "GP: 7.546331882476807\n",
      "Gradient norm: 1.191743016242981\n",
      "G loss: 15.223393440246582\n",
      "Iteration 51\n",
      "D loss: -43.80478286743164\n",
      "GP: 6.343360900878906\n",
      "Gradient norm: 1.1579546928405762\n",
      "G loss: 14.885217666625977\n",
      "Iteration 101\n",
      "D loss: -44.35717010498047\n",
      "GP: 8.8177490234375\n",
      "Gradient norm: 1.2078522443771362\n",
      "G loss: 15.822951316833496\n",
      "Iteration 151\n",
      "D loss: -41.587581634521484\n",
      "GP: 4.982982635498047\n",
      "Gradient norm: 1.143002986907959\n",
      "G loss: 14.892881393432617\n",
      "Iteration 201\n",
      "D loss: -40.81869888305664\n",
      "GP: 5.929971218109131\n",
      "Gradient norm: 1.1548453569412231\n",
      "G loss: 15.990400314331055\n",
      "Iteration 251\n",
      "D loss: -42.816795349121094\n",
      "GP: 7.705026626586914\n",
      "Gradient norm: 1.196132779121399\n",
      "G loss: 15.3483304977417\n",
      "Iteration 301\n",
      "D loss: -42.60691452026367\n",
      "GP: 5.962155342102051\n",
      "Gradient norm: 1.159671664237976\n",
      "G loss: 15.826342582702637\n",
      "Iteration 351\n",
      "D loss: -40.1549186706543\n",
      "GP: 7.191696643829346\n",
      "Gradient norm: 1.1809667348861694\n",
      "G loss: 14.497379302978516\n",
      "\n",
      "Epoch 286\n",
      "Iteration 1\n",
      "D loss: -42.295509338378906\n",
      "GP: 7.320600509643555\n",
      "Gradient norm: 1.182775616645813\n",
      "G loss: 15.825225830078125\n",
      "Iteration 51\n",
      "D loss: -42.401039123535156\n",
      "GP: 7.255898475646973\n",
      "Gradient norm: 1.1897716522216797\n",
      "G loss: 16.40825843811035\n",
      "Iteration 101\n",
      "D loss: -44.52410888671875\n",
      "GP: 6.7488813400268555\n",
      "Gradient norm: 1.1713653802871704\n",
      "G loss: 14.084426879882812\n",
      "Iteration 151\n",
      "D loss: -41.0142822265625\n",
      "GP: 5.873422145843506\n",
      "Gradient norm: 1.153240442276001\n",
      "G loss: 16.477474212646484\n",
      "Iteration 201\n",
      "D loss: -44.32524871826172\n",
      "GP: 6.513920783996582\n",
      "Gradient norm: 1.1695021390914917\n",
      "G loss: 15.964765548706055\n",
      "Iteration 251\n",
      "D loss: -39.82344436645508\n",
      "GP: 6.411591053009033\n",
      "Gradient norm: 1.1548848152160645\n",
      "G loss: 14.293911933898926\n",
      "Iteration 301\n",
      "D loss: -41.9196891784668\n",
      "GP: 6.797348976135254\n",
      "Gradient norm: 1.1776493787765503\n",
      "G loss: 15.241458892822266\n",
      "Iteration 351\n",
      "D loss: -42.37538146972656\n",
      "GP: 6.353255271911621\n",
      "Gradient norm: 1.164642572402954\n",
      "G loss: 15.683629989624023\n",
      "\n",
      "Epoch 287\n",
      "Iteration 1\n",
      "D loss: -44.04324722290039\n",
      "GP: 6.857516765594482\n",
      "Gradient norm: 1.1846805810928345\n",
      "G loss: 15.341779708862305\n",
      "Iteration 51\n",
      "D loss: -41.850502014160156\n",
      "GP: 7.157769680023193\n",
      "Gradient norm: 1.1876965761184692\n",
      "G loss: 14.822440147399902\n",
      "Iteration 101\n",
      "D loss: -39.275604248046875\n",
      "GP: 8.003313064575195\n",
      "Gradient norm: 1.1865214109420776\n",
      "G loss: 15.261735916137695\n",
      "Iteration 151\n",
      "D loss: -43.19502639770508\n",
      "GP: 7.813575744628906\n",
      "Gradient norm: 1.1935968399047852\n",
      "G loss: 15.028717994689941\n",
      "Iteration 201\n",
      "D loss: -42.21790313720703\n",
      "GP: 6.310689926147461\n",
      "Gradient norm: 1.1719059944152832\n",
      "G loss: 15.610148429870605\n",
      "Iteration 251\n",
      "D loss: -45.78610610961914\n",
      "GP: 7.853287696838379\n",
      "Gradient norm: 1.197389006614685\n",
      "G loss: 14.568707466125488\n",
      "Iteration 301\n",
      "D loss: -39.7423095703125\n",
      "GP: 6.104765892028809\n",
      "Gradient norm: 1.1560245752334595\n",
      "G loss: 14.913960456848145\n",
      "Iteration 351\n",
      "D loss: -41.70045852661133\n",
      "GP: 6.495440483093262\n",
      "Gradient norm: 1.1731758117675781\n",
      "G loss: 14.961494445800781\n",
      "\n",
      "Epoch 288\n",
      "Iteration 1\n",
      "D loss: -44.55854415893555\n",
      "GP: 7.725123405456543\n",
      "Gradient norm: 1.1908788681030273\n",
      "G loss: 14.453729629516602\n",
      "Iteration 51\n",
      "D loss: -43.24756622314453\n",
      "GP: 8.839234352111816\n",
      "Gradient norm: 1.1957606077194214\n",
      "G loss: 15.284952163696289\n",
      "Iteration 101\n",
      "D loss: -41.6232795715332\n",
      "GP: 8.146023750305176\n",
      "Gradient norm: 1.2014780044555664\n",
      "G loss: 15.482927322387695\n",
      "Iteration 151\n",
      "D loss: -42.988365173339844\n",
      "GP: 7.157968521118164\n",
      "Gradient norm: 1.178348422050476\n",
      "G loss: 13.976539611816406\n",
      "Iteration 201\n",
      "D loss: -43.06243896484375\n",
      "GP: 7.675595283508301\n",
      "Gradient norm: 1.1784145832061768\n",
      "G loss: 15.255393981933594\n",
      "Iteration 251\n",
      "D loss: -41.073787689208984\n",
      "GP: 6.993841648101807\n",
      "Gradient norm: 1.1757313013076782\n",
      "G loss: 14.715335845947266\n",
      "Iteration 301\n",
      "D loss: -42.72492980957031\n",
      "GP: 7.812033653259277\n",
      "Gradient norm: 1.1807739734649658\n",
      "G loss: 15.21712589263916\n",
      "Iteration 351\n",
      "D loss: -43.46724319458008\n",
      "GP: 7.3019561767578125\n",
      "Gradient norm: 1.1848942041397095\n",
      "G loss: 15.898785591125488\n",
      "\n",
      "Epoch 289\n",
      "Iteration 1\n",
      "D loss: -43.145328521728516\n",
      "GP: 6.656040191650391\n",
      "Gradient norm: 1.1649688482284546\n",
      "G loss: 15.071091651916504\n",
      "Iteration 51\n",
      "D loss: -40.59327697753906\n",
      "GP: 8.327127456665039\n",
      "Gradient norm: 1.193215012550354\n",
      "G loss: 14.480070114135742\n",
      "Iteration 101\n",
      "D loss: -40.2418098449707\n",
      "GP: 6.489808559417725\n",
      "Gradient norm: 1.1608846187591553\n",
      "G loss: 13.759634017944336\n",
      "Iteration 151\n",
      "D loss: -44.26045227050781\n",
      "GP: 8.0646333694458\n",
      "Gradient norm: 1.1980966329574585\n",
      "G loss: 14.891927719116211\n",
      "Iteration 201\n",
      "D loss: -42.71886444091797\n",
      "GP: 6.886110305786133\n",
      "Gradient norm: 1.1659979820251465\n",
      "G loss: 13.642952919006348\n",
      "Iteration 251\n",
      "D loss: -40.66370391845703\n",
      "GP: 5.410583019256592\n",
      "Gradient norm: 1.13533353805542\n",
      "G loss: 14.622931480407715\n",
      "Iteration 301\n",
      "D loss: -40.998252868652344\n",
      "GP: 6.570948600769043\n",
      "Gradient norm: 1.1504331827163696\n",
      "G loss: 14.384528160095215\n",
      "Iteration 351\n",
      "D loss: -41.88816833496094\n",
      "GP: 7.256950378417969\n",
      "Gradient norm: 1.1812655925750732\n",
      "G loss: 15.005351066589355\n",
      "\n",
      "Epoch 290\n",
      "Iteration 1\n",
      "D loss: -44.914581298828125\n",
      "GP: 6.392965316772461\n",
      "Gradient norm: 1.1681842803955078\n",
      "G loss: 14.450766563415527\n",
      "Iteration 51\n",
      "D loss: -44.13822555541992\n",
      "GP: 7.7323126792907715\n",
      "Gradient norm: 1.1850615739822388\n",
      "G loss: 15.740659713745117\n",
      "Iteration 101\n",
      "D loss: -44.717247009277344\n",
      "GP: 7.171952724456787\n",
      "Gradient norm: 1.1759147644042969\n",
      "G loss: 14.698331832885742\n",
      "Iteration 151\n",
      "D loss: -41.1534423828125\n",
      "GP: 6.846296310424805\n",
      "Gradient norm: 1.1857731342315674\n",
      "G loss: 15.035452842712402\n",
      "Iteration 201\n",
      "D loss: -42.97047424316406\n",
      "GP: 6.095542907714844\n",
      "Gradient norm: 1.170805811882019\n",
      "G loss: 15.739265441894531\n",
      "Iteration 251\n",
      "D loss: -42.83312225341797\n",
      "GP: 6.896830081939697\n",
      "Gradient norm: 1.1702196598052979\n",
      "G loss: 14.662312507629395\n",
      "Iteration 301\n",
      "D loss: -45.4207649230957\n",
      "GP: 7.31914758682251\n",
      "Gradient norm: 1.1822824478149414\n",
      "G loss: 14.260269165039062\n",
      "Iteration 351\n",
      "D loss: -43.690467834472656\n",
      "GP: 7.083402633666992\n",
      "Gradient norm: 1.156365990638733\n",
      "G loss: 15.7805814743042\n",
      "\n",
      "Epoch 291\n",
      "Iteration 1\n",
      "D loss: -42.658199310302734\n",
      "GP: 5.185534954071045\n",
      "Gradient norm: 1.1379060745239258\n",
      "G loss: 13.947760581970215\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 51\n",
      "D loss: -42.03729248046875\n",
      "GP: 5.707266330718994\n",
      "Gradient norm: 1.1609559059143066\n",
      "G loss: 15.049317359924316\n",
      "Iteration 101\n",
      "D loss: -40.483341217041016\n",
      "GP: 7.469112396240234\n",
      "Gradient norm: 1.199824571609497\n",
      "G loss: 14.912612915039062\n",
      "Iteration 151\n",
      "D loss: -43.98025131225586\n",
      "GP: 6.375463008880615\n",
      "Gradient norm: 1.1520804166793823\n",
      "G loss: 14.454839706420898\n",
      "Iteration 201\n",
      "D loss: -40.4051399230957\n",
      "GP: 6.833057403564453\n",
      "Gradient norm: 1.1792620420455933\n",
      "G loss: 16.533811569213867\n",
      "Iteration 251\n",
      "D loss: -41.30098342895508\n",
      "GP: 6.135644435882568\n",
      "Gradient norm: 1.1653536558151245\n",
      "G loss: 16.034698486328125\n",
      "Iteration 301\n",
      "D loss: -40.22223663330078\n",
      "GP: 5.945189476013184\n",
      "Gradient norm: 1.1301511526107788\n",
      "G loss: 16.05400848388672\n",
      "Iteration 351\n",
      "D loss: -43.476322174072266\n",
      "GP: 6.525203704833984\n",
      "Gradient norm: 1.1701632738113403\n",
      "G loss: 15.317209243774414\n",
      "\n",
      "Epoch 292\n",
      "Iteration 1\n",
      "D loss: -43.97296142578125\n",
      "GP: 5.414041996002197\n",
      "Gradient norm: 1.141632080078125\n",
      "G loss: 15.853673934936523\n",
      "Iteration 51\n",
      "D loss: -42.508216857910156\n",
      "GP: 7.511295318603516\n",
      "Gradient norm: 1.1907069683074951\n",
      "G loss: 15.63710880279541\n",
      "Iteration 101\n",
      "D loss: -43.208560943603516\n",
      "GP: 8.532305717468262\n",
      "Gradient norm: 1.2116179466247559\n",
      "G loss: 15.008979797363281\n",
      "Iteration 151\n",
      "D loss: -41.64094924926758\n",
      "GP: 6.22611141204834\n",
      "Gradient norm: 1.1636172533035278\n",
      "G loss: 15.386981964111328\n",
      "Iteration 201\n",
      "D loss: -43.05833435058594\n",
      "GP: 5.393148422241211\n",
      "Gradient norm: 1.1562776565551758\n",
      "G loss: 15.375016212463379\n",
      "Iteration 251\n",
      "D loss: -43.62340545654297\n",
      "GP: 6.574273586273193\n",
      "Gradient norm: 1.1770824193954468\n",
      "G loss: 16.088104248046875\n",
      "Iteration 301\n",
      "D loss: -44.934871673583984\n",
      "GP: 6.552669525146484\n",
      "Gradient norm: 1.187559962272644\n",
      "G loss: 15.833169937133789\n",
      "Iteration 351\n",
      "D loss: -43.23243713378906\n",
      "GP: 5.725324630737305\n",
      "Gradient norm: 1.1474530696868896\n",
      "G loss: 14.70162296295166\n",
      "\n",
      "Epoch 293\n",
      "Iteration 1\n",
      "D loss: -43.3812255859375\n",
      "GP: 6.8036699295043945\n",
      "Gradient norm: 1.185187816619873\n",
      "G loss: 14.33101749420166\n",
      "Iteration 51\n",
      "D loss: -43.81376266479492\n",
      "GP: 6.218142032623291\n",
      "Gradient norm: 1.1604825258255005\n",
      "G loss: 15.739837646484375\n",
      "Iteration 101\n",
      "D loss: -41.69908142089844\n",
      "GP: 6.591019153594971\n",
      "Gradient norm: 1.1560367345809937\n",
      "G loss: 15.600658416748047\n",
      "Iteration 151\n",
      "D loss: -42.79852294921875\n",
      "GP: 6.529114246368408\n",
      "Gradient norm: 1.1717077493667603\n",
      "G loss: 15.189387321472168\n",
      "Iteration 201\n",
      "D loss: -41.350990295410156\n",
      "GP: 7.760756015777588\n",
      "Gradient norm: 1.201586365699768\n",
      "G loss: 15.368478775024414\n",
      "Iteration 251\n",
      "D loss: -43.29627990722656\n",
      "GP: 6.652209758758545\n",
      "Gradient norm: 1.1580685377120972\n",
      "G loss: 13.826237678527832\n",
      "Iteration 301\n",
      "D loss: -42.88220977783203\n",
      "GP: 5.275359153747559\n",
      "Gradient norm: 1.1282297372817993\n",
      "G loss: 15.254436492919922\n",
      "Iteration 351\n",
      "D loss: -40.405364990234375\n",
      "GP: 6.312538146972656\n",
      "Gradient norm: 1.1476911306381226\n",
      "G loss: 14.85498046875\n",
      "\n",
      "Epoch 294\n",
      "Iteration 1\n",
      "D loss: -43.185123443603516\n",
      "GP: 7.608528137207031\n",
      "Gradient norm: 1.1918079853057861\n",
      "G loss: 14.21259593963623\n",
      "Iteration 51\n",
      "D loss: -42.9044189453125\n",
      "GP: 6.508206844329834\n",
      "Gradient norm: 1.1675946712493896\n",
      "G loss: 14.674257278442383\n",
      "Iteration 101\n",
      "D loss: -43.26877212524414\n",
      "GP: 6.095311641693115\n",
      "Gradient norm: 1.1798540353775024\n",
      "G loss: 14.828492164611816\n",
      "Iteration 151\n",
      "D loss: -41.21709060668945\n",
      "GP: 8.521991729736328\n",
      "Gradient norm: 1.2079225778579712\n",
      "G loss: 15.96088981628418\n",
      "Iteration 201\n",
      "D loss: -42.48704147338867\n",
      "GP: 7.347765922546387\n",
      "Gradient norm: 1.166988730430603\n",
      "G loss: 15.678878784179688\n",
      "Iteration 251\n",
      "D loss: -42.64624786376953\n",
      "GP: 7.2087907791137695\n",
      "Gradient norm: 1.1894758939743042\n",
      "G loss: 15.136070251464844\n",
      "Iteration 301\n",
      "D loss: -41.86537170410156\n",
      "GP: 6.995000839233398\n",
      "Gradient norm: 1.1642261743545532\n",
      "G loss: 16.38987159729004\n",
      "Iteration 351\n",
      "D loss: -41.73133850097656\n",
      "GP: 6.278345108032227\n",
      "Gradient norm: 1.1627299785614014\n",
      "G loss: 14.35496711730957\n",
      "\n",
      "Epoch 295\n",
      "Iteration 1\n",
      "D loss: -40.38668441772461\n",
      "GP: 5.697264671325684\n",
      "Gradient norm: 1.1347323656082153\n",
      "G loss: 13.718321800231934\n",
      "Iteration 51\n",
      "D loss: -43.10459518432617\n",
      "GP: 7.997951507568359\n",
      "Gradient norm: 1.2070881128311157\n",
      "G loss: 15.133477210998535\n",
      "Iteration 101\n",
      "D loss: -45.11552047729492\n",
      "GP: 6.309162139892578\n",
      "Gradient norm: 1.159196376800537\n",
      "G loss: 16.282901763916016\n",
      "Iteration 151\n",
      "D loss: -42.2686767578125\n",
      "GP: 7.4154767990112305\n",
      "Gradient norm: 1.1968300342559814\n",
      "G loss: 16.244794845581055\n",
      "Iteration 201\n",
      "D loss: -43.348350524902344\n",
      "GP: 6.765399932861328\n",
      "Gradient norm: 1.1635594367980957\n",
      "G loss: 15.167820930480957\n",
      "Iteration 251\n",
      "D loss: -42.38759994506836\n",
      "GP: 6.347849369049072\n",
      "Gradient norm: 1.1748830080032349\n",
      "G loss: 14.877439498901367\n",
      "Iteration 301\n",
      "D loss: -42.77467346191406\n",
      "GP: 6.264213562011719\n",
      "Gradient norm: 1.17128586769104\n",
      "G loss: 13.690817832946777\n",
      "Iteration 351\n",
      "D loss: -42.095375061035156\n",
      "GP: 6.667649745941162\n",
      "Gradient norm: 1.169248104095459\n",
      "G loss: 15.267358779907227\n",
      "\n",
      "Epoch 296\n",
      "Iteration 1\n",
      "D loss: -43.427276611328125\n",
      "GP: 7.252474784851074\n",
      "Gradient norm: 1.178382158279419\n",
      "G loss: 15.314138412475586\n",
      "Iteration 51\n",
      "D loss: -43.048126220703125\n",
      "GP: 6.403594493865967\n",
      "Gradient norm: 1.1813609600067139\n",
      "G loss: 15.994579315185547\n",
      "Iteration 101\n",
      "D loss: -43.7894287109375\n",
      "GP: 7.374129295349121\n",
      "Gradient norm: 1.2047653198242188\n",
      "G loss: 16.453683853149414\n",
      "Iteration 151\n",
      "D loss: -43.653263092041016\n",
      "GP: 7.084953308105469\n",
      "Gradient norm: 1.168839931488037\n",
      "G loss: 15.294432640075684\n",
      "Iteration 201\n",
      "D loss: -42.81487274169922\n",
      "GP: 6.131864547729492\n",
      "Gradient norm: 1.1546103954315186\n",
      "G loss: 15.322954177856445\n",
      "Iteration 251\n",
      "D loss: -43.073734283447266\n",
      "GP: 6.654691219329834\n",
      "Gradient norm: 1.1641875505447388\n",
      "G loss: 14.237524032592773\n",
      "Iteration 301\n",
      "D loss: -42.200679779052734\n",
      "GP: 7.2442121505737305\n",
      "Gradient norm: 1.17877197265625\n",
      "G loss: 14.847840309143066\n",
      "Iteration 351\n",
      "D loss: -41.38780975341797\n",
      "GP: 7.151815891265869\n",
      "Gradient norm: 1.1784356832504272\n",
      "G loss: 15.006114959716797\n",
      "\n",
      "Epoch 297\n",
      "Iteration 1\n",
      "D loss: -42.483097076416016\n",
      "GP: 5.468510627746582\n",
      "Gradient norm: 1.1518332958221436\n",
      "G loss: 15.55437183380127\n",
      "Iteration 51\n",
      "D loss: -45.561119079589844\n",
      "GP: 6.710973739624023\n",
      "Gradient norm: 1.166415810585022\n",
      "G loss: 14.594711303710938\n",
      "Iteration 101\n",
      "D loss: -41.79330825805664\n",
      "GP: 9.589420318603516\n",
      "Gradient norm: 1.231649398803711\n",
      "G loss: 15.834813117980957\n",
      "Iteration 151\n",
      "D loss: -43.96771240234375\n",
      "GP: 7.150132179260254\n",
      "Gradient norm: 1.1858855485916138\n",
      "G loss: 14.569096565246582\n",
      "Iteration 201\n",
      "D loss: -41.79127502441406\n",
      "GP: 7.779169082641602\n",
      "Gradient norm: 1.194461464881897\n",
      "G loss: 14.920331954956055\n",
      "Iteration 251\n",
      "D loss: -42.15447998046875\n",
      "GP: 6.897861957550049\n",
      "Gradient norm: 1.1749361753463745\n",
      "G loss: 15.38911247253418\n",
      "Iteration 301\n",
      "D loss: -43.234107971191406\n",
      "GP: 8.087685585021973\n",
      "Gradient norm: 1.1980303525924683\n",
      "G loss: 15.396186828613281\n",
      "Iteration 351\n",
      "D loss: -43.6264762878418\n",
      "GP: 7.514523983001709\n",
      "Gradient norm: 1.1751761436462402\n",
      "G loss: 14.772038459777832\n",
      "\n",
      "Epoch 298\n",
      "Iteration 1\n",
      "D loss: -40.21883010864258\n",
      "GP: 5.5601701736450195\n",
      "Gradient norm: 1.1268672943115234\n",
      "G loss: 15.47457504272461\n",
      "Iteration 51\n",
      "D loss: -44.403892517089844\n",
      "GP: 5.720000743865967\n",
      "Gradient norm: 1.1641288995742798\n",
      "G loss: 16.401031494140625\n",
      "Iteration 101\n",
      "D loss: -42.18831253051758\n",
      "GP: 6.6754584312438965\n",
      "Gradient norm: 1.171921968460083\n",
      "G loss: 15.379563331604004\n",
      "Iteration 151\n",
      "D loss: -40.524871826171875\n",
      "GP: 7.032423496246338\n",
      "Gradient norm: 1.177230715751648\n",
      "G loss: 16.413301467895508\n",
      "Iteration 201\n",
      "D loss: -43.4793586730957\n",
      "GP: 7.912725925445557\n",
      "Gradient norm: 1.2015661001205444\n",
      "G loss: 13.988274574279785\n",
      "Iteration 251\n",
      "D loss: -44.206626892089844\n",
      "GP: 5.181730270385742\n",
      "Gradient norm: 1.1455209255218506\n",
      "G loss: 16.1983585357666\n",
      "Iteration 301\n",
      "D loss: -42.76374053955078\n",
      "GP: 5.478740692138672\n",
      "Gradient norm: 1.139137864112854\n",
      "G loss: 13.324698448181152\n",
      "Iteration 351\n",
      "D loss: -44.15989685058594\n",
      "GP: 6.905209541320801\n",
      "Gradient norm: 1.171097993850708\n",
      "G loss: 14.3558349609375\n",
      "\n",
      "Epoch 299\n",
      "Iteration 1\n",
      "D loss: -43.945106506347656\n",
      "GP: 7.281133651733398\n",
      "Gradient norm: 1.1534100770950317\n",
      "G loss: 14.777206420898438\n",
      "Iteration 51\n",
      "D loss: -40.69863510131836\n",
      "GP: 5.798367977142334\n",
      "Gradient norm: 1.144349455833435\n",
      "G loss: 16.7938289642334\n",
      "Iteration 101\n",
      "D loss: -42.20576477050781\n",
      "GP: 5.305548191070557\n",
      "Gradient norm: 1.1591734886169434\n",
      "G loss: 15.438179016113281\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 151\n",
      "D loss: -42.28717803955078\n",
      "GP: 6.913317680358887\n",
      "Gradient norm: 1.1849315166473389\n",
      "G loss: 15.12977409362793\n",
      "Iteration 201\n",
      "D loss: -41.58648681640625\n",
      "GP: 6.068403720855713\n",
      "Gradient norm: 1.1589131355285645\n",
      "G loss: 16.135868072509766\n",
      "Iteration 251\n",
      "D loss: -43.388427734375\n",
      "GP: 6.162868499755859\n",
      "Gradient norm: 1.1746506690979004\n",
      "G loss: 15.206389427185059\n",
      "Iteration 301\n",
      "D loss: -43.209190368652344\n",
      "GP: 6.586469650268555\n",
      "Gradient norm: 1.1906607151031494\n",
      "G loss: 15.27439022064209\n",
      "Iteration 351\n",
      "D loss: -41.94281768798828\n",
      "GP: 7.157535076141357\n",
      "Gradient norm: 1.184496521949768\n",
      "G loss: 13.843328475952148\n",
      "\n",
      "Epoch 300\n",
      "Iteration 1\n",
      "D loss: -41.617828369140625\n",
      "GP: 6.488767147064209\n",
      "Gradient norm: 1.1469502449035645\n",
      "G loss: 15.238831520080566\n",
      "Iteration 51\n",
      "D loss: -45.588443756103516\n",
      "GP: 5.933089733123779\n",
      "Gradient norm: 1.1427028179168701\n",
      "G loss: 14.553888320922852\n",
      "Iteration 101\n",
      "D loss: -41.41157531738281\n",
      "GP: 7.2517242431640625\n",
      "Gradient norm: 1.1607108116149902\n",
      "G loss: 15.129729270935059\n",
      "Iteration 151\n",
      "D loss: -41.693382263183594\n",
      "GP: 7.39222526550293\n",
      "Gradient norm: 1.1633058786392212\n",
      "G loss: 14.730974197387695\n",
      "Iteration 201\n",
      "D loss: -45.03553771972656\n",
      "GP: 7.362814903259277\n",
      "Gradient norm: 1.1874758005142212\n",
      "G loss: 13.933690071105957\n",
      "Iteration 251\n",
      "D loss: -41.181121826171875\n",
      "GP: 4.266112327575684\n",
      "Gradient norm: 1.1212213039398193\n",
      "G loss: 14.729883193969727\n",
      "Iteration 301\n",
      "D loss: -44.74660873413086\n",
      "GP: 7.291236400604248\n",
      "Gradient norm: 1.158890724182129\n",
      "G loss: 14.613032341003418\n",
      "Iteration 351\n",
      "D loss: -43.00525665283203\n",
      "GP: 6.16497278213501\n",
      "Gradient norm: 1.1630098819732666\n",
      "G loss: 14.673897743225098\n",
      "\n",
      "Epoch 301\n",
      "Iteration 1\n",
      "D loss: -42.89694595336914\n",
      "GP: 6.85100793838501\n",
      "Gradient norm: 1.1869423389434814\n",
      "G loss: 15.846878051757812\n",
      "Iteration 51\n",
      "D loss: -41.01152801513672\n",
      "GP: 8.020822525024414\n",
      "Gradient norm: 1.1924598217010498\n",
      "G loss: 13.10733413696289\n",
      "Iteration 101\n",
      "D loss: -43.863582611083984\n",
      "GP: 7.119841575622559\n",
      "Gradient norm: 1.1669036149978638\n",
      "G loss: 15.925987243652344\n",
      "Iteration 151\n",
      "D loss: -43.07469940185547\n",
      "GP: 6.281295299530029\n",
      "Gradient norm: 1.1587597131729126\n",
      "G loss: 14.579011917114258\n",
      "Iteration 201\n",
      "D loss: -43.62101364135742\n",
      "GP: 7.336092948913574\n",
      "Gradient norm: 1.1942640542984009\n",
      "G loss: 14.21196174621582\n",
      "Iteration 251\n",
      "D loss: -42.156333923339844\n",
      "GP: 6.561895370483398\n",
      "Gradient norm: 1.1499948501586914\n",
      "G loss: 14.000509262084961\n",
      "Iteration 301\n",
      "D loss: -41.260154724121094\n",
      "GP: 6.490016937255859\n",
      "Gradient norm: 1.1639186143875122\n",
      "G loss: 14.537618637084961\n",
      "Iteration 351\n",
      "D loss: -42.976715087890625\n",
      "GP: 6.3589324951171875\n",
      "Gradient norm: 1.1668554544448853\n",
      "G loss: 15.633861541748047\n",
      "\n",
      "Epoch 302\n",
      "Iteration 1\n",
      "D loss: -44.61214065551758\n",
      "GP: 6.952510833740234\n",
      "Gradient norm: 1.1698942184448242\n",
      "G loss: 15.586718559265137\n",
      "Iteration 51\n",
      "D loss: -43.79045867919922\n",
      "GP: 7.3333234786987305\n",
      "Gradient norm: 1.1804238557815552\n",
      "G loss: 14.963860511779785\n",
      "Iteration 101\n",
      "D loss: -42.64967346191406\n",
      "GP: 6.167282581329346\n",
      "Gradient norm: 1.1347049474716187\n",
      "G loss: 14.693084716796875\n",
      "Iteration 151\n",
      "D loss: -43.788448333740234\n",
      "GP: 6.380503177642822\n",
      "Gradient norm: 1.1552428007125854\n",
      "G loss: 15.69432544708252\n",
      "Iteration 201\n",
      "D loss: -41.34904861450195\n",
      "GP: 4.980776786804199\n",
      "Gradient norm: 1.1311875581741333\n",
      "G loss: 15.128788948059082\n",
      "Iteration 251\n",
      "D loss: -40.21298599243164\n",
      "GP: 7.299221038818359\n",
      "Gradient norm: 1.1953914165496826\n",
      "G loss: 14.8319673538208\n",
      "Iteration 301\n",
      "D loss: -40.828582763671875\n",
      "GP: 5.065608978271484\n",
      "Gradient norm: 1.1240030527114868\n",
      "G loss: 14.162631034851074\n",
      "Iteration 351\n",
      "D loss: -41.6788330078125\n",
      "GP: 8.394014358520508\n",
      "Gradient norm: 1.2180790901184082\n",
      "G loss: 14.615228652954102\n",
      "\n",
      "Epoch 303\n",
      "Iteration 1\n",
      "D loss: -44.33024978637695\n",
      "GP: 5.983960151672363\n",
      "Gradient norm: 1.1624363660812378\n",
      "G loss: 14.858919143676758\n",
      "Iteration 51\n",
      "D loss: -44.222599029541016\n",
      "GP: 7.866850852966309\n",
      "Gradient norm: 1.1878200769424438\n",
      "G loss: 16.154705047607422\n",
      "Iteration 101\n",
      "D loss: -42.125701904296875\n",
      "GP: 5.775883674621582\n",
      "Gradient norm: 1.1702454090118408\n",
      "G loss: 14.854484558105469\n",
      "Iteration 151\n",
      "D loss: -44.494537353515625\n",
      "GP: 7.368743419647217\n",
      "Gradient norm: 1.1682168245315552\n",
      "G loss: 15.891071319580078\n",
      "Iteration 201\n",
      "D loss: -42.232337951660156\n",
      "GP: 6.53843879699707\n",
      "Gradient norm: 1.1713826656341553\n",
      "G loss: 14.781834602355957\n",
      "Iteration 251\n",
      "D loss: -41.91521453857422\n",
      "GP: 5.273116111755371\n",
      "Gradient norm: 1.1336851119995117\n",
      "G loss: 14.538811683654785\n",
      "Iteration 301\n",
      "D loss: -41.04148483276367\n",
      "GP: 8.08013916015625\n",
      "Gradient norm: 1.2032359838485718\n",
      "G loss: 13.292937278747559\n",
      "Iteration 351\n",
      "D loss: -43.52289581298828\n",
      "GP: 6.673295497894287\n",
      "Gradient norm: 1.1800562143325806\n",
      "G loss: 14.503125190734863\n",
      "\n",
      "Epoch 304\n",
      "Iteration 1\n",
      "D loss: -41.436927795410156\n",
      "GP: 5.8838653564453125\n",
      "Gradient norm: 1.1671489477157593\n",
      "G loss: 14.309333801269531\n",
      "Iteration 51\n",
      "D loss: -45.185665130615234\n",
      "GP: 6.6796040534973145\n",
      "Gradient norm: 1.1673860549926758\n",
      "G loss: 13.651750564575195\n",
      "Iteration 101\n",
      "D loss: -43.38719177246094\n",
      "GP: 7.175323963165283\n",
      "Gradient norm: 1.1820094585418701\n",
      "G loss: 15.102251052856445\n",
      "Iteration 151\n",
      "D loss: -41.38883972167969\n",
      "GP: 5.649607181549072\n",
      "Gradient norm: 1.1408796310424805\n",
      "G loss: 14.762460708618164\n",
      "Iteration 201\n",
      "D loss: -42.49746322631836\n",
      "GP: 6.007580280303955\n",
      "Gradient norm: 1.144691824913025\n",
      "G loss: 14.68569278717041\n",
      "Iteration 251\n",
      "D loss: -44.2530517578125\n",
      "GP: 6.186240196228027\n",
      "Gradient norm: 1.148457646369934\n",
      "G loss: 14.872989654541016\n",
      "Iteration 301\n",
      "D loss: -42.15181350708008\n",
      "GP: 8.58379077911377\n",
      "Gradient norm: 1.2090585231781006\n",
      "G loss: 14.932533264160156\n",
      "Iteration 351\n",
      "D loss: -42.13454055786133\n",
      "GP: 6.542033672332764\n",
      "Gradient norm: 1.1788597106933594\n",
      "G loss: 15.640604019165039\n",
      "\n",
      "Epoch 305\n",
      "Iteration 1\n",
      "D loss: -42.53288269042969\n",
      "GP: 7.343221664428711\n",
      "Gradient norm: 1.1914564371109009\n",
      "G loss: 13.673052787780762\n",
      "Iteration 51\n",
      "D loss: -41.91705322265625\n",
      "GP: 6.742738723754883\n",
      "Gradient norm: 1.1903016567230225\n",
      "G loss: 14.232659339904785\n",
      "Iteration 101\n",
      "D loss: -44.07616424560547\n",
      "GP: 8.883124351501465\n",
      "Gradient norm: 1.2214388847351074\n",
      "G loss: 14.313061714172363\n",
      "Iteration 151\n",
      "D loss: -41.73832702636719\n",
      "GP: 6.682534217834473\n",
      "Gradient norm: 1.1584748029708862\n",
      "G loss: 15.401768684387207\n",
      "Iteration 201\n",
      "D loss: -42.121788024902344\n",
      "GP: 7.82170295715332\n",
      "Gradient norm: 1.2181460857391357\n",
      "G loss: 14.041271209716797\n",
      "Iteration 251\n",
      "D loss: -40.94854736328125\n",
      "GP: 7.180194854736328\n",
      "Gradient norm: 1.1686317920684814\n",
      "G loss: 14.787164688110352\n",
      "Iteration 301\n",
      "D loss: -42.0247917175293\n",
      "GP: 6.171827793121338\n",
      "Gradient norm: 1.1552001237869263\n",
      "G loss: 15.278882026672363\n",
      "Iteration 351\n",
      "D loss: -44.52713394165039\n",
      "GP: 7.9349565505981445\n",
      "Gradient norm: 1.2074034214019775\n",
      "G loss: 14.490218162536621\n",
      "\n",
      "Epoch 306\n",
      "Iteration 1\n",
      "D loss: -42.494850158691406\n",
      "GP: 7.014829635620117\n",
      "Gradient norm: 1.1714599132537842\n",
      "G loss: 13.492453575134277\n",
      "Iteration 51\n",
      "D loss: -44.03430938720703\n",
      "GP: 7.939249038696289\n",
      "Gradient norm: 1.1934901475906372\n",
      "G loss: 14.23182487487793\n",
      "Iteration 101\n",
      "D loss: -39.27983474731445\n",
      "GP: 4.749927043914795\n",
      "Gradient norm: 1.1402945518493652\n",
      "G loss: 15.01863956451416\n",
      "Iteration 151\n",
      "D loss: -45.12258529663086\n",
      "GP: 7.6895246505737305\n",
      "Gradient norm: 1.1943573951721191\n",
      "G loss: 13.717650413513184\n",
      "Iteration 201\n",
      "D loss: -40.07853698730469\n",
      "GP: 9.534704208374023\n",
      "Gradient norm: 1.2276561260223389\n",
      "G loss: 15.20124340057373\n",
      "Iteration 251\n",
      "D loss: -42.29936218261719\n",
      "GP: 6.77022647857666\n",
      "Gradient norm: 1.189008116722107\n",
      "G loss: 14.316563606262207\n",
      "Iteration 301\n",
      "D loss: -43.829097747802734\n",
      "GP: 6.81347131729126\n",
      "Gradient norm: 1.1963915824890137\n",
      "G loss: 15.104249000549316\n",
      "Iteration 351\n",
      "D loss: -40.64347839355469\n",
      "GP: 7.488515853881836\n",
      "Gradient norm: 1.186753749847412\n",
      "G loss: 15.315088272094727\n",
      "\n",
      "Epoch 307\n",
      "Iteration 1\n",
      "D loss: -43.414794921875\n",
      "GP: 6.833906173706055\n",
      "Gradient norm: 1.1771339178085327\n",
      "G loss: 16.077774047851562\n",
      "Iteration 51\n",
      "D loss: -45.367427825927734\n",
      "GP: 7.192096710205078\n",
      "Gradient norm: 1.1722662448883057\n",
      "G loss: 13.902238845825195\n",
      "Iteration 101\n",
      "D loss: -41.44364929199219\n",
      "GP: 5.4236555099487305\n",
      "Gradient norm: 1.1692472696304321\n",
      "G loss: 15.449647903442383\n",
      "Iteration 151\n",
      "D loss: -43.610408782958984\n",
      "GP: 5.952973365783691\n",
      "Gradient norm: 1.1635807752609253\n",
      "G loss: 14.405075073242188\n",
      "Iteration 201\n",
      "D loss: -42.282936096191406\n",
      "GP: 7.490177154541016\n",
      "Gradient norm: 1.1793036460876465\n",
      "G loss: 14.460381507873535\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 251\n",
      "D loss: -43.49728775024414\n",
      "GP: 6.282994270324707\n",
      "Gradient norm: 1.1557822227478027\n",
      "G loss: 14.371079444885254\n",
      "Iteration 301\n",
      "D loss: -40.849422454833984\n",
      "GP: 7.305950164794922\n",
      "Gradient norm: 1.183656096458435\n",
      "G loss: 14.893842697143555\n",
      "Iteration 351\n",
      "D loss: -44.820743560791016\n",
      "GP: 7.934311866760254\n",
      "Gradient norm: 1.1941053867340088\n",
      "G loss: 15.819924354553223\n",
      "\n",
      "Epoch 308\n",
      "Iteration 1\n",
      "D loss: -44.354248046875\n",
      "GP: 7.223108291625977\n",
      "Gradient norm: 1.1778062582015991\n",
      "G loss: 14.916484832763672\n",
      "Iteration 51\n",
      "D loss: -44.616798400878906\n",
      "GP: 8.252628326416016\n",
      "Gradient norm: 1.2135109901428223\n",
      "G loss: 14.798463821411133\n",
      "Iteration 101\n",
      "D loss: -41.6249885559082\n",
      "GP: 6.800928592681885\n",
      "Gradient norm: 1.1714491844177246\n",
      "G loss: 15.447291374206543\n",
      "Iteration 151\n",
      "D loss: -41.45792770385742\n",
      "GP: 4.792254447937012\n",
      "Gradient norm: 1.129277229309082\n",
      "G loss: 15.885818481445312\n",
      "Iteration 201\n",
      "D loss: -41.69736862182617\n",
      "GP: 6.464781761169434\n",
      "Gradient norm: 1.1499284505844116\n",
      "G loss: 14.966992378234863\n",
      "Iteration 251\n",
      "D loss: -42.410789489746094\n",
      "GP: 7.417998313903809\n",
      "Gradient norm: 1.2042697668075562\n",
      "G loss: 15.05614948272705\n",
      "Iteration 301\n",
      "D loss: -43.920249938964844\n",
      "GP: 7.893847942352295\n",
      "Gradient norm: 1.1959495544433594\n",
      "G loss: 15.447574615478516\n",
      "Iteration 351\n",
      "D loss: -41.53217697143555\n",
      "GP: 7.673349857330322\n",
      "Gradient norm: 1.1885310411453247\n",
      "G loss: 15.137520790100098\n",
      "\n",
      "Epoch 309\n",
      "Iteration 1\n",
      "D loss: -43.45221710205078\n",
      "GP: 5.9909515380859375\n",
      "Gradient norm: 1.1406296491622925\n",
      "G loss: 14.76169204711914\n",
      "Iteration 51\n",
      "D loss: -42.90718460083008\n",
      "GP: 6.634151458740234\n",
      "Gradient norm: 1.1786330938339233\n",
      "G loss: 15.260019302368164\n",
      "Iteration 101\n",
      "D loss: -41.5836181640625\n",
      "GP: 7.311093330383301\n",
      "Gradient norm: 1.1749662160873413\n",
      "G loss: 15.569128036499023\n",
      "Iteration 151\n",
      "D loss: -42.375755310058594\n",
      "GP: 6.056452751159668\n",
      "Gradient norm: 1.1672911643981934\n",
      "G loss: 14.228161811828613\n",
      "Iteration 201\n",
      "D loss: -43.19425964355469\n",
      "GP: 6.224242210388184\n",
      "Gradient norm: 1.1705446243286133\n",
      "G loss: 14.71983528137207\n",
      "Iteration 251\n",
      "D loss: -43.57956314086914\n",
      "GP: 6.697604179382324\n",
      "Gradient norm: 1.191426157951355\n",
      "G loss: 14.080560684204102\n",
      "Iteration 301\n",
      "D loss: -43.012454986572266\n",
      "GP: 7.30233907699585\n",
      "Gradient norm: 1.168552041053772\n",
      "G loss: 13.525555610656738\n",
      "Iteration 351\n",
      "D loss: -40.928489685058594\n",
      "GP: 6.439083099365234\n",
      "Gradient norm: 1.1879085302352905\n",
      "G loss: 14.122655868530273\n",
      "\n",
      "Epoch 310\n",
      "Iteration 1\n",
      "D loss: -46.091556549072266\n",
      "GP: 6.850048065185547\n",
      "Gradient norm: 1.1825450658798218\n",
      "G loss: 15.671586036682129\n",
      "Iteration 51\n",
      "D loss: -39.22615051269531\n",
      "GP: 5.575689315795898\n",
      "Gradient norm: 1.144038438796997\n",
      "G loss: 16.104398727416992\n",
      "Iteration 101\n",
      "D loss: -43.426090240478516\n",
      "GP: 8.31236743927002\n",
      "Gradient norm: 1.2099249362945557\n",
      "G loss: 14.870877265930176\n",
      "Iteration 151\n",
      "D loss: -42.7846794128418\n",
      "GP: 7.037601470947266\n",
      "Gradient norm: 1.1862125396728516\n",
      "G loss: 15.286120414733887\n",
      "Iteration 201\n",
      "D loss: -42.73326110839844\n",
      "GP: 6.635490417480469\n",
      "Gradient norm: 1.1537578105926514\n",
      "G loss: 15.421527862548828\n",
      "Iteration 251\n",
      "D loss: -40.96296310424805\n",
      "GP: 6.316620826721191\n",
      "Gradient norm: 1.161927580833435\n",
      "G loss: 16.046005249023438\n",
      "Iteration 301\n",
      "D loss: -43.605125427246094\n",
      "GP: 7.421955585479736\n",
      "Gradient norm: 1.1936264038085938\n",
      "G loss: 16.532039642333984\n",
      "Iteration 351\n",
      "D loss: -41.65531921386719\n",
      "GP: 4.594243049621582\n",
      "Gradient norm: 1.1411951780319214\n",
      "G loss: 15.456124305725098\n",
      "\n",
      "Epoch 311\n",
      "Iteration 1\n",
      "D loss: -43.06875228881836\n",
      "GP: 7.971087455749512\n",
      "Gradient norm: 1.2016172409057617\n",
      "G loss: 14.888141632080078\n",
      "Iteration 51\n",
      "D loss: -45.05248260498047\n",
      "GP: 7.3641510009765625\n",
      "Gradient norm: 1.1910738945007324\n",
      "G loss: 14.842571258544922\n",
      "Iteration 101\n",
      "D loss: -40.554710388183594\n",
      "GP: 8.567304611206055\n",
      "Gradient norm: 1.2200093269348145\n",
      "G loss: 14.834709167480469\n",
      "Iteration 151\n",
      "D loss: -42.32831954956055\n",
      "GP: 7.846218109130859\n",
      "Gradient norm: 1.1788854598999023\n",
      "G loss: 15.764198303222656\n",
      "Iteration 201\n",
      "D loss: -42.37541198730469\n",
      "GP: 5.318750381469727\n",
      "Gradient norm: 1.148455023765564\n",
      "G loss: 15.944578170776367\n",
      "Iteration 251\n",
      "D loss: -43.66326141357422\n",
      "GP: 5.892374038696289\n",
      "Gradient norm: 1.1476306915283203\n",
      "G loss: 16.066377639770508\n",
      "Iteration 301\n",
      "D loss: -43.55168914794922\n",
      "GP: 7.499092102050781\n",
      "Gradient norm: 1.182917833328247\n",
      "G loss: 14.616029739379883\n",
      "Iteration 351\n",
      "D loss: -41.20917510986328\n",
      "GP: 6.763894557952881\n",
      "Gradient norm: 1.1857404708862305\n",
      "G loss: 14.253199577331543\n",
      "\n",
      "Epoch 312\n",
      "Iteration 1\n",
      "D loss: -43.32917022705078\n",
      "GP: 6.570352554321289\n",
      "Gradient norm: 1.1746859550476074\n",
      "G loss: 15.632859230041504\n",
      "Iteration 51\n",
      "D loss: -42.528507232666016\n",
      "GP: 7.0842766761779785\n",
      "Gradient norm: 1.194130539894104\n",
      "G loss: 14.943733215332031\n",
      "Iteration 101\n",
      "D loss: -40.69838333129883\n",
      "GP: 8.061264991760254\n",
      "Gradient norm: 1.190016508102417\n",
      "G loss: 13.61748218536377\n",
      "Iteration 151\n",
      "D loss: -45.78408432006836\n",
      "GP: 6.677096366882324\n",
      "Gradient norm: 1.1756162643432617\n",
      "G loss: 15.545766830444336\n",
      "Iteration 201\n",
      "D loss: -41.07461166381836\n",
      "GP: 6.263911724090576\n",
      "Gradient norm: 1.1721348762512207\n",
      "G loss: 14.985629081726074\n",
      "Iteration 251\n",
      "D loss: -41.75934600830078\n",
      "GP: 6.951480388641357\n",
      "Gradient norm: 1.180741310119629\n",
      "G loss: 14.40364933013916\n",
      "Iteration 301\n",
      "D loss: -44.062320709228516\n",
      "GP: 5.75079870223999\n",
      "Gradient norm: 1.1526579856872559\n",
      "G loss: 14.19758129119873\n",
      "Iteration 351\n",
      "D loss: -42.63335418701172\n",
      "GP: 6.955257415771484\n",
      "Gradient norm: 1.1886109113693237\n",
      "G loss: 14.028890609741211\n",
      "\n",
      "Epoch 313\n",
      "Iteration 1\n",
      "D loss: -40.623470306396484\n",
      "GP: 6.577601909637451\n",
      "Gradient norm: 1.1827785968780518\n",
      "G loss: 14.140145301818848\n",
      "Iteration 51\n",
      "D loss: -42.11857223510742\n",
      "GP: 6.5278143882751465\n",
      "Gradient norm: 1.174026370048523\n",
      "G loss: 14.266531944274902\n",
      "Iteration 101\n",
      "D loss: -40.88331985473633\n",
      "GP: 7.540423393249512\n",
      "Gradient norm: 1.1934248208999634\n",
      "G loss: 14.453316688537598\n",
      "Iteration 151\n",
      "D loss: -42.05296325683594\n",
      "GP: 6.329947471618652\n",
      "Gradient norm: 1.1516622304916382\n",
      "G loss: 14.71752643585205\n",
      "Iteration 201\n",
      "D loss: -42.51980209350586\n",
      "GP: 7.481875896453857\n",
      "Gradient norm: 1.1763699054718018\n",
      "G loss: 14.087723731994629\n",
      "Iteration 251\n",
      "D loss: -43.903907775878906\n",
      "GP: 7.695364952087402\n",
      "Gradient norm: 1.1939119100570679\n",
      "G loss: 13.829158782958984\n",
      "Iteration 301\n",
      "D loss: -40.3472785949707\n",
      "GP: 6.433487892150879\n",
      "Gradient norm: 1.1670640707015991\n",
      "G loss: 13.057558059692383\n",
      "Iteration 351\n",
      "D loss: -43.631370544433594\n",
      "GP: 6.176154613494873\n",
      "Gradient norm: 1.156855583190918\n",
      "G loss: 13.991188049316406\n",
      "\n",
      "Epoch 314\n",
      "Iteration 1\n",
      "D loss: -43.5960807800293\n",
      "GP: 6.773216724395752\n",
      "Gradient norm: 1.1577972173690796\n",
      "G loss: 13.97993278503418\n",
      "Iteration 51\n",
      "D loss: -41.44260787963867\n",
      "GP: 4.891972064971924\n",
      "Gradient norm: 1.1476165056228638\n",
      "G loss: 13.848408699035645\n",
      "Iteration 101\n",
      "D loss: -42.76531219482422\n",
      "GP: 5.496519088745117\n",
      "Gradient norm: 1.1480237245559692\n",
      "G loss: 14.589431762695312\n",
      "Iteration 151\n",
      "D loss: -42.682621002197266\n",
      "GP: 6.715946197509766\n",
      "Gradient norm: 1.1720741987228394\n",
      "G loss: 15.31002426147461\n",
      "Iteration 201\n",
      "D loss: -43.32353591918945\n",
      "GP: 6.805994510650635\n",
      "Gradient norm: 1.1685365438461304\n",
      "G loss: 14.02954387664795\n",
      "Iteration 251\n",
      "D loss: -44.100433349609375\n",
      "GP: 6.619876861572266\n",
      "Gradient norm: 1.1853394508361816\n",
      "G loss: 15.175451278686523\n",
      "Iteration 301\n",
      "D loss: -41.62285232543945\n",
      "GP: 6.280242919921875\n",
      "Gradient norm: 1.1767700910568237\n",
      "G loss: 15.688641548156738\n",
      "Iteration 351\n",
      "D loss: -44.1109619140625\n",
      "GP: 7.0490875244140625\n",
      "Gradient norm: 1.1699581146240234\n",
      "G loss: 13.482787132263184\n",
      "\n",
      "Epoch 315\n",
      "Iteration 1\n",
      "D loss: -42.01688003540039\n",
      "GP: 8.420181274414062\n",
      "Gradient norm: 1.18647301197052\n",
      "G loss: 14.445977210998535\n",
      "Iteration 51\n",
      "D loss: -44.043025970458984\n",
      "GP: 4.451800346374512\n",
      "Gradient norm: 1.1122992038726807\n",
      "G loss: 13.724273681640625\n",
      "Iteration 101\n",
      "D loss: -41.51715850830078\n",
      "GP: 5.894492149353027\n",
      "Gradient norm: 1.1574081182479858\n",
      "G loss: 14.442206382751465\n",
      "Iteration 151\n",
      "D loss: -42.474491119384766\n",
      "GP: 5.087158679962158\n",
      "Gradient norm: 1.1474677324295044\n",
      "G loss: 12.526816368103027\n",
      "Iteration 201\n",
      "D loss: -42.16082763671875\n",
      "GP: 8.023261070251465\n",
      "Gradient norm: 1.195777177810669\n",
      "G loss: 13.687026977539062\n",
      "Iteration 251\n",
      "D loss: -41.430118560791016\n",
      "GP: 8.649432182312012\n",
      "Gradient norm: 1.2093251943588257\n",
      "G loss: 14.797165870666504\n",
      "Iteration 301\n",
      "D loss: -41.2796630859375\n",
      "GP: 8.285187721252441\n",
      "Gradient norm: 1.1937777996063232\n",
      "G loss: 15.158500671386719\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 351\n",
      "D loss: -43.47230529785156\n",
      "GP: 6.00343132019043\n",
      "Gradient norm: 1.1533619165420532\n",
      "G loss: 14.19708251953125\n",
      "\n",
      "Epoch 316\n",
      "Iteration 1\n",
      "D loss: -41.58208465576172\n",
      "GP: 5.729331016540527\n",
      "Gradient norm: 1.1451222896575928\n",
      "G loss: 14.925904273986816\n",
      "Iteration 51\n",
      "D loss: -41.28083801269531\n",
      "GP: 5.584778785705566\n",
      "Gradient norm: 1.1644090414047241\n",
      "G loss: 13.488426208496094\n",
      "Iteration 101\n",
      "D loss: -43.46903991699219\n",
      "GP: 7.980356216430664\n",
      "Gradient norm: 1.1982450485229492\n",
      "G loss: 14.840719223022461\n",
      "Iteration 151\n",
      "D loss: -43.67188262939453\n",
      "GP: 8.904825210571289\n",
      "Gradient norm: 1.226051926612854\n",
      "G loss: 14.251545906066895\n",
      "Iteration 201\n",
      "D loss: -43.738739013671875\n",
      "GP: 8.209792137145996\n",
      "Gradient norm: 1.2118092775344849\n",
      "G loss: 12.956719398498535\n",
      "Iteration 251\n",
      "D loss: -43.0279655456543\n",
      "GP: 7.187864780426025\n",
      "Gradient norm: 1.1795697212219238\n",
      "G loss: 13.574797630310059\n",
      "Iteration 301\n",
      "D loss: -43.27953338623047\n",
      "GP: 8.370405197143555\n",
      "Gradient norm: 1.194297194480896\n",
      "G loss: 14.34000301361084\n",
      "Iteration 351\n",
      "D loss: -41.487159729003906\n",
      "GP: 7.542375564575195\n",
      "Gradient norm: 1.184714436531067\n",
      "G loss: 15.012337684631348\n",
      "\n",
      "Epoch 317\n",
      "Iteration 1\n",
      "D loss: -44.98196792602539\n",
      "GP: 8.189167022705078\n",
      "Gradient norm: 1.2172235250473022\n",
      "G loss: 13.333301544189453\n",
      "Iteration 51\n",
      "D loss: -41.01184844970703\n",
      "GP: 6.751269817352295\n",
      "Gradient norm: 1.1779042482376099\n",
      "G loss: 14.27109146118164\n",
      "Iteration 101\n",
      "D loss: -42.45207977294922\n",
      "GP: 8.18130874633789\n",
      "Gradient norm: 1.198103666305542\n",
      "G loss: 14.49053955078125\n",
      "Iteration 151\n",
      "D loss: -43.94232177734375\n",
      "GP: 7.4890971183776855\n",
      "Gradient norm: 1.1946022510528564\n",
      "G loss: 13.010443687438965\n",
      "Iteration 201\n",
      "D loss: -42.1868782043457\n",
      "GP: 8.505130767822266\n",
      "Gradient norm: 1.2072187662124634\n",
      "G loss: 14.561118125915527\n",
      "Iteration 251\n",
      "D loss: -44.45970916748047\n",
      "GP: 4.696233749389648\n",
      "Gradient norm: 1.123213291168213\n",
      "G loss: 15.472369194030762\n",
      "Iteration 301\n",
      "D loss: -41.06167984008789\n",
      "GP: 5.815884590148926\n",
      "Gradient norm: 1.1615790128707886\n",
      "G loss: 14.492862701416016\n",
      "Iteration 351\n",
      "D loss: -44.865257263183594\n",
      "GP: 7.260891914367676\n",
      "Gradient norm: 1.1832518577575684\n",
      "G loss: 14.198939323425293\n",
      "\n",
      "Epoch 318\n",
      "Iteration 1\n",
      "D loss: -42.14274597167969\n",
      "GP: 6.887472152709961\n",
      "Gradient norm: 1.1901679039001465\n",
      "G loss: 14.832222938537598\n",
      "Iteration 51\n",
      "D loss: -42.952293395996094\n",
      "GP: 7.27385139465332\n",
      "Gradient norm: 1.176562786102295\n",
      "G loss: 14.686993598937988\n",
      "Iteration 101\n",
      "D loss: -42.30868148803711\n",
      "GP: 6.133608341217041\n",
      "Gradient norm: 1.1519098281860352\n",
      "G loss: 13.765021324157715\n",
      "Iteration 151\n",
      "D loss: -45.80162811279297\n",
      "GP: 7.028388500213623\n",
      "Gradient norm: 1.1790885925292969\n",
      "G loss: 13.880943298339844\n",
      "Iteration 201\n",
      "D loss: -41.45013427734375\n",
      "GP: 6.190605163574219\n",
      "Gradient norm: 1.1627751588821411\n",
      "G loss: 15.980552673339844\n",
      "Iteration 251\n",
      "D loss: -43.41536331176758\n",
      "GP: 8.048230171203613\n",
      "Gradient norm: 1.2045296430587769\n",
      "G loss: 13.399605751037598\n",
      "Iteration 301\n",
      "D loss: -44.16838455200195\n",
      "GP: 7.126960277557373\n",
      "Gradient norm: 1.190577507019043\n",
      "G loss: 14.541568756103516\n",
      "Iteration 351\n",
      "D loss: -43.05112075805664\n",
      "GP: 6.390469551086426\n",
      "Gradient norm: 1.1528761386871338\n",
      "G loss: 15.263666152954102\n",
      "\n",
      "Epoch 319\n",
      "Iteration 1\n",
      "D loss: -44.05376052856445\n",
      "GP: 6.349025726318359\n",
      "Gradient norm: 1.1642252206802368\n",
      "G loss: 14.03031063079834\n",
      "Iteration 51\n",
      "D loss: -44.6790885925293\n",
      "GP: 7.4344282150268555\n",
      "Gradient norm: 1.1762237548828125\n",
      "G loss: 14.616501808166504\n",
      "Iteration 101\n",
      "D loss: -42.525230407714844\n",
      "GP: 6.231866836547852\n",
      "Gradient norm: 1.1733447313308716\n",
      "G loss: 15.139237403869629\n",
      "Iteration 151\n",
      "D loss: -41.69367599487305\n",
      "GP: 7.722375392913818\n",
      "Gradient norm: 1.1881505250930786\n",
      "G loss: 14.288664817810059\n",
      "Iteration 201\n",
      "D loss: -45.27058410644531\n",
      "GP: 7.636466026306152\n",
      "Gradient norm: 1.1801912784576416\n",
      "G loss: 15.24722957611084\n",
      "Iteration 251\n",
      "D loss: -42.82728576660156\n",
      "GP: 7.418806076049805\n",
      "Gradient norm: 1.1977391242980957\n",
      "G loss: 13.679618835449219\n",
      "Iteration 301\n",
      "D loss: -42.93141174316406\n",
      "GP: 5.239148139953613\n",
      "Gradient norm: 1.159821629524231\n",
      "G loss: 14.181502342224121\n",
      "Iteration 351\n",
      "D loss: -43.04947280883789\n",
      "GP: 6.364433765411377\n",
      "Gradient norm: 1.1712814569473267\n",
      "G loss: 15.74346923828125\n",
      "\n",
      "Epoch 320\n",
      "Iteration 1\n",
      "D loss: -42.654510498046875\n",
      "GP: 7.841498374938965\n",
      "Gradient norm: 1.1856725215911865\n",
      "G loss: 14.367833137512207\n",
      "Iteration 51\n",
      "D loss: -43.38836669921875\n",
      "GP: 7.491384506225586\n",
      "Gradient norm: 1.1891597509384155\n",
      "G loss: 13.98646068572998\n",
      "Iteration 101\n",
      "D loss: -41.883785247802734\n",
      "GP: 7.1682209968566895\n",
      "Gradient norm: 1.181536078453064\n",
      "G loss: 13.810492515563965\n",
      "Iteration 151\n",
      "D loss: -44.18033218383789\n",
      "GP: 8.247559547424316\n",
      "Gradient norm: 1.1966265439987183\n",
      "G loss: 13.993043899536133\n",
      "Iteration 201\n",
      "D loss: -40.32211685180664\n",
      "GP: 6.249627113342285\n",
      "Gradient norm: 1.162065029144287\n",
      "G loss: 14.702108383178711\n",
      "Iteration 251\n",
      "D loss: -40.264869689941406\n",
      "GP: 7.839803695678711\n",
      "Gradient norm: 1.1923807859420776\n",
      "G loss: 14.681974411010742\n",
      "Iteration 301\n",
      "D loss: -46.9409294128418\n",
      "GP: 7.163416862487793\n",
      "Gradient norm: 1.179430365562439\n",
      "G loss: 14.446602821350098\n",
      "Iteration 351\n",
      "D loss: -43.18016815185547\n",
      "GP: 7.539718151092529\n",
      "Gradient norm: 1.1992919445037842\n",
      "G loss: 14.956925392150879\n",
      "\n",
      "Epoch 321\n",
      "Iteration 1\n",
      "D loss: -42.69816970825195\n",
      "GP: 7.7635955810546875\n",
      "Gradient norm: 1.1989065408706665\n",
      "G loss: 14.72085189819336\n",
      "Iteration 51\n",
      "D loss: -39.710330963134766\n",
      "GP: 7.100063323974609\n",
      "Gradient norm: 1.1759964227676392\n",
      "G loss: 14.52261734008789\n",
      "Iteration 101\n",
      "D loss: -41.70794677734375\n",
      "GP: 7.974531650543213\n",
      "Gradient norm: 1.1903016567230225\n",
      "G loss: 14.92273235321045\n",
      "Iteration 151\n",
      "D loss: -42.49272155761719\n",
      "GP: 6.933858394622803\n",
      "Gradient norm: 1.1666665077209473\n",
      "G loss: 14.414510726928711\n",
      "Iteration 201\n",
      "D loss: -41.77013397216797\n",
      "GP: 6.991141319274902\n",
      "Gradient norm: 1.1857683658599854\n",
      "G loss: 14.301087379455566\n",
      "Iteration 251\n",
      "D loss: -42.595924377441406\n",
      "GP: 7.759803771972656\n",
      "Gradient norm: 1.1829302310943604\n",
      "G loss: 15.143604278564453\n",
      "Iteration 301\n",
      "D loss: -42.51047134399414\n",
      "GP: 8.633079528808594\n",
      "Gradient norm: 1.2116044759750366\n",
      "G loss: 16.19639778137207\n",
      "Iteration 351\n",
      "D loss: -40.832313537597656\n",
      "GP: 6.03394889831543\n",
      "Gradient norm: 1.1639043092727661\n",
      "G loss: 15.82960033416748\n",
      "\n",
      "Epoch 322\n",
      "Iteration 1\n",
      "D loss: -42.18879699707031\n",
      "GP: 6.813386917114258\n",
      "Gradient norm: 1.1872146129608154\n",
      "G loss: 14.511480331420898\n",
      "Iteration 51\n",
      "D loss: -41.8375244140625\n",
      "GP: 4.7608442306518555\n",
      "Gradient norm: 1.1417909860610962\n",
      "G loss: 13.398599624633789\n",
      "Iteration 101\n",
      "D loss: -43.38087463378906\n",
      "GP: 6.780523300170898\n",
      "Gradient norm: 1.1629390716552734\n",
      "G loss: 14.557755470275879\n",
      "Iteration 151\n",
      "D loss: -41.9316291809082\n",
      "GP: 7.918919563293457\n",
      "Gradient norm: 1.1937817335128784\n",
      "G loss: 14.249534606933594\n",
      "Iteration 201\n",
      "D loss: -42.403221130371094\n",
      "GP: 7.020030975341797\n",
      "Gradient norm: 1.1793944835662842\n",
      "G loss: 14.179447174072266\n",
      "Iteration 251\n",
      "D loss: -42.31894302368164\n",
      "GP: 6.147506237030029\n",
      "Gradient norm: 1.1576182842254639\n",
      "G loss: 14.01995849609375\n",
      "Iteration 301\n",
      "D loss: -41.7628173828125\n",
      "GP: 7.597439289093018\n",
      "Gradient norm: 1.1701664924621582\n",
      "G loss: 14.750608444213867\n",
      "Iteration 351\n",
      "D loss: -42.93244171142578\n",
      "GP: 6.065131187438965\n",
      "Gradient norm: 1.1736853122711182\n",
      "G loss: 14.675239562988281\n",
      "\n",
      "Epoch 323\n",
      "Iteration 1\n",
      "D loss: -42.279327392578125\n",
      "GP: 7.156007289886475\n",
      "Gradient norm: 1.1921789646148682\n",
      "G loss: 14.700277328491211\n",
      "Iteration 51\n",
      "D loss: -42.86570739746094\n",
      "GP: 7.774243354797363\n",
      "Gradient norm: 1.1854768991470337\n",
      "G loss: 15.023812294006348\n",
      "Iteration 101\n",
      "D loss: -42.51432800292969\n",
      "GP: 7.354755401611328\n",
      "Gradient norm: 1.1860798597335815\n",
      "G loss: 14.488398551940918\n",
      "Iteration 151\n",
      "D loss: -42.3659782409668\n",
      "GP: 6.0773515701293945\n",
      "Gradient norm: 1.1626437902450562\n",
      "G loss: 14.508344650268555\n",
      "Iteration 201\n",
      "D loss: -42.53788757324219\n",
      "GP: 7.757341384887695\n",
      "Gradient norm: 1.1988080739974976\n",
      "G loss: 14.225007057189941\n",
      "Iteration 251\n",
      "D loss: -40.969425201416016\n",
      "GP: 6.959735870361328\n",
      "Gradient norm: 1.1675665378570557\n",
      "G loss: 15.32229995727539\n",
      "Iteration 301\n",
      "D loss: -41.978858947753906\n",
      "GP: 7.255516052246094\n",
      "Gradient norm: 1.1794129610061646\n",
      "G loss: 14.480233192443848\n",
      "Iteration 351\n",
      "D loss: -42.79731750488281\n",
      "GP: 6.709623336791992\n",
      "Gradient norm: 1.170640230178833\n",
      "G loss: 14.441010475158691\n",
      "\n",
      "Epoch 324\n",
      "Iteration 1\n",
      "D loss: -40.290733337402344\n",
      "GP: 6.128184795379639\n",
      "Gradient norm: 1.147940993309021\n",
      "G loss: 15.017373085021973\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 51\n",
      "D loss: -41.881412506103516\n",
      "GP: 9.08729076385498\n",
      "Gradient norm: 1.2028220891952515\n",
      "G loss: 15.871959686279297\n",
      "Iteration 101\n",
      "D loss: -42.99567413330078\n",
      "GP: 6.093926429748535\n",
      "Gradient norm: 1.1536998748779297\n",
      "G loss: 14.309103012084961\n",
      "Iteration 151\n",
      "D loss: -41.47746658325195\n",
      "GP: 7.231563568115234\n",
      "Gradient norm: 1.180808424949646\n",
      "G loss: 14.556607246398926\n",
      "Iteration 201\n",
      "D loss: -44.19929885864258\n",
      "GP: 7.54260778427124\n",
      "Gradient norm: 1.1881661415100098\n",
      "G loss: 15.201431274414062\n",
      "Iteration 251\n",
      "D loss: -42.928348541259766\n",
      "GP: 6.299198150634766\n",
      "Gradient norm: 1.1597485542297363\n",
      "G loss: 16.4534912109375\n",
      "Iteration 301\n",
      "D loss: -42.29159164428711\n",
      "GP: 7.434815406799316\n",
      "Gradient norm: 1.2042499780654907\n",
      "G loss: 15.483532905578613\n",
      "Iteration 351\n",
      "D loss: -40.47570037841797\n",
      "GP: 6.147586822509766\n",
      "Gradient norm: 1.1578856706619263\n",
      "G loss: 16.0343074798584\n",
      "\n",
      "Epoch 325\n",
      "Iteration 1\n",
      "D loss: -44.023555755615234\n",
      "GP: 8.854351997375488\n",
      "Gradient norm: 1.2207151651382446\n",
      "G loss: 16.351303100585938\n",
      "Iteration 51\n",
      "D loss: -41.08697509765625\n",
      "GP: 7.002748489379883\n",
      "Gradient norm: 1.1693538427352905\n",
      "G loss: 15.627347946166992\n",
      "Iteration 101\n",
      "D loss: -40.732112884521484\n",
      "GP: 6.783927917480469\n",
      "Gradient norm: 1.192617654800415\n",
      "G loss: 15.86340618133545\n",
      "Iteration 151\n",
      "D loss: -41.85527420043945\n",
      "GP: 7.093198299407959\n",
      "Gradient norm: 1.1642884016036987\n",
      "G loss: 14.060492515563965\n",
      "Iteration 201\n",
      "D loss: -41.72500991821289\n",
      "GP: 6.578730583190918\n",
      "Gradient norm: 1.1657227277755737\n",
      "G loss: 16.155628204345703\n",
      "Iteration 251\n",
      "D loss: -43.493316650390625\n",
      "GP: 7.646557807922363\n",
      "Gradient norm: 1.1974514722824097\n",
      "G loss: 14.924168586730957\n",
      "Iteration 301\n",
      "D loss: -42.169288635253906\n",
      "GP: 7.19038724899292\n",
      "Gradient norm: 1.1745778322219849\n",
      "G loss: 14.792680740356445\n",
      "Iteration 351\n",
      "D loss: -43.016815185546875\n",
      "GP: 5.950125694274902\n",
      "Gradient norm: 1.1611090898513794\n",
      "G loss: 15.561728477478027\n",
      "\n",
      "Epoch 326\n",
      "Iteration 1\n",
      "D loss: -40.72430419921875\n",
      "GP: 7.050271987915039\n",
      "Gradient norm: 1.1597819328308105\n",
      "G loss: 15.528460502624512\n",
      "Iteration 51\n",
      "D loss: -42.15072250366211\n",
      "GP: 5.510266304016113\n",
      "Gradient norm: 1.1527069807052612\n",
      "G loss: 14.523018836975098\n",
      "Iteration 101\n",
      "D loss: -46.27128982543945\n",
      "GP: 7.695771217346191\n",
      "Gradient norm: 1.1832540035247803\n",
      "G loss: 14.254486083984375\n",
      "Iteration 151\n",
      "D loss: -42.084815979003906\n",
      "GP: 6.191343784332275\n",
      "Gradient norm: 1.1628097295761108\n",
      "G loss: 17.254926681518555\n",
      "Iteration 201\n",
      "D loss: -42.14796447753906\n",
      "GP: 6.416850566864014\n",
      "Gradient norm: 1.1792672872543335\n",
      "G loss: 14.581557273864746\n",
      "Iteration 251\n",
      "D loss: -41.90180206298828\n",
      "GP: 6.056177616119385\n",
      "Gradient norm: 1.1618748903274536\n",
      "G loss: 17.02931785583496\n",
      "Iteration 301\n",
      "D loss: -41.879669189453125\n",
      "GP: 6.988229751586914\n",
      "Gradient norm: 1.1763286590576172\n",
      "G loss: 15.638749122619629\n",
      "Iteration 351\n",
      "D loss: -41.1362419128418\n",
      "GP: 6.414684295654297\n",
      "Gradient norm: 1.1541857719421387\n",
      "G loss: 15.78885555267334\n",
      "\n",
      "Epoch 327\n",
      "Iteration 1\n",
      "D loss: -42.534339904785156\n",
      "GP: 7.400932312011719\n",
      "Gradient norm: 1.1817724704742432\n",
      "G loss: 15.580695152282715\n",
      "Iteration 51\n",
      "D loss: -42.822200775146484\n",
      "GP: 5.666627407073975\n",
      "Gradient norm: 1.1540952920913696\n",
      "G loss: 15.367310523986816\n",
      "Iteration 101\n",
      "D loss: -44.504676818847656\n",
      "GP: 8.131532669067383\n",
      "Gradient norm: 1.2032172679901123\n",
      "G loss: 14.933542251586914\n",
      "Iteration 151\n",
      "D loss: -40.76580810546875\n",
      "GP: 7.2600417137146\n",
      "Gradient norm: 1.1764484643936157\n",
      "G loss: 14.844042778015137\n",
      "Iteration 201\n",
      "D loss: -40.63731002807617\n",
      "GP: 6.494576930999756\n",
      "Gradient norm: 1.1575672626495361\n",
      "G loss: 16.008636474609375\n",
      "Iteration 251\n",
      "D loss: -41.53545379638672\n",
      "GP: 8.032370567321777\n",
      "Gradient norm: 1.1942076683044434\n",
      "G loss: 16.04878807067871\n",
      "Iteration 301\n",
      "D loss: -40.215309143066406\n",
      "GP: 8.812180519104004\n",
      "Gradient norm: 1.2172667980194092\n",
      "G loss: 15.743941307067871\n",
      "Iteration 351\n",
      "D loss: -43.37184524536133\n",
      "GP: 8.558486938476562\n",
      "Gradient norm: 1.1974542140960693\n",
      "G loss: 15.058627128601074\n",
      "\n",
      "Epoch 328\n",
      "Iteration 1\n",
      "D loss: -41.21217727661133\n",
      "GP: 7.474240779876709\n",
      "Gradient norm: 1.1820148229599\n",
      "G loss: 14.941158294677734\n",
      "Iteration 51\n",
      "D loss: -43.225669860839844\n",
      "GP: 5.601426124572754\n",
      "Gradient norm: 1.1611216068267822\n",
      "G loss: 15.066750526428223\n",
      "Iteration 101\n",
      "D loss: -43.5842399597168\n",
      "GP: 6.279475688934326\n",
      "Gradient norm: 1.1776951551437378\n",
      "G loss: 16.718189239501953\n",
      "Iteration 151\n",
      "D loss: -43.690467834472656\n",
      "GP: 5.205924034118652\n",
      "Gradient norm: 1.1452113389968872\n",
      "G loss: 15.318256378173828\n",
      "Iteration 201\n",
      "D loss: -43.39353942871094\n",
      "GP: 5.001188278198242\n",
      "Gradient norm: 1.130063772201538\n",
      "G loss: 14.866616249084473\n",
      "Iteration 251\n",
      "D loss: -40.67387390136719\n",
      "GP: 6.457763195037842\n",
      "Gradient norm: 1.1571118831634521\n",
      "G loss: 15.879392623901367\n",
      "Iteration 301\n",
      "D loss: -43.478309631347656\n",
      "GP: 6.7205281257629395\n",
      "Gradient norm: 1.1772624254226685\n",
      "G loss: 15.715536117553711\n",
      "Iteration 351\n",
      "D loss: -43.92831039428711\n",
      "GP: 6.574715614318848\n",
      "Gradient norm: 1.171935796737671\n",
      "G loss: 15.001415252685547\n",
      "\n",
      "Epoch 329\n",
      "Iteration 1\n",
      "D loss: -43.19256591796875\n",
      "GP: 7.158669471740723\n",
      "Gradient norm: 1.1876546144485474\n",
      "G loss: 15.47547435760498\n",
      "Iteration 51\n",
      "D loss: -43.317955017089844\n",
      "GP: 8.323938369750977\n",
      "Gradient norm: 1.1789301633834839\n",
      "G loss: 16.175634384155273\n",
      "Iteration 101\n",
      "D loss: -44.20974349975586\n",
      "GP: 6.782403469085693\n",
      "Gradient norm: 1.182176113128662\n",
      "G loss: 14.79638957977295\n",
      "Iteration 151\n",
      "D loss: -42.02326202392578\n",
      "GP: 6.003446102142334\n",
      "Gradient norm: 1.149605393409729\n",
      "G loss: 16.24485206604004\n",
      "Iteration 201\n",
      "D loss: -40.764427185058594\n",
      "GP: 6.891336441040039\n",
      "Gradient norm: 1.1597172021865845\n",
      "G loss: 16.929975509643555\n",
      "Iteration 251\n",
      "D loss: -40.472103118896484\n",
      "GP: 7.01407527923584\n",
      "Gradient norm: 1.158412218093872\n",
      "G loss: 15.91848087310791\n",
      "Iteration 301\n",
      "D loss: -43.547607421875\n",
      "GP: 6.064598083496094\n",
      "Gradient norm: 1.163215160369873\n",
      "G loss: 16.411314010620117\n",
      "Iteration 351\n",
      "D loss: -41.047977447509766\n",
      "GP: 6.667006015777588\n",
      "Gradient norm: 1.174592137336731\n",
      "G loss: 15.92940616607666\n",
      "\n",
      "Epoch 330\n",
      "Iteration 1\n",
      "D loss: -43.325965881347656\n",
      "GP: 9.191656112670898\n",
      "Gradient norm: 1.2012895345687866\n",
      "G loss: 16.293684005737305\n",
      "Iteration 51\n",
      "D loss: -41.230987548828125\n",
      "GP: 6.597497940063477\n",
      "Gradient norm: 1.1784650087356567\n",
      "G loss: 14.946723937988281\n",
      "Iteration 101\n",
      "D loss: -41.98588943481445\n",
      "GP: 7.848927974700928\n",
      "Gradient norm: 1.1873886585235596\n",
      "G loss: 16.860082626342773\n",
      "Iteration 151\n",
      "D loss: -40.46160125732422\n",
      "GP: 6.796563148498535\n",
      "Gradient norm: 1.1815478801727295\n",
      "G loss: 15.562004089355469\n",
      "Iteration 201\n",
      "D loss: -40.67499542236328\n",
      "GP: 8.234655380249023\n",
      "Gradient norm: 1.204082727432251\n",
      "G loss: 17.348005294799805\n",
      "Iteration 251\n",
      "D loss: -41.643821716308594\n",
      "GP: 6.23432731628418\n",
      "Gradient norm: 1.1558300256729126\n",
      "G loss: 16.003259658813477\n",
      "Iteration 301\n",
      "D loss: -42.42432403564453\n",
      "GP: 7.255515098571777\n",
      "Gradient norm: 1.1758748292922974\n",
      "G loss: 15.076713562011719\n",
      "Iteration 351\n",
      "D loss: -45.265533447265625\n",
      "GP: 9.042594909667969\n",
      "Gradient norm: 1.2140469551086426\n",
      "G loss: 15.883110046386719\n",
      "\n",
      "Epoch 331\n",
      "Iteration 1\n",
      "D loss: -41.7822265625\n",
      "GP: 6.36279296875\n",
      "Gradient norm: 1.1647839546203613\n",
      "G loss: 15.178403854370117\n",
      "Iteration 51\n",
      "D loss: -39.77775573730469\n",
      "GP: 6.838249683380127\n",
      "Gradient norm: 1.1760612726211548\n",
      "G loss: 17.388816833496094\n",
      "Iteration 101\n",
      "D loss: -43.64490509033203\n",
      "GP: 7.96964168548584\n",
      "Gradient norm: 1.2011735439300537\n",
      "G loss: 15.480859756469727\n",
      "Iteration 151\n",
      "D loss: -43.4171142578125\n",
      "GP: 7.939207553863525\n",
      "Gradient norm: 1.2109583616256714\n",
      "G loss: 14.889659881591797\n",
      "Iteration 201\n",
      "D loss: -43.18394088745117\n",
      "GP: 5.380680084228516\n",
      "Gradient norm: 1.1365162134170532\n",
      "G loss: 15.298274993896484\n",
      "Iteration 251\n",
      "D loss: -42.395263671875\n",
      "GP: 7.700035572052002\n",
      "Gradient norm: 1.1827094554901123\n",
      "G loss: 14.197226524353027\n",
      "Iteration 301\n",
      "D loss: -42.96155548095703\n",
      "GP: 8.710058212280273\n",
      "Gradient norm: 1.1899304389953613\n",
      "G loss: 15.395502090454102\n",
      "Iteration 351\n",
      "D loss: -45.039337158203125\n",
      "GP: 7.329964637756348\n",
      "Gradient norm: 1.1917232275009155\n",
      "G loss: 16.171615600585938\n",
      "\n",
      "Epoch 332\n",
      "Iteration 1\n",
      "D loss: -42.04840087890625\n",
      "GP: 6.888389587402344\n",
      "Gradient norm: 1.1788408756256104\n",
      "G loss: 15.788568496704102\n",
      "Iteration 51\n",
      "D loss: -41.43791198730469\n",
      "GP: 7.440498352050781\n",
      "Gradient norm: 1.1968997716903687\n",
      "G loss: 16.143346786499023\n",
      "Iteration 101\n",
      "D loss: -43.655029296875\n",
      "GP: 8.377825736999512\n",
      "Gradient norm: 1.2046700716018677\n",
      "G loss: 15.809073448181152\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 151\n",
      "D loss: -40.75502014160156\n",
      "GP: 6.776220321655273\n",
      "Gradient norm: 1.1724313497543335\n",
      "G loss: 16.276086807250977\n",
      "Iteration 201\n",
      "D loss: -43.319698333740234\n",
      "GP: 8.063986778259277\n",
      "Gradient norm: 1.1785480976104736\n",
      "G loss: 15.344833374023438\n",
      "Iteration 251\n",
      "D loss: -41.17856979370117\n",
      "GP: 7.548998832702637\n",
      "Gradient norm: 1.1948007345199585\n",
      "G loss: 16.124618530273438\n",
      "Iteration 301\n",
      "D loss: -42.98761749267578\n",
      "GP: 7.808412551879883\n",
      "Gradient norm: 1.2012457847595215\n",
      "G loss: 15.41102123260498\n",
      "Iteration 351\n",
      "D loss: -45.53874969482422\n",
      "GP: 7.355103015899658\n",
      "Gradient norm: 1.2012526988983154\n",
      "G loss: 14.791263580322266\n",
      "\n",
      "Epoch 333\n",
      "Iteration 1\n",
      "D loss: -45.965545654296875\n",
      "GP: 7.000078201293945\n",
      "Gradient norm: 1.171728253364563\n",
      "G loss: 15.305352210998535\n",
      "Iteration 51\n",
      "D loss: -40.012367248535156\n",
      "GP: 7.865874767303467\n",
      "Gradient norm: 1.1971509456634521\n",
      "G loss: 15.58775520324707\n",
      "Iteration 101\n",
      "D loss: -41.033817291259766\n",
      "GP: 6.6937360763549805\n",
      "Gradient norm: 1.1751121282577515\n",
      "G loss: 15.384153366088867\n",
      "Iteration 151\n",
      "D loss: -41.654808044433594\n",
      "GP: 6.716835975646973\n",
      "Gradient norm: 1.1720457077026367\n",
      "G loss: 15.243124008178711\n",
      "Iteration 201\n",
      "D loss: -42.4935302734375\n",
      "GP: 7.211777687072754\n",
      "Gradient norm: 1.1700794696807861\n",
      "G loss: 16.79209327697754\n",
      "Iteration 251\n",
      "D loss: -43.721553802490234\n",
      "GP: 7.825117588043213\n",
      "Gradient norm: 1.1950569152832031\n",
      "G loss: 15.877559661865234\n",
      "Iteration 301\n",
      "D loss: -41.80941390991211\n",
      "GP: 6.349277973175049\n",
      "Gradient norm: 1.1842936277389526\n",
      "G loss: 15.941940307617188\n",
      "Iteration 351\n",
      "D loss: -44.69762420654297\n",
      "GP: 5.445221900939941\n",
      "Gradient norm: 1.1513358354568481\n",
      "G loss: 16.621322631835938\n",
      "\n",
      "Epoch 334\n",
      "Iteration 1\n",
      "D loss: -41.969215393066406\n",
      "GP: 5.625982284545898\n",
      "Gradient norm: 1.1489726305007935\n",
      "G loss: 15.83109188079834\n",
      "Iteration 51\n",
      "D loss: -40.98414611816406\n",
      "GP: 6.994009017944336\n",
      "Gradient norm: 1.1787909269332886\n",
      "G loss: 15.651965141296387\n",
      "Iteration 101\n",
      "D loss: -42.12690734863281\n",
      "GP: 8.015829086303711\n",
      "Gradient norm: 1.1924500465393066\n",
      "G loss: 14.38058090209961\n",
      "Iteration 151\n",
      "D loss: -42.50593185424805\n",
      "GP: 5.2974042892456055\n",
      "Gradient norm: 1.1460832357406616\n",
      "G loss: 15.962759971618652\n",
      "Iteration 201\n",
      "D loss: -43.655296325683594\n",
      "GP: 5.188601493835449\n",
      "Gradient norm: 1.1313917636871338\n",
      "G loss: 16.691326141357422\n",
      "Iteration 251\n",
      "D loss: -43.59628677368164\n",
      "GP: 6.225200653076172\n",
      "Gradient norm: 1.166171669960022\n",
      "G loss: 14.953612327575684\n",
      "Iteration 301\n",
      "D loss: -45.7703857421875\n",
      "GP: 6.9535346031188965\n",
      "Gradient norm: 1.1672691106796265\n",
      "G loss: 16.01137924194336\n",
      "Iteration 351\n",
      "D loss: -44.27069091796875\n",
      "GP: 7.634712219238281\n",
      "Gradient norm: 1.18393075466156\n",
      "G loss: 15.918201446533203\n",
      "\n",
      "Epoch 335\n",
      "Iteration 1\n",
      "D loss: -40.34153366088867\n",
      "GP: 7.149430274963379\n",
      "Gradient norm: 1.1924419403076172\n",
      "G loss: 15.45007610321045\n",
      "Iteration 51\n",
      "D loss: -41.45983123779297\n",
      "GP: 5.3373332023620605\n",
      "Gradient norm: 1.1630605459213257\n",
      "G loss: 15.238048553466797\n",
      "Iteration 101\n",
      "D loss: -42.68399429321289\n",
      "GP: 7.534283638000488\n",
      "Gradient norm: 1.1953976154327393\n",
      "G loss: 14.928657531738281\n",
      "Iteration 151\n",
      "D loss: -42.886634826660156\n",
      "GP: 7.007732391357422\n",
      "Gradient norm: 1.1836433410644531\n",
      "G loss: 15.849867820739746\n",
      "Iteration 201\n",
      "D loss: -41.679298400878906\n",
      "GP: 7.787445068359375\n",
      "Gradient norm: 1.1693161725997925\n",
      "G loss: 15.24026870727539\n",
      "Iteration 251\n",
      "D loss: -40.59714889526367\n",
      "GP: 5.533259391784668\n",
      "Gradient norm: 1.1514842510223389\n",
      "G loss: 14.949146270751953\n",
      "Iteration 301\n",
      "D loss: -41.78316116333008\n",
      "GP: 6.255578517913818\n",
      "Gradient norm: 1.15512216091156\n",
      "G loss: 16.521169662475586\n",
      "Iteration 351\n",
      "D loss: -41.49775695800781\n",
      "GP: 5.6367340087890625\n",
      "Gradient norm: 1.140364170074463\n",
      "G loss: 17.02044677734375\n",
      "\n",
      "Epoch 336\n",
      "Iteration 1\n",
      "D loss: -42.595252990722656\n",
      "GP: 5.6142730712890625\n",
      "Gradient norm: 1.1446828842163086\n",
      "G loss: 16.53705406188965\n",
      "Iteration 51\n",
      "D loss: -43.71738815307617\n",
      "GP: 7.141705513000488\n",
      "Gradient norm: 1.1620798110961914\n",
      "G loss: 16.73567008972168\n",
      "Iteration 101\n",
      "D loss: -42.42924499511719\n",
      "GP: 7.405097484588623\n",
      "Gradient norm: 1.1912330389022827\n",
      "G loss: 14.781790733337402\n",
      "Iteration 151\n",
      "D loss: -43.24480438232422\n",
      "GP: 6.93348503112793\n",
      "Gradient norm: 1.1758687496185303\n",
      "G loss: 16.768848419189453\n",
      "Iteration 201\n",
      "D loss: -40.56539535522461\n",
      "GP: 6.51088285446167\n",
      "Gradient norm: 1.1620906591415405\n",
      "G loss: 15.723864555358887\n",
      "Iteration 251\n",
      "D loss: -42.58098220825195\n",
      "GP: 4.715222358703613\n",
      "Gradient norm: 1.136827826499939\n",
      "G loss: 16.500629425048828\n",
      "Iteration 301\n",
      "D loss: -44.15816116333008\n",
      "GP: 9.139513969421387\n",
      "Gradient norm: 1.2195123434066772\n",
      "G loss: 15.985978126525879\n",
      "Iteration 351\n",
      "D loss: -40.96027374267578\n",
      "GP: 6.42169189453125\n",
      "Gradient norm: 1.169151782989502\n",
      "G loss: 16.46806526184082\n",
      "\n",
      "Epoch 337\n",
      "Iteration 1\n",
      "D loss: -40.7861213684082\n",
      "GP: 6.893368721008301\n",
      "Gradient norm: 1.1858571767807007\n",
      "G loss: 16.291746139526367\n",
      "Iteration 51\n",
      "D loss: -43.30305480957031\n",
      "GP: 7.573313236236572\n",
      "Gradient norm: 1.1880824565887451\n",
      "G loss: 16.73261260986328\n",
      "Iteration 101\n",
      "D loss: -40.352272033691406\n",
      "GP: 6.164796352386475\n",
      "Gradient norm: 1.1600944995880127\n",
      "G loss: 15.699323654174805\n",
      "Iteration 151\n",
      "D loss: -41.44243621826172\n",
      "GP: 7.411492347717285\n",
      "Gradient norm: 1.1918505430221558\n",
      "G loss: 16.558988571166992\n",
      "Iteration 201\n",
      "D loss: -40.90782165527344\n",
      "GP: 8.006606101989746\n",
      "Gradient norm: 1.1924809217453003\n",
      "G loss: 17.170351028442383\n",
      "Iteration 251\n",
      "D loss: -40.93623352050781\n",
      "GP: 7.694456100463867\n",
      "Gradient norm: 1.1905900239944458\n",
      "G loss: 17.322107315063477\n",
      "Iteration 301\n",
      "D loss: -42.67066955566406\n",
      "GP: 6.55999755859375\n",
      "Gradient norm: 1.1510257720947266\n",
      "G loss: 15.895835876464844\n",
      "Iteration 351\n",
      "D loss: -42.92768859863281\n",
      "GP: 6.914937496185303\n",
      "Gradient norm: 1.170055627822876\n",
      "G loss: 14.099420547485352\n",
      "\n",
      "Epoch 338\n",
      "Iteration 1\n",
      "D loss: -41.00225830078125\n",
      "GP: 5.515520095825195\n",
      "Gradient norm: 1.1506749391555786\n",
      "G loss: 16.204557418823242\n",
      "Iteration 51\n",
      "D loss: -42.116973876953125\n",
      "GP: 8.047845840454102\n",
      "Gradient norm: 1.1966934204101562\n",
      "G loss: 16.451379776000977\n",
      "Iteration 101\n",
      "D loss: -41.01837921142578\n",
      "GP: 6.227555274963379\n",
      "Gradient norm: 1.1575273275375366\n",
      "G loss: 15.523283004760742\n",
      "Iteration 151\n",
      "D loss: -44.330291748046875\n",
      "GP: 7.433169364929199\n",
      "Gradient norm: 1.1769636869430542\n",
      "G loss: 16.216535568237305\n",
      "Iteration 201\n",
      "D loss: -42.41606140136719\n",
      "GP: 5.605049133300781\n",
      "Gradient norm: 1.1404414176940918\n",
      "G loss: 17.013439178466797\n",
      "Iteration 251\n",
      "D loss: -40.98503112792969\n",
      "GP: 9.041398048400879\n",
      "Gradient norm: 1.2157831192016602\n",
      "G loss: 17.000185012817383\n",
      "Iteration 301\n",
      "D loss: -42.704505920410156\n",
      "GP: 7.710233688354492\n",
      "Gradient norm: 1.1976691484451294\n",
      "G loss: 15.880152702331543\n",
      "Iteration 351\n",
      "D loss: -42.81196975708008\n",
      "GP: 7.954020023345947\n",
      "Gradient norm: 1.1899566650390625\n",
      "G loss: 15.742364883422852\n",
      "\n",
      "Epoch 339\n",
      "Iteration 1\n",
      "D loss: -42.71432876586914\n",
      "GP: 7.0619096755981445\n",
      "Gradient norm: 1.1852458715438843\n",
      "G loss: 16.121309280395508\n",
      "Iteration 51\n",
      "D loss: -44.45446014404297\n",
      "GP: 6.388124465942383\n",
      "Gradient norm: 1.1684646606445312\n",
      "G loss: 15.29491138458252\n",
      "Iteration 101\n",
      "D loss: -41.24428939819336\n",
      "GP: 5.6096296310424805\n",
      "Gradient norm: 1.1235918998718262\n",
      "G loss: 17.513212203979492\n",
      "Iteration 151\n",
      "D loss: -43.343727111816406\n",
      "GP: 4.810550212860107\n",
      "Gradient norm: 1.1374285221099854\n",
      "G loss: 15.471165657043457\n",
      "Iteration 201\n",
      "D loss: -39.16252136230469\n",
      "GP: 6.045379638671875\n",
      "Gradient norm: 1.1633989810943604\n",
      "G loss: 16.081192016601562\n",
      "Iteration 251\n",
      "D loss: -43.14555740356445\n",
      "GP: 6.718936443328857\n",
      "Gradient norm: 1.1739866733551025\n",
      "G loss: 16.065216064453125\n",
      "Iteration 301\n",
      "D loss: -43.04906463623047\n",
      "GP: 7.074081897735596\n",
      "Gradient norm: 1.1808016300201416\n",
      "G loss: 15.7855224609375\n",
      "Iteration 351\n",
      "D loss: -40.78128433227539\n",
      "GP: 5.759738922119141\n",
      "Gradient norm: 1.1654753684997559\n",
      "G loss: 16.17749786376953\n",
      "\n",
      "Epoch 340\n",
      "Iteration 1\n",
      "D loss: -43.931549072265625\n",
      "GP: 6.613739967346191\n",
      "Gradient norm: 1.1750942468643188\n",
      "G loss: 16.61063003540039\n",
      "Iteration 51\n",
      "D loss: -42.06111145019531\n",
      "GP: 6.226734638214111\n",
      "Gradient norm: 1.1520882844924927\n",
      "G loss: 16.560346603393555\n",
      "Iteration 101\n",
      "D loss: -44.62725067138672\n",
      "GP: 6.77989387512207\n",
      "Gradient norm: 1.1707414388656616\n",
      "G loss: 16.99785614013672\n",
      "Iteration 151\n",
      "D loss: -42.47026824951172\n",
      "GP: 7.066230773925781\n",
      "Gradient norm: 1.1845738887786865\n",
      "G loss: 15.884559631347656\n",
      "Iteration 201\n",
      "D loss: -40.90227508544922\n",
      "GP: 7.90468168258667\n",
      "Gradient norm: 1.1685328483581543\n",
      "G loss: 16.791767120361328\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 251\n",
      "D loss: -43.886085510253906\n",
      "GP: 6.93950080871582\n",
      "Gradient norm: 1.1736130714416504\n",
      "G loss: 17.610231399536133\n",
      "Iteration 301\n",
      "D loss: -43.45652389526367\n",
      "GP: 8.59015941619873\n",
      "Gradient norm: 1.1950385570526123\n",
      "G loss: 17.213632583618164\n",
      "Iteration 351\n",
      "D loss: -43.217079162597656\n",
      "GP: 8.291900634765625\n",
      "Gradient norm: 1.1930657625198364\n",
      "G loss: 15.965286254882812\n",
      "\n",
      "Epoch 341\n",
      "Iteration 1\n",
      "D loss: -42.22818374633789\n",
      "GP: 8.691071510314941\n",
      "Gradient norm: 1.2184667587280273\n",
      "G loss: 16.002294540405273\n",
      "Iteration 51\n",
      "D loss: -43.3702278137207\n",
      "GP: 7.047425270080566\n",
      "Gradient norm: 1.1839748620986938\n",
      "G loss: 17.164583206176758\n",
      "Iteration 101\n",
      "D loss: -44.11222839355469\n",
      "GP: 6.6114420890808105\n",
      "Gradient norm: 1.1634234189987183\n",
      "G loss: 16.217697143554688\n",
      "Iteration 151\n",
      "D loss: -43.879642486572266\n",
      "GP: 7.293965816497803\n",
      "Gradient norm: 1.1890113353729248\n",
      "G loss: 16.51691246032715\n",
      "Iteration 201\n",
      "D loss: -42.329368591308594\n",
      "GP: 6.814722061157227\n",
      "Gradient norm: 1.162304162979126\n",
      "G loss: 17.233407974243164\n",
      "Iteration 251\n",
      "D loss: -41.37750244140625\n",
      "GP: 6.62401008605957\n",
      "Gradient norm: 1.1578835248947144\n",
      "G loss: 16.870214462280273\n",
      "Iteration 301\n",
      "D loss: -43.84434127807617\n",
      "GP: 6.552597999572754\n",
      "Gradient norm: 1.1662431955337524\n",
      "G loss: 14.540284156799316\n",
      "Iteration 351\n",
      "D loss: -42.038909912109375\n",
      "GP: 7.309606075286865\n",
      "Gradient norm: 1.1921700239181519\n",
      "G loss: 15.266782760620117\n",
      "\n",
      "Epoch 342\n",
      "Iteration 1\n",
      "D loss: -42.91892623901367\n",
      "GP: 7.505667686462402\n",
      "Gradient norm: 1.186452031135559\n",
      "G loss: 16.84684181213379\n",
      "Iteration 51\n",
      "D loss: -43.41958999633789\n",
      "GP: 6.517189025878906\n",
      "Gradient norm: 1.1614391803741455\n",
      "G loss: 16.77783966064453\n",
      "Iteration 101\n",
      "D loss: -43.4296875\n",
      "GP: 6.508266925811768\n",
      "Gradient norm: 1.1706606149673462\n",
      "G loss: 16.739904403686523\n",
      "Iteration 151\n",
      "D loss: -42.58226776123047\n",
      "GP: 7.417657375335693\n",
      "Gradient norm: 1.179574728012085\n",
      "G loss: 16.20418357849121\n",
      "Iteration 201\n",
      "D loss: -41.186973571777344\n",
      "GP: 8.532630920410156\n",
      "Gradient norm: 1.200429916381836\n",
      "G loss: 16.465349197387695\n",
      "Iteration 251\n",
      "D loss: -41.267539978027344\n",
      "GP: 6.3398590087890625\n",
      "Gradient norm: 1.1779732704162598\n",
      "G loss: 14.975494384765625\n",
      "Iteration 301\n",
      "D loss: -41.53990936279297\n",
      "GP: 7.3194684982299805\n",
      "Gradient norm: 1.1674778461456299\n",
      "G loss: 13.587967872619629\n",
      "Iteration 351\n",
      "D loss: -42.395381927490234\n",
      "GP: 6.114748954772949\n",
      "Gradient norm: 1.16499662399292\n",
      "G loss: 14.460384368896484\n",
      "\n",
      "Epoch 343\n",
      "Iteration 1\n",
      "D loss: -41.063636779785156\n",
      "GP: 5.631965637207031\n",
      "Gradient norm: 1.1605310440063477\n",
      "G loss: 14.395185470581055\n",
      "Iteration 51\n",
      "D loss: -43.675086975097656\n",
      "GP: 8.452875137329102\n",
      "Gradient norm: 1.210275650024414\n",
      "G loss: 14.191423416137695\n",
      "Iteration 101\n",
      "D loss: -42.09074783325195\n",
      "GP: 8.935713768005371\n",
      "Gradient norm: 1.2099013328552246\n",
      "G loss: 14.090068817138672\n",
      "Iteration 151\n",
      "D loss: -43.033119201660156\n",
      "GP: 4.628671646118164\n",
      "Gradient norm: 1.1396045684814453\n",
      "G loss: 16.614192962646484\n",
      "Iteration 201\n",
      "D loss: -43.31538772583008\n",
      "GP: 6.516421318054199\n",
      "Gradient norm: 1.1731795072555542\n",
      "G loss: 14.968021392822266\n",
      "Iteration 251\n",
      "D loss: -43.7688102722168\n",
      "GP: 7.228360176086426\n",
      "Gradient norm: 1.1646844148635864\n",
      "G loss: 15.263222694396973\n",
      "Iteration 301\n",
      "D loss: -42.16777420043945\n",
      "GP: 6.452927112579346\n",
      "Gradient norm: 1.1739754676818848\n",
      "G loss: 15.007719039916992\n",
      "Iteration 351\n",
      "D loss: -41.7862663269043\n",
      "GP: 7.34637451171875\n",
      "Gradient norm: 1.1887086629867554\n",
      "G loss: 15.082900047302246\n",
      "\n",
      "Epoch 344\n",
      "Iteration 1\n",
      "D loss: -42.93629837036133\n",
      "GP: 7.560450553894043\n",
      "Gradient norm: 1.1815497875213623\n",
      "G loss: 16.1519775390625\n",
      "Iteration 51\n",
      "D loss: -42.862823486328125\n",
      "GP: 7.3662214279174805\n",
      "Gradient norm: 1.191778540611267\n",
      "G loss: 16.809642791748047\n",
      "Iteration 101\n",
      "D loss: -44.40137481689453\n",
      "GP: 7.616890907287598\n",
      "Gradient norm: 1.1905897855758667\n",
      "G loss: 17.000526428222656\n",
      "Iteration 151\n",
      "D loss: -41.26807403564453\n",
      "GP: 6.574361801147461\n",
      "Gradient norm: 1.1727595329284668\n",
      "G loss: 17.222043991088867\n",
      "Iteration 201\n",
      "D loss: -41.244075775146484\n",
      "GP: 8.271306037902832\n",
      "Gradient norm: 1.2016783952713013\n",
      "G loss: 15.337929725646973\n",
      "Iteration 251\n",
      "D loss: -45.64032745361328\n",
      "GP: 6.784958839416504\n",
      "Gradient norm: 1.1705392599105835\n",
      "G loss: 14.5466947555542\n",
      "Iteration 301\n",
      "D loss: -42.397037506103516\n",
      "GP: 9.230938911437988\n",
      "Gradient norm: 1.207735300064087\n",
      "G loss: 14.042581558227539\n",
      "Iteration 351\n",
      "D loss: -43.42509460449219\n",
      "GP: 6.674056053161621\n",
      "Gradient norm: 1.1881318092346191\n",
      "G loss: 15.020092010498047\n",
      "\n",
      "Epoch 345\n",
      "Iteration 1\n",
      "D loss: -40.08916473388672\n",
      "GP: 7.234825134277344\n",
      "Gradient norm: 1.1718153953552246\n",
      "G loss: 16.38274574279785\n",
      "Iteration 51\n",
      "D loss: -42.23395919799805\n",
      "GP: 6.068522930145264\n",
      "Gradient norm: 1.1677684783935547\n",
      "G loss: 16.462793350219727\n",
      "Iteration 101\n",
      "D loss: -43.026676177978516\n",
      "GP: 6.481070041656494\n",
      "Gradient norm: 1.1698668003082275\n",
      "G loss: 16.118850708007812\n",
      "Iteration 151\n",
      "D loss: -40.42884826660156\n",
      "GP: 5.760908603668213\n",
      "Gradient norm: 1.1652288436889648\n",
      "G loss: 16.545724868774414\n",
      "Iteration 201\n",
      "D loss: -41.95343017578125\n",
      "GP: 7.2423529624938965\n",
      "Gradient norm: 1.1707972288131714\n",
      "G loss: 15.754995346069336\n",
      "Iteration 251\n",
      "D loss: -42.03464126586914\n",
      "GP: 6.893866062164307\n",
      "Gradient norm: 1.1735475063323975\n",
      "G loss: 16.24800682067871\n",
      "Iteration 301\n",
      "D loss: -41.20036315917969\n",
      "GP: 7.197702407836914\n",
      "Gradient norm: 1.180835247039795\n",
      "G loss: 16.404184341430664\n",
      "Iteration 351\n",
      "D loss: -39.96883773803711\n",
      "GP: 9.815574645996094\n",
      "Gradient norm: 1.215863585472107\n",
      "G loss: 16.405927658081055\n",
      "\n",
      "Epoch 346\n",
      "Iteration 1\n",
      "D loss: -44.05897521972656\n",
      "GP: 7.669825077056885\n",
      "Gradient norm: 1.1879347562789917\n",
      "G loss: 15.465157508850098\n",
      "Iteration 51\n",
      "D loss: -44.42008972167969\n",
      "GP: 8.781522750854492\n",
      "Gradient norm: 1.213479995727539\n",
      "G loss: 16.1262264251709\n",
      "Iteration 101\n",
      "D loss: -42.36054229736328\n",
      "GP: 5.993546962738037\n",
      "Gradient norm: 1.1595876216888428\n",
      "G loss: 14.551179885864258\n",
      "Iteration 151\n",
      "D loss: -41.171573638916016\n",
      "GP: 9.02248764038086\n",
      "Gradient norm: 1.2145891189575195\n",
      "G loss: 14.945014953613281\n",
      "Iteration 201\n",
      "D loss: -43.80589294433594\n",
      "GP: 5.400767803192139\n",
      "Gradient norm: 1.157886266708374\n",
      "G loss: 15.302464485168457\n",
      "Iteration 251\n",
      "D loss: -41.46151351928711\n",
      "GP: 6.96122407913208\n",
      "Gradient norm: 1.186787486076355\n",
      "G loss: 15.756617546081543\n",
      "Iteration 301\n",
      "D loss: -42.13418197631836\n",
      "GP: 8.328327178955078\n",
      "Gradient norm: 1.197615146636963\n",
      "G loss: 16.018749237060547\n",
      "Iteration 351\n",
      "D loss: -43.03504180908203\n",
      "GP: 8.071037292480469\n",
      "Gradient norm: 1.191325068473816\n",
      "G loss: 16.1622314453125\n",
      "\n",
      "Epoch 347\n",
      "Iteration 1\n",
      "D loss: -43.40791320800781\n",
      "GP: 7.660454750061035\n",
      "Gradient norm: 1.1782066822052002\n",
      "G loss: 16.920013427734375\n",
      "Iteration 51\n",
      "D loss: -42.761444091796875\n",
      "GP: 6.5545654296875\n",
      "Gradient norm: 1.156022310256958\n",
      "G loss: 16.61940574645996\n",
      "Iteration 101\n",
      "D loss: -41.16526412963867\n",
      "GP: 7.924075126647949\n",
      "Gradient norm: 1.1933035850524902\n",
      "G loss: 16.43440818786621\n",
      "Iteration 151\n",
      "D loss: -43.11854553222656\n",
      "GP: 6.897331237792969\n",
      "Gradient norm: 1.1681057214736938\n",
      "G loss: 16.802600860595703\n",
      "Iteration 201\n",
      "D loss: -44.31715393066406\n",
      "GP: 7.185571670532227\n",
      "Gradient norm: 1.1897608041763306\n",
      "G loss: 16.475933074951172\n",
      "Iteration 251\n",
      "D loss: -44.074684143066406\n",
      "GP: 9.50333023071289\n",
      "Gradient norm: 1.2313742637634277\n",
      "G loss: 15.478181838989258\n",
      "Iteration 301\n",
      "D loss: -43.57227325439453\n",
      "GP: 6.508373737335205\n",
      "Gradient norm: 1.1751906871795654\n",
      "G loss: 16.505220413208008\n",
      "Iteration 351\n",
      "D loss: -42.777252197265625\n",
      "GP: 7.531320571899414\n",
      "Gradient norm: 1.1815205812454224\n",
      "G loss: 15.863615989685059\n",
      "\n",
      "Epoch 348\n",
      "Iteration 1\n",
      "D loss: -40.316959381103516\n",
      "GP: 7.261085510253906\n",
      "Gradient norm: 1.1889407634735107\n",
      "G loss: 15.871033668518066\n",
      "Iteration 51\n",
      "D loss: -43.285953521728516\n",
      "GP: 8.775973320007324\n",
      "Gradient norm: 1.195163369178772\n",
      "G loss: 16.590085983276367\n",
      "Iteration 101\n",
      "D loss: -43.206199645996094\n",
      "GP: 7.727295875549316\n",
      "Gradient norm: 1.1990429162979126\n",
      "G loss: 16.664813995361328\n",
      "Iteration 151\n",
      "D loss: -40.720787048339844\n",
      "GP: 9.397858619689941\n",
      "Gradient norm: 1.206044316291809\n",
      "G loss: 16.136310577392578\n",
      "Iteration 201\n",
      "D loss: -40.66747283935547\n",
      "GP: 8.454145431518555\n",
      "Gradient norm: 1.2050275802612305\n",
      "G loss: 17.546428680419922\n",
      "Iteration 251\n",
      "D loss: -41.74523162841797\n",
      "GP: 7.42807674407959\n",
      "Gradient norm: 1.1919054985046387\n",
      "G loss: 17.46617317199707\n",
      "Iteration 301\n",
      "D loss: -44.50748062133789\n",
      "GP: 6.7902092933654785\n",
      "Gradient norm: 1.1749296188354492\n",
      "G loss: 17.16988754272461\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 351\n",
      "D loss: -43.049285888671875\n",
      "GP: 5.712432861328125\n",
      "Gradient norm: 1.153333306312561\n",
      "G loss: 15.945523262023926\n",
      "\n",
      "Epoch 349\n",
      "Iteration 1\n",
      "D loss: -42.606536865234375\n",
      "GP: 7.842981338500977\n",
      "Gradient norm: 1.1774033308029175\n",
      "G loss: 15.4450101852417\n",
      "Iteration 51\n",
      "D loss: -39.40745544433594\n",
      "GP: 8.392369270324707\n",
      "Gradient norm: 1.20436429977417\n",
      "G loss: 16.92148208618164\n",
      "Iteration 101\n",
      "D loss: -42.29683303833008\n",
      "GP: 7.1097731590271\n",
      "Gradient norm: 1.2028604745864868\n",
      "G loss: 15.492683410644531\n",
      "Iteration 151\n",
      "D loss: -42.2095832824707\n",
      "GP: 5.178646564483643\n",
      "Gradient norm: 1.1426517963409424\n",
      "G loss: 15.818634986877441\n",
      "Iteration 201\n",
      "D loss: -40.9426383972168\n",
      "GP: 9.839564323425293\n",
      "Gradient norm: 1.233717679977417\n",
      "G loss: 16.46194839477539\n",
      "Iteration 251\n",
      "D loss: -39.488765716552734\n",
      "GP: 5.299559593200684\n",
      "Gradient norm: 1.1570018529891968\n",
      "G loss: 16.566513061523438\n",
      "Iteration 301\n",
      "D loss: -42.919681549072266\n",
      "GP: 7.548839569091797\n",
      "Gradient norm: 1.1968947649002075\n",
      "G loss: 16.382122039794922\n",
      "Iteration 351\n",
      "D loss: -42.86063766479492\n",
      "GP: 7.4401021003723145\n",
      "Gradient norm: 1.1924625635147095\n",
      "G loss: 15.937528610229492\n",
      "\n",
      "Epoch 350\n",
      "Iteration 1\n",
      "D loss: -43.30155944824219\n",
      "GP: 8.110734939575195\n",
      "Gradient norm: 1.2017924785614014\n",
      "G loss: 15.909183502197266\n",
      "Iteration 51\n",
      "D loss: -43.07059860229492\n",
      "GP: 7.122105598449707\n",
      "Gradient norm: 1.162704586982727\n",
      "G loss: 16.769168853759766\n",
      "Iteration 101\n",
      "D loss: -41.426822662353516\n",
      "GP: 8.451763153076172\n",
      "Gradient norm: 1.2063733339309692\n",
      "G loss: 16.210647583007812\n",
      "Iteration 151\n",
      "D loss: -44.41230010986328\n",
      "GP: 7.6112895011901855\n",
      "Gradient norm: 1.191132664680481\n",
      "G loss: 15.509536743164062\n",
      "Iteration 201\n",
      "D loss: -41.73832702636719\n",
      "GP: 5.924337387084961\n",
      "Gradient norm: 1.1675456762313843\n",
      "G loss: 16.416410446166992\n",
      "Iteration 251\n",
      "D loss: -43.96403503417969\n",
      "GP: 5.684537887573242\n",
      "Gradient norm: 1.1753257513046265\n",
      "G loss: 15.582502365112305\n",
      "Iteration 301\n",
      "D loss: -40.35808181762695\n",
      "GP: 4.748836517333984\n",
      "Gradient norm: 1.1440767049789429\n",
      "G loss: 17.13364028930664\n",
      "Iteration 351\n",
      "D loss: -41.672462463378906\n",
      "GP: 8.667678833007812\n",
      "Gradient norm: 1.2048879861831665\n",
      "G loss: 16.22865867614746\n",
      "\n",
      "Epoch 351\n",
      "Iteration 1\n",
      "D loss: -40.56693649291992\n",
      "GP: 6.193943023681641\n",
      "Gradient norm: 1.1758095026016235\n",
      "G loss: 14.475015640258789\n",
      "Iteration 51\n",
      "D loss: -43.73103332519531\n",
      "GP: 7.133492469787598\n",
      "Gradient norm: 1.1724610328674316\n",
      "G loss: 15.572925567626953\n",
      "Iteration 101\n",
      "D loss: -41.929813385009766\n",
      "GP: 7.692080497741699\n",
      "Gradient norm: 1.2001535892486572\n",
      "G loss: 16.01011848449707\n",
      "Iteration 151\n",
      "D loss: -42.49309539794922\n",
      "GP: 6.586980819702148\n",
      "Gradient norm: 1.17665433883667\n",
      "G loss: 17.142990112304688\n",
      "Iteration 201\n",
      "D loss: -40.59717559814453\n",
      "GP: 6.572937488555908\n",
      "Gradient norm: 1.1700341701507568\n",
      "G loss: 15.957818031311035\n"
     ]
    }
   ],
   "source": [
    "all_transforms = transforms.Compose([transforms.Resize(32),transforms.ToTensor()])\n",
    "# Get train and test data\n",
    "train_data = dset.MNIST('./Datasets', train=True, download=True,\n",
    "                            transform=all_transforms)\n",
    "test_data = dset.MNIST('./Datasets', train=False,\n",
    "                           transform=all_transforms)\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(train_data, batch_size=128, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=128, shuffle=True)\n",
    "\n",
    "img_size = (32, 32, 3)\n",
    "\n",
    "generator = Generator_c()\n",
    "discriminator = Discriminator_c()\n",
    "\n",
    "# Initialize optimizers\n",
    "lr = 1e-4\n",
    "betas = (.9, .99)\n",
    "G_optimizer = optim.Adam(generator.parameters(), lr=lr, betas=betas)\n",
    "D_optimizer = optim.Adam(discriminator.parameters(), lr=lr, betas=betas)\n",
    "\n",
    "# Train model\n",
    "epochs = 600\n",
    "trainer = Trainer(generator, discriminator, G_optimizer, D_optimizer,\n",
    "                  use_cuda=torch.cuda.is_available())\n",
    "G_loss,D_loss,GP_loss=trainer.train(loader_train, epochs, save_training_gif=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving a new best\n",
      "=> Saving a new best\n"
     ]
    }
   ],
   "source": [
    "save_checkpoint({'epoch': 50,'state_dict': generator.state_dict()}, True, filename='ckpt_generator.pth.tar')\n",
    "save_checkpoint({'epoch': 50,'state_dict': discriminator.state_dict()}, True, filename='ckpt_discriminator.pth.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n",
      "ERROR:tornado.general:Uncaught exception in ZMQStream callback\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/shared/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n",
      "    callback(*args, **kwargs)\n",
      "  File \"/home/shared/anaconda3/lib/python3.6/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/shared/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n",
      "    return self.dispatch_shell(stream, msg)\n",
      "  File \"/home/shared/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n",
      "    handler(stream, idents, msg)\n",
      "  File \"/home/shared/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n",
      "    user_expressions, allow_stdin)\n",
      "  File \"/home/shared/anaconda3/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n",
      "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
      "  File \"/home/shared/anaconda3/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n",
      "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
      "  File \"/home/shared/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2664, in run_cell\n",
      "    self.events.trigger('post_execute')\n",
      "  File \"/home/shared/anaconda3/lib/python3.6/site-packages/IPython/core/events.py\", line 88, in trigger\n",
      "    func(*args, **kwargs)\n",
      "  File \"/home/shared/anaconda3/lib/python3.6/site-packages/matplotlib/pyplot.py\", line 149, in post_execute\n",
      "    draw_all()\n",
      "  File \"/home/shared/anaconda3/lib/python3.6/site-packages/matplotlib/_pylab_helpers.py\", line 136, in draw_all\n",
      "    f_mgr.canvas.draw_idle()\n",
      "  File \"/home/shared/anaconda3/lib/python3.6/site-packages/matplotlib/backend_bases.py\", line 2055, in draw_idle\n",
      "    self.draw(*args, **kwargs)\n",
      "  File \"/home/shared/anaconda3/lib/python3.6/site-packages/matplotlib/backends/backend_agg.py\", line 433, in draw\n",
      "    self.figure.draw(self.renderer)\n",
      "  File \"/home/shared/anaconda3/lib/python3.6/site-packages/matplotlib/artist.py\", line 55, in draw_wrapper\n",
      "    return draw(artist, renderer, *args, **kwargs)\n",
      "  File \"/home/shared/anaconda3/lib/python3.6/site-packages/matplotlib/figure.py\", line 1475, in draw\n",
      "    renderer, self, artists, self.suppressComposite)\n",
      "  File \"/home/shared/anaconda3/lib/python3.6/site-packages/matplotlib/image.py\", line 141, in _draw_list_compositing_images\n",
      "    a.draw(renderer)\n",
      "  File \"/home/shared/anaconda3/lib/python3.6/site-packages/matplotlib/artist.py\", line 55, in draw_wrapper\n",
      "    return draw(artist, renderer, *args, **kwargs)\n",
      "  File \"/home/shared/anaconda3/lib/python3.6/site-packages/matplotlib/axes/_base.py\", line 2607, in draw\n",
      "    mimage._draw_list_compositing_images(renderer, self, artists)\n",
      "  File \"/home/shared/anaconda3/lib/python3.6/site-packages/matplotlib/image.py\", line 141, in _draw_list_compositing_images\n",
      "    a.draw(renderer)\n",
      "  File \"/home/shared/anaconda3/lib/python3.6/site-packages/matplotlib/artist.py\", line 55, in draw_wrapper\n",
      "    return draw(artist, renderer, *args, **kwargs)\n",
      "  File \"/home/shared/anaconda3/lib/python3.6/site-packages/matplotlib/lines.py\", line 833, in draw\n",
      "    rgbaFace)\n",
      "  File \"/home/shared/anaconda3/lib/python3.6/site-packages/matplotlib/backends/backend_agg.py\", line 122, in draw_markers\n",
      "    return self._renderer.draw_markers(*kl, **kw)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "sns.set(style=\"darkgrid\")\n",
    "plt.title('Accuracy Plots')\n",
    "plt.plot(G_loss,'o',label='Generator loss')\n",
    "plt.plot(D_loss,'o',label='Discriminator loss')\n",
    "plt.plot(GP_loss,'o',label='GP loss')\n",
    "plt.xlabel('Iteration')\n",
    "plt.legend(ncol=2, loc='lower right')\n",
    "plt.gcf().set_size_inches(15, 12)\n",
    "plt.savefig(\"cnn_accuracy.png\")\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('gancifar_hist.pkl', 'wb') as f:  # Python 3: open(..., 'wb')\n",
    "    pickle.dump([G_loss,D_loss,GP_loss], f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shared/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:20: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 images complete\n",
      "1000 images complete\n",
      "2000 images complete\n",
      "3000 images complete\n",
      "4000 images complete\n",
      "5000 images complete\n",
      "6000 images complete\n",
      "7000 images complete\n",
      "8000 images complete\n",
      "9000 images complete\n",
      "10000 images complete\n",
      "11000 images complete\n",
      "12000 images complete\n",
      "13000 images complete\n",
      "14000 images complete\n",
      "15000 images complete\n",
      "16000 images complete\n",
      "17000 images complete\n",
      "18000 images complete\n",
      "19000 images complete\n",
      "20000 images complete\n",
      "21000 images complete\n",
      "22000 images complete\n",
      "23000 images complete\n",
      "24000 images complete\n",
      "25000 images complete\n",
      "26000 images complete\n",
      "27000 images complete\n",
      "28000 images complete\n",
      "29000 images complete\n",
      "30000 images complete\n",
      "31000 images complete\n",
      "32000 images complete\n",
      "33000 images complete\n",
      "34000 images complete\n",
      "35000 images complete\n",
      "36000 images complete\n",
      "37000 images complete\n",
      "38000 images complete\n",
      "39000 images complete\n",
      "40000 images complete\n",
      "41000 images complete\n",
      "42000 images complete\n",
      "43000 images complete\n",
      "44000 images complete\n",
      "45000 images complete\n",
      "46000 images complete\n",
      "47000 images complete\n",
      "48000 images complete\n",
      "49000 images complete\n",
      "50000 images complete\n",
      "51000 images complete\n",
      "52000 images complete\n",
      "53000 images complete\n",
      "54000 images complete\n",
      "55000 images complete\n",
      "56000 images complete\n",
      "57000 images complete\n",
      "58000 images complete\n",
      "59000 images complete\n",
      "60000 images complete\n",
      "61000 images complete\n",
      "62000 images complete\n",
      "63000 images complete\n",
      "64000 images complete\n",
      "65000 images complete\n",
      "66000 images complete\n",
      "67000 images complete\n",
      "68000 images complete\n",
      "69000 images complete\n",
      "70000 images complete\n",
      "71000 images complete\n",
      "72000 images complete\n",
      "73000 images complete\n",
      "74000 images complete\n",
      "75000 images complete\n",
      "76000 images complete\n",
      "77000 images complete\n",
      "78000 images complete\n",
      "79000 images complete\n",
      "80000 images complete\n",
      "81000 images complete\n",
      "82000 images complete\n",
      "83000 images complete\n",
      "84000 images complete\n",
      "85000 images complete\n",
      "86000 images complete\n",
      "87000 images complete\n",
      "88000 images complete\n",
      "89000 images complete\n",
      "90000 images complete\n",
      "91000 images complete\n",
      "92000 images complete\n",
      "93000 images complete\n",
      "94000 images complete\n",
      "95000 images complete\n",
      "96000 images complete\n",
      "97000 images complete\n",
      "98000 images complete\n",
      "99000 images complete\n",
      "100000 images complete\n",
      "101000 images complete\n",
      "102000 images complete\n",
      "103000 images complete\n",
      "104000 images complete\n",
      "105000 images complete\n",
      "106000 images complete\n",
      "107000 images complete\n",
      "108000 images complete\n",
      "109000 images complete\n",
      "110000 images complete\n",
      "111000 images complete\n",
      "112000 images complete\n",
      "113000 images complete\n",
      "114000 images complete\n",
      "115000 images complete\n",
      "116000 images complete\n",
      "117000 images complete\n",
      "118000 images complete\n",
      "119000 images complete\n",
      "120000 images complete\n",
      "121000 images complete\n",
      "122000 images complete\n",
      "123000 images complete\n",
      "124000 images complete\n",
      "125000 images complete\n",
      "126000 images complete\n",
      "127000 images complete\n",
      "128000 images complete\n",
      "129000 images complete\n",
      "130000 images complete\n",
      "131000 images complete\n",
      "132000 images complete\n",
      "133000 images complete\n",
      "134000 images complete\n",
      "135000 images complete\n",
      "136000 images complete\n",
      "137000 images complete\n",
      "138000 images complete\n",
      "139000 images complete\n",
      "140000 images complete\n",
      "141000 images complete\n",
      "142000 images complete\n",
      "143000 images complete\n",
      "144000 images complete\n",
      "145000 images complete\n",
      "146000 images complete\n",
      "148000 images complete\n",
      "149000 images complete\n",
      "150000 images complete\n",
      "151000 images complete\n",
      "152000 images complete\n",
      "153000 images complete\n",
      "154000 images complete\n",
      "155000 images complete\n",
      "156000 images complete\n",
      "157000 images complete\n",
      "158000 images complete\n",
      "159000 images complete\n",
      "160000 images complete\n",
      "161000 images complete\n",
      "162000 images complete\n",
      "163000 images complete\n",
      "164000 images complete\n",
      "165000 images complete\n",
      "166000 images complete\n",
      "167000 images complete\n",
      "168000 images complete\n",
      "169000 images complete\n",
      "170000 images complete\n",
      "171000 images complete\n",
      "172000 images complete\n",
      "173000 images complete\n",
      "174000 images complete\n",
      "175000 images complete\n",
      "176000 images complete\n",
      "177000 images complete\n",
      "178000 images complete\n",
      "179000 images complete\n",
      "180000 images complete\n",
      "181000 images complete\n",
      "182000 images complete\n",
      "183000 images complete\n",
      "184000 images complete\n",
      "185000 images complete\n",
      "186000 images complete\n",
      "187000 images complete\n",
      "188000 images complete\n",
      "189000 images complete\n",
      "190000 images complete\n",
      "191000 images complete\n",
      "192000 images complete\n",
      "193000 images complete\n",
      "194000 images complete\n",
      "195000 images complete\n",
      "196000 images complete\n",
      "197000 images complete\n",
      "198000 images complete\n",
      "199000 images complete\n",
      "200000 images complete\n",
      "201000 images complete\n",
      "202000 images complete\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'datasets' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-3eeea2d891d4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m ])\n\u001b[1;32m     44\u001b[0m \u001b[0mdata_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Datasets/resized_celebA'\u001b[0m          \u001b[0;31m# this path depends on your computer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m \u001b[0mdset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImageFolder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0mtrain_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0mtemp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'datasets' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.misc import imresize\n",
    "\n",
    "# root path depends on your computer\n",
    "root = 'Datasets/img_align_celeba/'\n",
    "save_root = 'Datasets/resized_celebA/'\n",
    "resize_size = 64\n",
    "\n",
    "if not os.path.isdir(save_root):\n",
    "    os.mkdir(save_root)\n",
    "if not os.path.isdir(save_root + 'celebA'):\n",
    "    os.mkdir(save_root + 'celebA')\n",
    "img_list = os.listdir(root)\n",
    "\n",
    "# ten_percent = len(img_list) // 10\n",
    "\n",
    "for i in range(len(img_list)):\n",
    "    img = plt.imread(root + img_list[i])\n",
    "    img = imresize(img, (resize_size, resize_size))\n",
    "    plt.imsave(fname=save_root + 'celebA/' + img_list[i], arr=img)\n",
    "\n",
    "    if (i % 1000) == 0:\n",
    "        print('%d images complete' % i)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_loader\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from torchvision import datasets, transforms\n",
    "img_size = 64\n",
    "isCrop = False\n",
    "if isCrop:\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Scale(108),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
    "    ])\n",
    "else:\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
    "    ])\n",
    "transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
    "])\n",
    "data_dir = 'Datasets/resized_celebA'          # this path depends on your computer\n",
    "dset = datasets.ImageFolder(data_dir, transform)\n",
    "train_loader = torch.utils.data.DataLoader(dset, batch_size=128, shuffle=True)\n",
    "temp = plt.imread(train_loader.dataset.imgs[0][0])\n",
    "if (temp.shape[0] != img_size) or (temp.shape[0] != img_size):\n",
    "    sys.stderr.write('Error! image size is not 64 x 64! run \\\"celebA_data_preprocess.py\\\" !!!')\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal_init(m, mean, std):\n",
    "    if isinstance(m, nn.ConvTranspose2d) or isinstance(m, nn.Conv2d):\n",
    "        m.weight.data.normal_(mean, std)\n",
    "        m.bias.data.zero_()\n",
    "\n",
    "fixed_z_ = torch.randn((5 * 5, 100)).view(-1, 100, 1, 1)    # fixed noise\n",
    "fixed_z_ = Variable(fixed_z_.cuda(), volatile=True)\n",
    "def show_result(num_epoch, show = False, save = False, path = 'result.png', isFix=False):\n",
    "    z_ = torch.randn((5*5, 100)).view(-1, 100, 1, 1)\n",
    "    z_ = Variable(z_.cuda(), volatile=True)\n",
    "\n",
    "    G.eval()\n",
    "    if isFix:\n",
    "        test_images = G(fixed_z_)\n",
    "    else:\n",
    "        test_images = G(z_)\n",
    "    G.train()\n",
    "\n",
    "    size_figure_grid = 5\n",
    "    fig, ax = plt.subplots(size_figure_grid, size_figure_grid, figsize=(5, 5))\n",
    "    for i, j in itertools.product(range(size_figure_grid), range(size_figure_grid)):\n",
    "        ax[i, j].get_xaxis().set_visible(False)\n",
    "        ax[i, j].get_yaxis().set_visible(False)\n",
    "\n",
    "    for k in range(5*5):\n",
    "        i = k // 5\n",
    "        j = k % 5\n",
    "        ax[i, j].cla()\n",
    "        ax[i, j].imshow((test_images[k].cpu().data.numpy().transpose(1, 2, 0) + 1) / 2)\n",
    "\n",
    "    label = 'Epoch {0}'.format(num_epoch)\n",
    "    fig.text(0.5, 0.04, label, ha='center')\n",
    "    plt.savefig(path)\n",
    "\n",
    "    if show:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "img_size = (64, 64, 3)\n",
    "\n",
    "generator = generatorcA()\n",
    "discriminator = discriminatorcA()\n",
    "\n",
    "# Initialize optimizers\n",
    "lr = 1e-4\n",
    "betas = (.9, .99)\n",
    "G_optimizer = optim.Adam(generator.parameters(), lr=lr, betas=betas)\n",
    "D_optimizer = optim.Adam(discriminator.parameters(), lr=lr, betas=betas)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shared/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:164: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "/home/shared/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:110: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "/home/shared/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:120: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1\n",
      "D loss: 48.0268669128418\n",
      "GP: 48.041595458984375\n",
      "Gradient norm: 0.30698245763778687\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shared/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:137: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "epochs = 30\n",
    "trainer = Trainer(generator, discriminator, G_optimizer, D_optimizer,\n",
    "                  use_cuda=torch.cuda.is_available())\n",
    "G_loss,D_loss,GP_loss=trainer.train(train_loader, epochs, save_training_gif=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving a new best\n",
      "=> Saving a new best\n"
     ]
    }
   ],
   "source": [
    "save_checkpoint({'epoch': 22,'state_dict': generator.state_dict()}, True,\"GcelebA_checkpoint.pth.tar\")\n",
    "save_checkpoint({'epoch': 22,'state_dict': discriminator.state_dict()}, True,\"DcelebA_checkpoint.pth.tar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
